{
  
    
        "post0": {
            "title": "O som do átomo de hidrogênio",
            "content": "Inspirado pelo vídeo do minute physics decidi fazer esse mini projeto para mostrar como fazer projetos envolvendo faixas de som audíveis no próprio navegador, totalmente em python através do Jupyter Notebook. . Para isso, vamos usar o espectro do átomo de hidrogênio, e vamos fazer um shift linear nele para dentro do espectro audível de frequências sonoras. O nome dado a esse processo é sonificação, você pode usar o link para saber mais caso se interesse. . %matplotlib inline . from __future__ import division, print_function, absolute_import import scipy.constants as const import numpy as np import scipy from matplotlib.pyplot import plot from scipy.io import wavfile from IPython.core.display import HTML, display . try: from IPython.display import Audio def wavPlayer(data, rate): display(Audio(data, rate=rate)) except ImportError: pass . from numpy import sin, pi rate = 44100 # freq = 44.1 khz duration = 3 # duração do audio # função para normalizar o seno na amplitude certa para um arquivo wav normedsin = lambda f,t : 2**13*sin(2*pi*f*t) time = np.linspace(0,duration, num=rate*duration) . Primeiro teste para o wav Player . Para começar, vamos começar tocando algo em 440 Hz . import numpy import struct import warnings . la = lambda t : normedsin(440,t) # plot rápido da onda nos primeiros 25 ms plot(time[0:1000], la(time)[0:1000]) ampl = la(time).astype(np.int16) # função que salva o áudio e toca ele em HTML5 wavPlayer(ampl, rate) . Your browser does not support the audio element. As diferentes frequências emitidas por um átomo de hidrogênio são analiticamente calculadas através da formula de Rydberg: . $$ frac{1}{ lambda} = R left( frac{1}{n_1} - frac{1}{n_2} right) $$ . Como sabemos, essa relação tem um formato muito similar a das freqências emitidas do hidrogênio . $$ f_{n,m} = frac{c}{ lambda} = frac{R_h}{h} left( frac{1}{n} - frac{1}{m} right) $$ . Para o caso de $n = 1$ reconhecemos a famosa série de Lyman, e para $n = 2$ a série de Balmer . f0 = const.Rydberg*const.c print(&quot;A maior frequência do hidrogênio é &quot;,f0,&quot;Hz. e corresponde a n = 1, m = ∞&quot;) fshift = 440 print(&quot;todavia odemos shiftar o espectro para 440 Hz (A)&quot;) . A maior frequência do hidrogênio é 3289841960250880.5 Hz. e corresponde a n = 1, m = ∞ todavia odemos shiftar o espectro para 440 Hz (A) . ryd = lambda n,m : fshift*(1/(n**2) -1/(m**2)) flyman = lambda x : ryd(1,x) fbalmer = lambda x : ryd(2,x) . ser = lambda t : sum( [normedsin(flyman(i),t)+normedsin(fbalmer(i+1),t) for i in range(2,8)]) # e uma função em forma de vetor para realizar operações em cada um dos seus elementos serv = scipy.vectorize(ser) . ss= serv(time) . plot(time, ss) ss = 2**15*ss/ss.max() . wavPlayer(ss.astype(np.int16),rate) . Your browser does not support the audio element.",
            "url": "https://resteche.github.io/REsteche_blog/estrutura%20da%20mat%C3%A9ria/f%C3%ADsica%20moderna/f%C3%ADsica%20atomica/musica/2021/06/10/Som_hidrog%C3%AAnio.html",
            "relUrl": "/estrutura%20da%20mat%C3%A9ria/f%C3%ADsica%20moderna/f%C3%ADsica%20atomica/musica/2021/06/10/Som_hidrog%C3%AAnio.html",
            "date": " • Jun 10, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Duds Teste",
            "content": "The idea of this blog post is to give a deeper idea on how to perform Optimal Transport for Domain Adaptation. Throughout this tutorial, we will use the works of Courty et al. [1], who were the first authors to propose using Optimal Transport for adapting models in a unsupervised way. . We do not give much details on how to perform Optimal Transport. We simply rely on the awesome library Python Optimal Transport. For more information you can take a look on the textbook by Peyré and Cuturi [2]. In this tutorial, we explore how to adapt a Convolutional Neural Network for classifying slightly different digits. . !pip install pot !pip install torchinfo . Requirement already satisfied: pot in /usr/local/lib/python3.7/dist-packages (0.8.2) Requirement already satisfied: numpy&gt;=1.16 in /usr/local/lib/python3.7/dist-packages (from pot) (1.21.6) Requirement already satisfied: scipy&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from pot) (1.4.1) Requirement already satisfied: torchinfo in /usr/local/lib/python3.7/dist-packages (1.6.5) . . import ot import numpy as np import matplotlib.pyplot as plt from tqdm import tqdm import torch from torchinfo import summary from torchvision import datasets from torchvision import transforms from torchvision.utils import make_grid from sklearn.metrics import accuracy_score from sklearn.neighbors import KNeighborsClassifier . device = &#39;cpu&#39; . Loading the datasets . For the transfer learning task, we will use MNIST [3] and USPS [4] datasets. These are very simillar datasets which consists on handwritten digits (in white) on a solid background (in black). Our goal is to adapt a model learned on MNIST to classify digits in USPS correctly. . This adaptation problem has been extensively studied by the community (e.g. [5, 6]). Here we provide a show case that does not focuses on performance. We perform the following preprocessing steps to the digits: . We convert each pixel from uint8 [0, 255] to float32 in [0, 1]. This is done by the transforms.ToTensor() call, as you can see below. | We resize each image to $32 times 32$. Note that this introduces artifacts, especially in USPS digits which were $16 times 16$ originally. | We replicate the image accross the 3 RGB channels | This is the step-by-step of the transform composition shown below. . T = transforms.Compose([ transforms.ToTensor(), transforms.Resize((32, 32)), transforms.Lambda(lambda x: x.repeat([3, 1, 1])) ]) . Once the preprocessing pipeline is constructed, we can create source and target dataloaders . src_dataset = datasets.MNIST(root=&#39;./.tmp&#39;, train=True, transform=T, download=True) src_loader = torch.utils.data.DataLoader(dataset=src_dataset, batch_size=256, shuffle=True) tgt_dataset = datasets.USPS(root=&#39;./.tmp&#39;, train=True, transform=T, download=True) tgt_loader = torch.utils.data.DataLoader(dataset=tgt_dataset, batch_size=256, shuffle=True) . We can see a few samples from both datasets by iterating through the dataloaders . for xs, ys in src_loader: break for xt, yt in tgt_loader: break . fig, axes = plt.subplots(1, 2, figsize=(10, 5)) ind = np.random.choice(np.arange(len(xs)), size=64) samples = xs[ind] grid = make_grid(samples).numpy().transpose([1, 2, 0]) print(&#39;Source Labels: {}&#39;.format(ys[ind].reshape(8, 8))) axes[0].set_title(&#39;Source Domain Samples&#39;) axes[0].imshow(grid) axes[0].axis(&#39;off&#39;) ind = np.random.choice(np.arange(len(xs)), size=64) samples = xt[ind] grid = make_grid(samples).numpy().transpose([1, 2, 0]) print(&#39;Target Labels: {}&#39;.format(yt[ind].reshape(8, 8))) axes[1].set_title(&#39;Target Domain Samples&#39;) axes[1].imshow(grid) axes[1].axis(&#39;off&#39;) . Source Labels: tensor([[3, 1, 2, 9, 1, 9, 6, 3], [5, 4, 4, 5, 0, 0, 6, 3], [7, 7, 4, 8, 8, 6, 4, 8], [4, 9, 4, 0, 8, 1, 7, 8], [2, 3, 4, 5, 4, 0, 5, 1], [5, 6, 6, 4, 5, 8, 9, 5], [0, 5, 0, 7, 0, 3, 3, 3], [3, 5, 9, 3, 8, 3, 6, 9]]) Target Labels: tensor([[7, 5, 2, 7, 8, 2, 5, 0], [4, 8, 8, 9, 1, 7, 6, 4], [0, 4, 7, 3, 2, 7, 6, 6], [0, 1, 2, 5, 2, 5, 0, 7], [9, 1, 0, 3, 7, 0, 1, 1], [7, 6, 3, 8, 7, 3, 9, 6], [4, 3, 7, 7, 7, 1, 3, 4], [1, 7, 4, 6, 6, 6, 4, 4]]) . (-0.5, 273.5, 273.5, -0.5) . Note that even though much simillar, the source and target datasets are quite different. Especially, MNIST digits are centered on the grid $32 times 32$, while USPS digits tend to occupy the whole grid. As it turns out this is quite important for the CNN, despite not interfering with human recognition. . This is an example of the known covariate shift phenomenon, which corresponds to when $P_{S}(X) neq P_{T}(X)$. In other words, the marginal feature distribution changes accross domains. This happens when the statistical properties of the data change. . In what follows, we will, . Load a CNN feature extractor pre-trained on MNIST | Measure its performance on USPS | Use Optimal Transport to enhance the performance on USPS. | . Loading a pretrained a Feature Extractor . Here we use the classical architecture LeNet5 [6], especifically designed for classifying MNIST digits. Here you can see its overall description. . . We adopt a few tricks. First, we increase the number of channels in the convolutional layer, from 6 to 32. In overall this is beneficial for performance, with a slight danger of overfitting. Moreover, the fully connected layers all have 100 units. The architecture is straightforward to read in the next code block. . class LeNet5(torch.nn.Module): def __init__(self, n_channels=3): self.n_channels = n_channels super(LeNet5, self).__init__() self.feature_extractor = torch.nn.Sequential( torch.nn.Conv2d(in_channels=n_channels, out_channels=32, kernel_size=5), torch.nn.ReLU(), torch.nn.MaxPool2d(stride=2, kernel_size=2), torch.nn.Conv2d(in_channels=32, out_channels=48, kernel_size=5), torch.nn.ReLU() ) self.class_discriminator = torch.nn.Sequential( torch.nn.Linear(in_features=48 * 10 * 10, out_features=100), torch.nn.ReLU(), torch.nn.Linear(in_features=100, out_features=100), torch.nn.ReLU(), torch.nn.Linear(in_features=100, out_features=10), torch.nn.Softmax(dim=1) ) def forward(self, x): y = self.feature_extractor(x) h = y.view(-1, 48 * 10 * 10) features = self.class_discriminator[:-2](h) predicted_labels = self.class_discriminator(h) return features, predicted_labels . Here we use a pretrained file downloaded from github . import requests . url = &quot;https://raw.githubusercontent.com/eddardd/my-personal-blog/master/_notebooks/files/lenet5.pt&quot; r = requests.get(url) with open(&#39;./lenet5.pt&#39;, &#39;wb&#39;) as f: f.write(r.content) . model = torch.load(&#39;./lenet5.pt&#39;, map_location=torch.device(&#39;cpu&#39;)) . Measuring the Model Performance . A standard case in domain adaptation is the naive approach: we do not try to adapt the model. This corresponds to applying the classifier to a dataset that follows a different probability distribution -- and hence may not work as well as the source dataset on which the classifier was trained. This is known in the literature as baseline. . Here we proceed as in [1], and evaluate the baseline with a 1-NN classifier on the features extracted by the CNN. Our first step is to construct a matrix $H_{S} in mathbb{R}^{n_{S} times 100}$, which serves as the training data for the 1-NN. We do so by defining . $$h_{S}^{i} = phi(x_{S}^{i})$$ . where $ phi$ is the convolutional feature extractor. . Hs, Ys = [], [] for xs, ys in tqdm(src_loader): with torch.no_grad(): hs, _ = model(xs) Hs.append(hs) Ys.append(ys) Hs = torch.cat(Hs, dim=0) Ys = torch.cat(Ys) . 100%|██████████| 235/235 [00:58&lt;00:00, 4.02it/s] . We perform a similar operation on the target domain . Ht, Yt = [], [] for xt, yt in tqdm(tgt_loader): with torch.no_grad(): ht, _ = model(xt) Ht.append(ht) Yt.append(yt) Ht = torch.cat(Ht, dim=0) Yt = torch.cat(Yt) . 100%|██████████| 29/29 [00:04&lt;00:00, 5.83it/s] . Finally, the 1-NN procedure consists on calculating the pairwise distances between source and target domain data. This is done through the function cdist of Pytorch. . C = torch.cdist(Hs, Ht, 2) ** 2 ind_opt = C.argmin(dim=0) del C . Yp = Ys[ind_opt] . print(accuracy_score(Yt, Yp)) . 0.7819229186668495 . This will serve as a baseline for the performance of domain adaptation. In this case, a succesfull adaptation implies on an accuracy higher than 78.19% . Optimal Transport . Background . Optimal Transport (OT) [7] is a mathematical theory concerned with mass displacement at least effort. The field was founded by the French mathematician Gaspard Monge, and recast in the 20-th century in terms of linear programming, by nobel prize Leonid Kantorovich. . But, what makes OT interesting for ML, and more specifically Transfer Learning? Well, one can understand probability distributions as distributions of mass. This way, OT serve as a framework for manipulating probability distributions. This is of key importance in many fields of ML (e.g. Generative Modeling), especially transfer learning. . To set up the OT problem, let us consider hypothetical, unknown probability distributions $P_{S}(X)$ and $P_{T}(X)$ from which source and target features are drawn from. These can be approximated empirically from samples, $$ hat{P}_{S}(x) = dfrac{1}{n_{S}} sum_{i=1}^{n_{S}} delta( mathbf{x}- mathbf{x}_{S}^{i}),$$ where $ delta$ is the Dirac delta function. For data matrices $ mathbf{X}_{S} in mathbb{R}^{n_{S} times d}$ and $ mathbf{X}_{T} in mathbb{R}^{n_{T} times d}$, the OT problem revolves around findng a transportation plan $ pi in mathbb{R}^{n_{S} times n_{T}}$, that specieis how much mass is transported between samples $ mathbf{x}_{S}^{i}$ and $ mathbf{x}_{T}^{j}$. As such, $ pi$ should conserve mass, that is, it should not create nor distruct mass through transportation. This amounts to imposing, $$ sum_{i=1}^{n_{S}} pi_{ij} = dfrac{1}{n_{T}} text{ and } sum_{j=1}^{n_{T}} pi_{ij} = dfrac{1}{n_{S}}. $$ . These are linear constraints imposed on $ pi_{ij}$. If we further define a cost of transportation $c( cdot, cdot)$ between $ mathbf{x}_{S}^{i}$ and $ mathbf{x}_{T}^{j}$, we can define an objective function that corresponds to the total effort $E( pi)$, . $$E( pi) = sum_{i=1}^{n_{S}} sum_{j=1}^{n_{T}} pi_{ij}c( mathbf{x}_{S}^{i}, mathbf{x}_{T}^{j})$$ . The OT problem is thus a linear program, which means that its cost and constraints are linear with respect to the variables $ pi_{ij}$. This is, nonetheless, a large-scale problem, as the number of variables scales with the number of samples. In the era of Deep Learning, this is a huge constraint as the number of samples in datasets is remarkably huge. We will get back to this matter in a bit. . Entropic Regularization . Instead of using linear programming for solving OT, we follow [8] and calculate an efficient approximation of $ pi$ by introducing an entropic regularization term to $E$. This consists on changing the objective function to, . $$E( pi) = sum_{i=1}^{n_{S}} sum_{j=1}^{n_{T}} pi_{ij}c( mathbf{x}_{S}^{i}, mathbf{x}_{T}^{j}) + epsilon sum_{i=1}^{n_{S}} sum_{j=1}^{n_{T}} pi_{ij} log pi_{ij}$$ . In practice, this has two effects: (i) it smoothes the linear programming problem, leading to a smooth $ pi$. (ii) it allows for a faster calculation based on matrix-scaling algorithms. These 2 factors often lead to better adaptation results. . Optimal Transport for Domain Adaptation . Even though $ pi$ is informative about source and target distributions, it does not tells us how to map different samples. This is solved by introducing the concept of barycentric mapping, which is defined as, . $$T_{ pi}( mathbf{x}_{S}^{i}) = underset{ mathbf{x} in mathbb{R}^{d}}{ text{argmin}} sum_{j=1}^{n_{T}} pi_{ij}c( mathbf{x}, mathbf{x}_{T}^{j}).$$ . When $c( mathbf{x}_{S}^{i}, mathbf{x}_{T}^{j}) = lVert mathbf{x}_{S}^{i} - mathbf{x}_{T}^{j} rVert_{2}^{2}$, this problem has closed form solution, . $$T_{ pi}( mathbf{X}_{S}) = n_{S} pi mathbf{X}_{T}$$ . however, note that $T_{ pi}$ is only defined for the source domain samples it has been defined on. For our particular dataset, this would amount to calculating an OT plan of size $60000 times 7291$, which can be done, but would take an enormous amount of time and storage. . A workaround was introduced by [9], which consists on downsampling the datasets (to a sub-set of representative points), fitting the map $T_{ pi}$ on this sub-sample, then interpolating $T_{ pi}$ on novel samples, . $$T_{ pi}( mathbf{x}) = T_{ pi}( mathbf{x}_{S}^{i_{ star}}) + mathbf{x} - mathbf{x}_{S}^{i_{ star}}$$ . where $1 leq i_{ star} leq n_{S}$ is the index of the nearest neighbor $ mathbf{x}$ in the data matrix $ mathbf{X}_{S}$. . Extracting data . As mentioned previously, we cannot fit OT to the entire dataset. We thus extract a sub-sample from the original datasets in order to have a feasible $T_{ pi}$. As follows, we extract 10 batches from the source and target datasets. . _Hs = [] k = 0 for xs, _ in src_loader: with torch.no_grad(): hs, _ = model(xs) _Hs.append(hs) k += 1 if k == 10: break _Hs = torch.cat(_Hs, dim=0).numpy() . _Ht = [] k = 0 for xt, _ in tgt_loader: with torch.no_grad(): ht, _ = model(xt) _Ht.append(ht) k += 1 if k == 10: break _Ht = torch.cat(_Ht, dim=0).numpy() . _Hs.shape, _Ht.shape . ((2560, 100), (2560, 100)) . Fitting the Barycentric Mapping . Here we use Python Optimal Transport for fitting $T_{ pi}$ to our sample. . otda = ot.da.SinkhornTransport(reg_e=1e-2, norm=&#39;max&#39;) otda.fit(Xs=_Hs, ys=None, Xt=_Ht, yt=None) . /usr/local/lib/python3.7/dist-packages/ot/bregman.py:517: UserWarning: Sinkhorn did not converge. You might want to increase the number of iterations `numItermax` or the regularization parameter `reg`. warnings.warn(&#34;Sinkhorn did not converge. You might want to &#34; . &lt;ot.da.SinkhornTransport at 0x7fbd03299790&gt; . . next we visualize the optimal transport plan, . plt.imshow(np.log(otda.coupling_ + 1e-12), cmap=&#39;Reds&#39;) . &lt;matplotlib.image.AxesImage at 0x7fbd078c1590&gt; . Transporting and Evaluating on Target Domain . Now, we get to our final step, where we measure the success of adaptation. First we need to extract features from source domain samples, then transport them to the target domain. This is done by composing the feature extractor with the mapping $T_{ pi}$, . $$ hat{h}_{S}^{i} = T_{ pi}( phi( mathbf{x}_{S}^{i}))$$ . THs, Ys = [], [] for xs, ys in tqdm(src_loader): with torch.no_grad(): hs, _ = model(xs) hs = torch.from_numpy(otda.transform(hs.numpy())) THs.append(hs) Ys.append(ys) THs = torch.cat(THs, dim=0) Ys = torch.cat(Ys, dim=0) . 100%|██████████| 235/235 [01:14&lt;00:00, 3.14it/s] . In order to avoid overflowing the memory, we calculate the distance of mini-batches of target domain samples to the transported features, . Yp = torch.zeros_like(Yt) num_batches = len(Ht) // 64 + 1 for i in tqdm(range(num_batches), total=num_batches): ht = Ht[i * 64: (i + 1) * 64] C = torch.cdist(THs, ht, 2) ** 2 ind_opt = C.argmin(dim=0) Yp[i * 64: (i + 1) * 64] = Ys[ind_opt] . 100%|██████████| 114/114 [00:09&lt;00:00, 11.77it/s] . print(accuracy_score(Yt, Yp)) . 0.7955013029762721 . Want to Know More? . Optimal Transport has made a significant impact on the machine learning community in past 10 years. If you are interested in more resources and papers on the interplay between OT and Transfer Learning, you can consider looking on, . How to induce specific structures (e.g. classes) in OT maps [1, 6] | Solving issues w/ extending barycentric mappings [6, 10] | OTDA on joint distributions [11, 12] | Multi-Source Domain Adaptation [13, 14] | . References . [1] N. Courty, R. Flamary, D. Tuia, and A. Rakotomamonjy, “Optimal transport for domain adaptation,” IEEE transactions on pattern analysis and machine intelligence, vol. 39, no. 9, pp. 1853–1865, 2016. . [2] Peyré, G., &amp; Cuturi, M. (2019). Computational optimal transport: With applications to data science. Foundations and Trends® in Machine Learning, 11(5-6), 355-607. . [3] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel, “Backpropagation applied to handwritten zip code recognition,” Neural computation, vol. 1, no. 4, pp. 541–551, 1989. . [4] J. J. Hull, “A database for handwritten text recognition research,” IEEE Transactions on pattern analysis and machine intelligence, vol. 16, no. 5, pp. 550–554, 1994 . [5] Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., ... &amp; Lempitsky, V. (2016). Domain-adversarial training of neural networks. The journal of machine learning research, 17(1), 2096-2030. . [6] Seguy, V., Damodaran, B. B., Flamary, R., Courty, N., Rolet, A., &amp; Blondel, M. (2017). Large-scale optimal transport and mapping estimation. arXiv preprint arXiv:1711.02283. . [7] Villani, C. (2009). Optimal transport: old and new (Vol. 338, p. 23). Berlin: springer. . [8] Cuturi, M. (2013). Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural information processing systems, 26. . [9] Ferradans, S., Papadakis, N., Peyré, G., &amp; Aujol, J. F. (2014). Regularized discrete optimal transport. SIAM Journal on Imaging Sciences, 7(3), 1853-1882. . [10] Perrot, M., Courty, N., Flamary, R., &amp; Habrard, A. (2016). Mapping estimation for discrete optimal transport. Advances in Neural Information Processing Systems, 29. . [11] Courty, N., Flamary, R., Habrard, A., &amp; Rakotomamonjy, A. (2017). Joint distribution optimal transportation for domain adaptation. Advances in Neural Information Processing Systems, 30. . [12] Damodaran, B. B., Kellenberger, B., Flamary, R., Tuia, D., &amp; Courty, N. (2018). Deepjdot: Deep joint distribution optimal transport for unsupervised domain adaptation. In Proceedings of the European Conference on Computer Vision (ECCV) (pp. 447-463). . [13] Nguyen, T., Le, T., Zhao, H., Tran, Q. H., Nguyen, T., &amp; Phung, D. (2021, December). Most: Multi-source domain adaptation via optimal transport for student-teacher learning. In Uncertainty in Artificial Intelligence (pp. 225-235). PMLR. . [14] Montesuma, E. F., &amp; Mboula, F. M. N. (2021). Wasserstein Barycenter for Multi-Source Domain Adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 16785-16793). .",
            "url": "https://resteche.github.io/REsteche_blog/optimal%20transport/transfer%20learning/image%20classification/2021/03/01/Test_project.html",
            "relUrl": "/optimal%20transport/transfer%20learning/image%20classification/2021/03/01/Test_project.html",
            "date": " • Mar 1, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "fastcore: An Underrated Python Library",
            "content": ". Background . I recently embarked on a journey to sharpen my python skills: I wanted to learn advanced patterns, idioms, and techniques. I started with reading books on advanced Python, however, the information didn&#39;t seem to stick without having somewhere to apply it. I also wanted the ability to ask questions from an expert while I was learning -- which is an arrangement that is hard to find! That&#39;s when it occurred to me: What if I could find an open source project that has fairly advanced python code and write documentation and tests? I made a bet that if I did this it would force me to learn everything very deeply, and the maintainers would be appreciative of my work and be willing to answer my questions. . And that&#39;s exactly what I did over the past month! I&#39;m pleased to report that it has been the most efficient learning experience I&#39;ve ever experienced. I&#39;ve discovered that writing documentation forced me to deeply understand not just what the code does but also why the code works the way it does, and to explore edge cases while writing tests. Most importantly, I was able to ask questions when I was stuck, and maintainers were willing to devote extra time knowing that their mentorship was in service of making their code more accessible! It turns out the library I choose, fastcore is some of the most fascinating Python I have ever encountered as its purpose and goals are fairly unique. . For the uninitiated, fastcore is a library on top of which many fast.ai projects are built on. Most importantly, fastcore extends the python programming language and strives to eliminate boilerplate and add useful functionality for common tasks. In this blog post, I&#39;m going to highlight some of my favorite tools that fastcore provides, rather than sharing what I learned about python. My goal is to pique your interest in this library, and hopefully motivate you to check out the documentation after you are done to learn more! . Why fastcore is interesting . Get exposed to ideas from other languages without leaving python: I’ve always heard that it is beneficial to learn other languages in order to become a better programmer. From a pragmatic point of view, I’ve found it difficult to learn other languages because I could never use them at work. Fastcore extends python to include patterns found in languages as diverse as Julia, Ruby and Haskell. Now that I understand these tools I am motivated to learn other languages. | You get a new set of pragmatic tools: fastcore includes utilities that will allow you to write more concise expressive code, and perhaps solve new problems. | Learn more about the Python programming language: Because fastcore extends the python programming language, many advanced concepts are exposed during the process. For the motivated, this is a great way to see how many of the internals of python work. | A whirlwind tour through fastcore . Here are some things you can do with fastcore that immediately caught my attention. . . Making **kwargs transparent . Whenever I see a function that has the argument **kwargs, I cringe a little. This is because it means the API is obfuscated and I have to read the source code to figure out what valid parameters might be. Consider the below example: . def baz(a, b=2, c=3, d=4): return a + b + c def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, **kwargs)&gt; . Without reading the source code, it might be hard for me to know that foo also accepts and additional parameters b and d. We can fix this with delegates: . def baz(a, b=2, c=3, d=4): return a + b + c @delegates(baz) # this decorator will pass down keyword arguments from baz def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, b=2, d=4)&gt; . You can customize the behavior of this decorator. For example, you can have your cake and eat it too by passing down your arguments and also keeping **kwargs: . @delegates(baz, keep=True) def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, b=2, d=4, **kwargs)&gt; . You can also exclude arguments. For example, we exclude argument d from delegation: . def basefoo(a, b=2, c=3, d=4): pass @delegates(basefoo, but=[&#39;d&#39;]) # exclude `d` def foo(c, a, **kwargs): pass inspect.signature(foo) . &lt;Signature (c, a, b=2)&gt; . You can also delegate between classes: . class BaseFoo: def __init__(self, e, c=2): pass @delegates()# since no argument was passsed here we delegate to the superclass class Foo(BaseFoo): def __init__(self, a, b=1, **kwargs): super().__init__(**kwargs) inspect.signature(Foo) . &lt;Signature (a, b=1, c=2)&gt; . For more information, read the docs on delegates. . . Avoid boilerplate when setting instance attributes . Have you ever wondered if it was possible to avoid the boilerplate involved with setting attributes in __init__? . class Test: def __init__(self, a, b ,c): self.a, self.b, self.c = a, b, c . Ouch! That was painful. Look at all the repeated variable names. Do I really have to repeat myself like this when defining a class? Not Anymore! Checkout store_attr: . class Test: def __init__(self, a, b, c): store_attr() t = Test(5,4,3) assert t.b == 4 . You can also exclude certain attributes: . class Test: def __init__(self, a, b, c): store_attr(but=[&#39;c&#39;]) t = Test(5,4,3) assert t.b == 4 assert not hasattr(t, &#39;c&#39;) . There are many more ways of customizing and using store_attr than I highlighted here. Check out the docs for more detail. . P.S. you might be thinking that Python dataclasses also allow you to avoid this boilerplate. While true in some cases, store_attr is more flexible.1 . 1. For example, store_attr does not rely on inheritance, which means you won&#39;t get stuck using multiple inheritance when using this with your own classes. Also, unlike dataclasses, store_attr does not require python 3.7 or higher. Furthermore, you can use store_attr anytime in the object lifecycle, and in any location in your class to customize the behavior of how and when variables are stored.↩ . . Avoiding subclassing boilerplate . One thing I hate about python is the __super__().__init__() boilerplate associated with subclassing. For example: . class ParentClass: def __init__(self): self.some_attr = &#39;hello&#39; class ChildClass(ParentClass): def __init__(self): super().__init__() cc = ChildClass() assert cc.some_attr == &#39;hello&#39; # only accessible b/c you used super . We can avoid this boilerplate by using the metaclass PrePostInitMeta. We define a new class called NewParent that is a wrapper around the ParentClass: . class NewParent(ParentClass, metaclass=PrePostInitMeta): def __pre_init__(self, *args, **kwargs): super().__init__() class ChildClass(NewParent): def __init__(self):pass sc = ChildClass() assert sc.some_attr == &#39;hello&#39; . . Type Dispatch . Type dispatch, or Multiple dispatch, allows you to change the way a function behaves based upon the input types it receives. This is a prominent feature in some programming languages like Julia. For example, this is a conceptual example of how multiple dispatch works in Julia, returning different values depending on the input types of x and y: . collide_with(x::Asteroid, y::Asteroid) = ... # deal with asteroid hitting asteroid collide_with(x::Asteroid, y::Spaceship) = ... # deal with asteroid hitting spaceship collide_with(x::Spaceship, y::Asteroid) = ... # deal with spaceship hitting asteroid collide_with(x::Spaceship, y::Spaceship) = ... # deal with spaceship hitting spaceship . Type dispatch can be especially useful in data science, where you might allow different input types (i.e. Numpy arrays and Pandas dataframes) to a function that processes data. Type dispatch allows you to have a common API for functions that do similar tasks. . Unfortunately, Python does not support this out-of-the box. Fortunately, there is the @typedispatch decorator to the rescue. This decorator relies upon type hints in order to route inputs the correct version of the function: . @typedispatch def f(x:str, y:str): return f&#39;{x}{y}&#39; @typedispatch def f(x:np.ndarray): return x.sum() @typedispatch def f(x:int, y:int): return x+y . Below is a demonstration of type dispatch at work for the function f: . f(&#39;Hello &#39;, &#39;World!&#39;) . &#39;Hello World!&#39; . f(2,3) . 5 . f(np.array([5,5,5,5])) . 20 . There are limitations of this feature, as well as other ways of using this functionality that you can read about here. In the process of learning about typed dispatch, I also found a python library called multipledispatch made by Mathhew Rocklin (the creator of Dask). . After using this feature, I am now motivated to learn languages like Julia to discover what other paradigms I might be missing. . . A better version of functools.partial . functools.partial is a great utility that creates functions from other functions that lets you set default values. Lets take this function for example that filters a list to only contain values &gt;= val: . test_input = [1,2,3,4,5,6] def f(arr, val): &quot;Filter a list to remove any values that are less than val.&quot; return [x for x in arr if x &gt;= val] f(test_input, 3) . [3, 4, 5, 6] . You can create a new function out of this function using partial that sets the default value to 5: . filter5 = partial(f, val=5) filter5(test_input) . [5, 6] . One problem with partial is that it removes the original docstring and replaces it with a generic docstring: . filter5.__doc__ . &#39;partial(func, *args, **keywords) - new function with partial application n of the given arguments and keywords. n&#39; . fastcore.utils.partialler fixes this, and makes sure the docstring is retained such that the new API is transparent: . filter5 = partialler(f, val=5) filter5.__doc__ . &#39;Filter a list to remove any values that are less than val.&#39; . . Composition of functions . A technique that is pervasive in functional programming languages is function composition, whereby you chain a bunch of functions together to achieve some kind of result. This is especially useful when applying various data transformations. Consider a toy example where I have three functions: (1) Removes elements of a list less than 5 (from the prior section) (2) adds 2 to each number (3) sums all the numbers: . def add(arr, val): return [x + val for x in arr] def arrsum(arr): return sum(arr) # See the previous section on partialler add2 = partialler(add, val=2) transform = compose(filter5, add2, arrsum) transform([1,2,3,4,5,6]) . 15 . But why is this useful? You might me thinking, I can accomplish the same thing with: . arrsum(add2(filter5([1,2,3,4,5,6]))) . You are not wrong! However, composition gives you a convenient interface in case you want to do something like the following: . def fit(x, transforms:list): &quot;fit a model after performing transformations&quot; x = compose(*transforms)(x) y = [np.mean(x)] * len(x) # its a dumb model. Don&#39;t judge me return y # filters out elements &lt; 5, adds 2, then predicts the mean fit(x=[1,2,3,4,5,6], transforms=[filter5, add2]) . [7.5, 7.5] . For more information about compose, read the docs. . . A more useful __repr__ . In python, __repr__ helps you get information about an object for logging and debugging. Below is what you get by default when you define a new class. (Note: we are using store_attr, which was discussed earlier). . class Test: def __init__(self, a, b=2, c=3): store_attr() # `store_attr` was discussed previously Test(1) . &lt;__main__.Test at 0x7ffcd766cee0&gt; . We can use basic_repr to quickly give us a more sensible default: . class Test: def __init__(self, a, b=2, c=3): store_attr() __repr__ = basic_repr(&#39;a,b,c&#39;) Test(2) . Test(a=2, b=2, c=3) . . Monkey Patching With A Decorator . It can be convenient to monkey patch with a decorator, which is especially helpful when you want to patch an external library you are importing. We can use the decorator @patch from fastcore.foundation along with type hints like so: . class MyClass(int): pass @patch def func(self:MyClass, a): return self+a mc = MyClass(3) . Now, MyClass has an additional method named func: . mc.func(10) . 13 . Still not convinced? I&#39;ll show you another example of this kind of patching in the next section. . . A better pathlib.Path . When you see these extensions to pathlib.path you won&#39;t ever use vanilla pathlib again! A number of additional methods have been added to pathlib, such as: . Path.readlines: same as with open(&#39;somefile&#39;, &#39;r&#39;) as f: f.readlines() | Path.read: same as with open(&#39;somefile&#39;, &#39;r&#39;) as f: f.read() | Path.save: saves file as pickle | Path.load: loads pickle file | Path.ls: shows the contents of the path as a list. | etc. | . Read more about this here. Here is a demonstration of ls: . from fastcore.utils import * from pathlib import Path p = Path(&#39;.&#39;) p.ls() # you don&#39;t get this with vanilla Pathlib.Path!! . (#7) [Path(&#39;2020-09-01-fastcore.ipynb&#39;),Path(&#39;README.md&#39;),Path(&#39;fastcore_imgs&#39;),Path(&#39;2020-02-20-test.ipynb&#39;),Path(&#39;.ipynb_checkpoints&#39;),Path(&#39;2020-02-21-introducing-fastpages.ipynb&#39;),Path(&#39;my_icons&#39;)] . Wait! What&#39;s going on here? We just imported pathlib.Path - why are we getting this new functionality? Thats because we imported the fastcore.utils module, which patches this module via the @patch decorator discussed earlier. Just to drive the point home on why the @patch decorator is useful, I&#39;ll go ahead and add another method to Path right now: . @patch def fun(self:Path): return &quot;This is fun!&quot; p.fun() . &#39;This is fun!&#39; . That is magical, right? I know! That&#39;s why I&#39;m writing about it! . . An Even More Concise Way To Create Lambdas . Self, with an uppercase S, is an even more concise way to create lambdas that are calling methods on an object. For example, let&#39;s create a lambda for taking the sum of a Numpy array: . arr=np.array([5,4,3,2,1]) f = lambda a: a.sum() assert f(arr) == 15 . You can use Self in the same way: . f = Self.sum() assert f(arr) == 15 . Let&#39;s create a lambda that does a groupby and max of a Pandas dataframe: . import pandas as pd df=pd.DataFrame({&#39;Some Column&#39;: [&#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, ], &#39;Another Column&#39;: [5, 7, 50, 70]}) f = Self.groupby(&#39;Some Column&#39;).mean() f(df) . Another Column . Some Column . a 6 | . b 60 | . Read more about Self in the docs). . . Notebook Functions . These are simple but handy, and allow you to know whether or not code is executing in a Jupyter Notebook, Colab, or an Ipython Shell: . from fastcore.imports import in_notebook, in_colab, in_ipython in_notebook(), in_colab(), in_ipython() . (True, False, True) . This is useful if you are displaying certain types of visualizations, progress bars or animations in your code that you may want to modify or toggle depending on the environment. . . A Drop-In Replacement For List . You might be pretty happy with Python&#39;s list. This is one of those situations that you don&#39;t know you needed a better list until someone showed one to you. Enter L, a list like object with many extra goodies. . The best way I can describe L is to pretend that list and numpy had a pretty baby: . define a list (check out the nice __repr__ that shows the length of the list!) . L(1,2,3) . (#3) [1,2,3] . Shuffle a list: . p = L.range(20).shuffle() p . (#20) [8,7,5,12,14,16,2,15,19,6...] . Index into a list: . p[2,4,6] . (#3) [5,14,2] . L has sensible defaults, for example appending an element to a list: . 1 + L(2,3,4) . (#4) [1,2,3,4] . There is much more L has to offer. Read the docs to learn more. . But Wait ... There&#39;s More! . There are more things I would like to show you about fastcore, but there is no way they would reasonably fit into a blog post. Here is a list of some of my favorite things that I didn&#39;t demo in this blog post: . Utilities . The Basics section contain many shortcuts to perform common tasks or provide an additional interface to what standard python provides. . mk_class: quickly add a bunch of attributes to a class | wrap_class: add new methods to a class with a simple decorator | groupby: similar to Scala&#39;s groupby | merge: merge dicts | fasttuple: a tuple on steroids | Infinite Lists: useful for padding and testing | chunked: for batching and organizing stuff | . Multiprocessing . The Multiprocessing section extends python&#39;s multiprocessing library by offering features like: . progress bars | ability to pause to mitigate race conditions with external services | processing things in batches on each worker, ex: if you have a vectorized operation to perform in chunks | . Functional Programming . The functional programming section is my favorite part of this library. . maps: a map that also composes functions | mapped: A more robust map | using_attr: compose a function that operates on an attribute | . Transforms . Transforms is a collection of utilities for creating data transformations and associated pipelines. These transformation utilities build upon many of the building blocks discussed in this blog post. . Further Reading . It should be noted that you should read the main page of the docs first, followed by the section on tests to fully understand the documentation. . The fastcore documentation site. | The fastcore GitHub repo. | Blog post on delegation. | . Shameless plug: fastpages . This blog post was written entirely in a Jupyter Notebook, which GitHub automatically converted into to a blog post! Sound interesting? Check out fastpages. .",
            "url": "https://resteche.github.io/REsteche_blog/fastcore/",
            "relUrl": "/fastcore/",
            "date": " • Sep 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hello again 👋, . In case you want to know more about my recent work, you can check my github here . I promiss I’ll do my best to keep it updated! . Remember you can contact me through my institutional or professional email anytime for doubts or sugestions. . Make sure you check out everything that interests you on my site again! I hope you like my content. .",
          "url": "https://resteche.github.io/REsteche_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://resteche.github.io/REsteche_blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}