{
  
    
        "post0": {
            "title": "Basic quantum mechanics: the Schrodinger equation",
            "content": "Introduction . Many of us know the main properties of the wave function, a probability amplitude of complex value that is responsible for describing the state of an isolated quantum system, as is also clear its mathematical origin, the Schrodinger equation. Despite dealing daily with this impressive relationship between physics and mathematics, this week I decided to take a little extra time to review its fundamentals and study its details again, since I only did this the first time I studied the fundamentals of quantum mechanics during my graduation. . Here, taking advantage of the opportunity given the advancement of my knowledge about MQ and different programming and visualization tools, I decided to produce a publication on this blog in order to share this knowledge in a different way to what is usually found on the internet, in a more constructive and visual way. . Time dependent Schr&#246;dinger equation . The idea we are going to use here is that: we want to describe an electron wavefunction by a wavepacket $ psi (x,y)$ that is a function of position $x$ and time $t$. We can than assume that the electron is initially localized around $x_0$, and following the uncertainty principle, we can model the system by a Gaussian multiplying a plane wave (wich we know as a phase in the wave function): . $$ psi(x,t=0)= exp{ left[- frac{1}{2} left( frac{x-x_0}{ sigma _0} right)^2 right ]} e^{ik_0x} $$As mentioned before, this wave function does not correspond to an electron with a well defined momentum. However, if the width of the Gaussian $ sigma _0$ is made very large, the electron gets spread over a sufficiently large region of space and can be considered as a plane wave with momentum $k_0$ with a slowly varying amplitude, that is, we are dealing with a system coherent with the ideas of Heisenberg. . The behavior of this wave packet as a function of time is described by the time-dependent Schröedinger equation (here in 1d): . $$i frac{ partial psi}{ partial t}=H psi(x,t).$$ . $H$ is the Hamiltonian operator: . $$H=- frac{1}{2m} frac{ partial ^2}{ partial x^2}+V(x),$$ . where $V(x)$ is a time independent potential. The Hamiltonian is chosen to be real. Note that we have picked the energy units such that $ hbar=1$, and from now on, we will pick mass units such that $2m=1$ only to make equations simpler. . Scrhödinger’s equation is obviously a P.D.E., we can then proceed through the usual methods known to solve this type of equation. The main observation is that this time we have to deal with complex numbers, and the function $ psi (x,y)$ has real and imaginary parts: . $$ psi (x,t) = R(x,t)+iI(x,t).$$ . However, is this section we will present an alternative method that makes the quantum mechanical nature of this problem more transparent. . The wave function properties . Before we proceed, I would like to list here the main properties of the wave function, which is effectively the solution of the Schrodinger equation defined previously. . Linearity: | . If $ Psi_1(x,t)$ is a solution for the Schrödinger equation and $ Psi_2(x,t)$ is also a solution then any linear combination $ Psi(x,t) = alpha Psi_1(x,t) + beta Psi_2(x,t)$ is a solution where $ alpha$ and $ beta in mathbb{C}$. . Unitarity: | . The wave function is unitary (for real potentials) which means it preservers the probability distribution through the evolution of time . $$ frac{d}{dt} int_{- infty}^{ infty} | Psi(x,t)|^2 dx = 0 $$ . Deterministic: | . If we have complete knowledge of the system at some moment of time $t_0$, $ Psi(x,t_0)$ we can determine unambiguously the wave function in all subsequent of time $ Psi(x,t)$. . The time-evolution operator . The Scrödinger equation presented in the previous section can be integrated in a formal sense to obtain: . $$ psi(x,t)=U(t) psi(x,t=0)=e^{-iHt} psi(x,t).$$ . From here we deduce that the wave function (which is unitary, then its norm is preserved during the following operation) can be evolved forward in time by applying the time-evolution operator $U(t)= exp{-iHt}$: . $$ psi(t+ Delta t)= e^{-iH delta t} psi(t).$$ . Likewise, the inverse of the time-evolution operator moves the wave function back in time: . $$ psi(t- Delta t)=e^{iH Delta t} psi(t),$$ . where we have use the property $$U^{-1}(t)=U(-t).$$ Although it would be nice to have an algorithm based on the direct application of $U$, it has been shown that this is not stable. Hence, we apply the following relation: . $$ psi(t+ Delta t)= psi(t- Delta t)+ left[e^{-iH Delta t}-e^{iH Delta t} right] psi(t).$$ . Now, the derivatives with recpect to $x$ can be approximated by . $$ begin{aligned} frac{ partial psi}{ partial t} &amp; sim&amp; frac{ psi(x,t+ Delta t)- psi(x, t)}{ Delta t}, frac{ partial ^2 psi}{ partial x^2} &amp; sim&amp; frac{ psi(x+ Delta % x,t)+ psi(x- Delta x,t)-2 psi(x,t)}{( Delta x)^2}. end{aligned}$$ . The time evolution operator is approximated by: . $$U( Delta t)=e^{-iH Delta t} sim 1+iH Delta t.$$ . Replacing the expression for $H$, we obtain: . $$ psi(x,t+ Delta t)= psi(x,t)-i[(2 alpha+ Delta t V(x)) psi(x,t)- alpha( psi(x+ Delta x,t)+ psi(x- Delta x,t))], $$ . with $ alpha= frac{ Delta t}{( Delta x)^2}$. The probability of finding an electron at $(x,t)$ is given as we know by the probability density of the wave funciton, which can be obtained by multiplying it by its complex conjugate, or as we well know $| psi(x,t)|^2$. This equations do not conserve this probability exactly, but the error is of the order of $( Delta t)^2$. Note that the convergence can be determined by using smaller steps. . We can write this expression explicitly for the real and imaginary parts, becoming: . $$ begin{aligned} mathrm{Im} psi(x, t + Delta t) = mathrm{Im} psi(x, t) + alpha mathrm{Re} psi (x + Delta x, t) + alpha mathrm{Re} psi(x − Delta x, t) − (2 alpha + Delta t V (x)) mathrm{Re} psi(x, t) mathrm{Re} psi(x, t + Delta t) = mathrm{Re} psi(x, t) − alpha mathrm{Im} psi (x + Delta x, t) − alpha mathrm{Im} psi (x − Delta x, t) + (2 alpha + Delta tV (x)) mathrm{Im} psi(x, t) end{aligned}$$Notice the symmetry between these equations: while the calculation of the imaginary part of the wave function at the later time involves a weighted average of the real part of the wave function at diferent positions from the earlier time, the calculation of the real part involves a weighted average of the imaginary part for di erent positions at the earlier time. This intermixing of the real and imaginary parts of the wave function may seem a bit strange, but remember that this situation is a direct result of our breaking up the wave function into its real and imaginary parts in the first place! . Experimenting in python: The Harmonic Potential . Now, let&#39;s try the exercise of animating the behavior of a Gaussian wave-packet moving along the $x$ axis under action of a harmonic potential! . %matplotlib inline import numpy as np from matplotlib import pyplot import math import matplotlib.animation as animation from IPython.display import HTML lx=20 dx = 0.04 nx = int(lx/dx) dt = dx**2/20. V0 = 60 alpha = dt/dx**2 fig = pyplot.figure() ax = pyplot.axes(xlim=(0, lx), ylim=(0, 2), xlabel=&#39;x&#39;, ylabel=&#39;|Psi|^2&#39;) points, = ax.plot([], [], marker=&#39;&#39;, linestyle=&#39;-&#39;, lw=3) psi0_r = np.zeros(nx+1) psi0_i = np.zeros(nx+1) x = np.arange(0, lx+dx, dx) #Define your potential V = np.zeros(nx+1) V = V0*(x-lx/2)**2 #Initial conditions: wave packet sigma2 = 0.5**2 k0 = np.pi*10.5 x0 = lx/2 for ix in range(0,nx): psi0_r[ix] = math.exp(-0.5*((ix*dx-x0)**2)/sigma2)*math.cos(k0*ix*dx) psi0_i[ix] = math.exp(-0.5*((ix*dx-x0)**2)/sigma2)*math.sin(k0*ix*dx) def solve(i): global psi0_r, psi0_i for ix in range(1,nx-2): psi0_i[ix]=psi0_i[ix]+alpha*psi0_r[ix+1]+alpha*psi0_r[ix-1]-(2.*alpha+dt*V[ix])*psi0_r[ix] for ix in range(1,nx-2): psi0_r[ix]=psi0_r[ix]-alpha*psi0_i[ix+1]-alpha*psi0_i[ix-1]+(2.*alpha+dt*V[ix])*psi0_i[ix] points.set_data(x,psi0_r**2 + psi0_i**2) return (points,) anim = animation.FuncAnimation(fig, solve, frames = 1000, interval=50) HTML(anim.to_jshtml()) . . Adding a potential barrier . Let&#39;s try to do it one more time, but now lets do a Gaussian wave-packet moving along the x axis passing through a potential barrier, which is a situation that i have to deal daily in my research works. To avoid tunneling (which a haven&#39;t mentioned in this post, so I&#39;m going to save it to another special post), we are going to set a really high potential compared to the energy of our wave, so most of the wave is going to be reflected by the potential . import matplotlib.animation as animation from IPython.display import HTML fig = pyplot.figure() ax = pyplot.axes(xlim=(0, lx), ylim=(0, 2), xlabel=&#39;x&#39;, ylabel=&#39;$| Psi|^2$&#39;) points, = ax.plot([], [], marker=&#39;&#39;, linestyle=&#39;-&#39;, lw=3) x0=6 for ix in range(0,nx): psi0_r[ix] = math.exp(-0.5*((ix*dx-x0)**2)/sigma2)*math.cos(k0*ix*dx) psi0_i[ix] = math.exp(-0.5*((ix*dx-x0)**2)/sigma2)*math.sin(k0*ix*dx) x = np.arange(0, lx+dx, dx) V = np.zeros(nx+1) for ix in range(nx//2-20,nx//2+20): V[ix]=2000. def solve(i): global psi0_r, psi0_i psi0_r[1:-1] = psi0_r[1:-1]- alpha*(psi0_i[2:]+psi0_i[:-2]-2*psi0_i[1:-1])+dt*V[1:-1]*psi0_i[1:-1] psi0_i[1:-1] = psi0_i[1:-1]+ alpha*(psi0_r[2:]+psi0_r[:-2]-2*psi0_r[1:-1])-dt*V[1:-1]*psi0_r[1:-1] points.set_data(x,psi0_r**2 + psi0_i**2) return points anim = animation.FuncAnimation(fig, solve, frames = 1000, interval=20) HTML(anim.to_jshtml()) . . The last challenge: Single-slit diffraction . Young’s single-slit experiment consists of a wave passing though a small slit, which causes the emerging wavelets to intefere with eachother forming a diffraction pattern. In quantum mechanics, where particles are represented by probabilities, and probabilities by wave packets, it means that the same phenomenon should occur when a particle (electron, neutron) passes though a small slit. Here, lets consider a wave packet of initial width 3 incident on a slit of width 5, and plot the probability density $| psi ^2|$ as the packet crosses the slit, while generalizing the time-evolution equation for 2 dimensions. The model of the slit with a potential wall is so that: . $$V(x,y)=100 , , , , , mathrm{for} , ,x=10,|y| geq 2.5.$$ . %matplotlib inline import numpy as np from matplotlib import pyplot import math lx = 20 #Box length in x ly = 20 #Box length in y dx = 0.25 #Incremental step size in x (Increased this to decrease the time of the sim) dy = dx #Incremental step size in y nx = int(lx/dx) #Number of steps in x ny = int(ly/dy) #Number of steps in y dt = dx**2/20. #Incremental step size in time sigma2 = 0.5**2 #Sigma2 Value k0 = np.pi*10.5 #K0 value amp = math.pow(1./2., 64) #Amplitude (to avoid large values out of range. This was one issue) alpha = (dt/2.)/dx**2 #Alpha psi0_r = np.zeros(shape=(ny+1,nx+1)) #Initialize real part of psi psi0_i = np.zeros(shape=(ny+1,nx+1)) #Initialize imaginary part of psi V = np.zeros(shape=(ny+1,nx+1)) #Initialize Potential #Define your potential wall V = np.zeros(shape=(ny+1,nx+1)) for ix in range(nx//2-20,nx//2+20): for iy in range(0,ny): if(abs(iy*dy-ly/2)&gt;2.5): V[iy,ix] = 200. #Initial conditions: wave packet x0 = 6. y0 = ly/2. for x in range(0,nx): for y in range(0,ny): psi0_r[y,x] = math.exp(-0.5*((x*dx-x0)**2+(y*dy-y0)**2)/sigma2)*math.cos(k0*x*dx) psi0_i[y,x] = math.exp(-0.5*((x*dx-x0)**2+(y*dy-y0)**2)/sigma2)*math.sin(k0*x*dx) x = np.arange(0, lx+dx, dx) y = np.arange(0, ly+dy, dy) X, Y = np.meshgrid(x, y) pyplot.contourf(X,Y,psi0_r**2+psi0_i**2) pyplot.contour(X,Y,V) #Function to solve incremental changes in psi def solve(): #Grab psi lists global psi0_r, psi0_i #Calculate Imaginary Part in all points except for last 2 (because of indice notation) for x in range(1,nx-2): for y in range(1,ny-2): psi0_i[y,x] = psi0_i[y,x] + alpha*psi0_r[y,x+1] + alpha*psi0_r[y,x-1] - (2*alpha + dt*V[y,x])*psi0_r[y,x] + alpha*psi0_r[y+1,x] + alpha*psi0_r[y-1,x] - (2*alpha+dt*V[y,x])*psi0_r[y,x] #Calculate Real Part in all points except for last 2 (because of indice notation) for x in range(1,nx-2): for y in range(1,ny-2): psi0_r[y,x] = psi0_r[y,x] - alpha*psi0_i[y,x+1] - alpha*psi0_i[y,x-1] + (2*alpha + dt*V[y,x])*psi0_i[y,x] - alpha*psi0_i[y+1,x] - alpha*psi0_i[y-1,x] + (2*alpha+dt*V[y,x])*psi0_i[y,x] for i in range(1000): solve() pyplot.contourf(X,Y,psi0_r**2+psi0_i**2); .",
            "url": "https://resteche.github.io/REsteche_blog/quantum%20mechanics/quantum%20foundations/schrodinger%20equation/2022/05/31/Schroedinger.html",
            "relUrl": "/quantum%20mechanics/quantum%20foundations/schrodinger%20equation/2022/05/31/Schroedinger.html",
            "date": " • May 31, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": " Quasiprobability functions using QuTip",
            "content": "%matplotlib inline import matplotlib.pylab as plt import matplotlib as mpl from mpl_toolkits.mplot3d import Axes3D from matplotlib import cm . import numpy as np . from qutip import * . Introduction . Hello again readers! My intention today is to talk a little more about quasi-probability distributions, some of their properties and their importance in my field, quantum mechanics. In case you still don&#39;t know what a quasi-probability distribution is, it is a mathematical object similar to a probability distribution, but which relaxes some of Kolmogorov&#39;s axioms of probability theory. They share several features with the ordinary distrbutions in wich we are used to, like the ability to yield expectation values with respect to the weights of the distribution - which is crucial to quantum mechanics as you probably already know. . They can however violate the $σ$-additivity axiom: integrating them over does not necessarily yield probabilities of mutually exclusive states. Indeed, quasiprobability distributions also counterintuitively have regions of negative probability density, contradicting the first axiom. One of the most useful quasiprobability distributions in my area of research is the Wigner quasiprobability distribution - which I intend to explain and show different ways of visualization and application through QuTip tools during this publication. . Definition of the Wigner function . The Wigner function or the Wigner–Ville distribution (named after Eugene Wigner and Jean-André Ville) is a famous quasiprobability distribution. It was introduced by Eugene Wigner in 1932 to study quantum corrections to classical statistical mechanics. The goal was to link the wavefunction that appears in Schrödinger&#39;s equation to a probability distribution in phase space. . We can see it as a generating function for all spatial autocorrelation functions of a given quantum-mechanical wavefunction $ psi(x)$. Thus, it maps on the quantum density matrix in the map between real phase-space functions and Hermitian operators. It was later rederived by Jean Ville in 1948 as a quadratic (in signal) representation of the local time-frequency energy of a signal, effectively a spectrogram, which makes it widely used in this area until today. . The formal defnition of the Wigner distribution $W(x,p)$ of a pure state is defined as: . $$ W(x,p)~{ stackrel { text{def}}{=}}~{ frac {1}{ pi hbar }} int _{- infty }^{ infty } psi ^{*}(x+y) psi (x-y)e^{2ipy/ hbar }dy $$ . where $ psi$ is the wavefunction, and $x$ and $p$ are position and momentum, but could be any conjugate variable pair (e.g. real and imaginary parts of the electric field or frequency and time of a signal). Note that it may have support in $x$ even in regions where $ psi$ has no support in $x$ (&quot;beats&quot;). . Just out of curiosity, I think it&#39;s important to point out that it is symmetric in $x$ and $p$ using the normalized momentum-space wave function $ varphi$, which is obtained by the Fourier transform os $ psi$. . $$ W(x,p)={ frac {1}{ pi hbar }} int _{- infty }^{ infty } varphi ^{*}(p+q) varphi (p-q)e^{-2ixq/ hbar }dq $$ . Visualizing the wigner function for superposition of fock states . Let $ |m rangle equiv {a^{ dagger m}}/{ sqrt{m!}}|0 rangle$ be the $m$-Fock state of a quantum harmonic oscillator. The wigner function associeted to this system where discovered by Groenewold (1946) in deminesionless variables as . $$ W_{|m rangle }(x,p)={ frac {(-1)^{m}}{ pi }}e^{-(x^{2}+p^{2})}L_{m}{ big (}2(p^{2}+x^{2}){ big )} $$ . Where $L_m (x)$ denotes the $m$-th Laguerre polynomial. . This may follow from the expression for the static eigenstate wavefunctions, . $$ u_{m}(x)= pi ^{-1/4}H_{m}(x)e^{-x^{2}/2}$$ . where $H_m$ is the $m$-th Hermite polynomial.From the above definition of the Wigner function, upon a change of integration variables, . $$ W_{|m rangle }(x,p)={ frac {(-1)^{m}}{ pi ^{3/2}2^{m}m!}}e^{-x^{2}-p^{2}} int _{- infty }^{ infty }d zeta ,e^{- zeta ^{2}}H_{m}( zeta -ip+x)H_{m}( zeta -ip-x) $$ . The expression then follows from the integral relation between Hermite and Laguerre polynomials. One can use the QuTip-wigner() function to make that definition visible like so: . x = 1.0 / np.sqrt(2) * (basis(10, 4) + basis(10, 2)) xvec = np.arange(-5, 5, 10.0 / 100) yvec = xvec W = wigner(x, xvec, yvec) cmap = wigner_cmap(W) X, Y = np.meshgrid(xvec, yvec) . fig = plt.figure(figsize=(8,6)) plt.contourf(X, Y, W, 50, cmap=cmap) plt.colorbar(); . Here, we use the same color system ideia of the last post to highlight the probability distribution in a 2D plot, but we can also add another axis to make it even more interesting and easy to comprehend . (also adding a legend for the ColorBar to make the chart more independent) . fig = plt.figure(figsize=(10,8)) ax = Axes3D(fig, azim=-30, elev=73) ax.plot_surface(X, Y, W, cmap=cmap, rstride=1, cstride=1, alpha=1, linewidth=0) ax.set_zlim3d(-0.25, 0.25) for a in ax.w_zaxis.get_ticklines() + ax.w_zaxis.get_ticklabels(): a.set_visible(False) nrm = mpl.colors.Normalize(W.min(), W.max()) cax, kw = mpl.colorbar.make_axes(ax, shrink=.66, pad=.02) cb1 = mpl.colorbar.ColorbarBase(cax, cmap=cmap, norm=nrm) cb1.set_label(&#39;Pseudoprobability&#39;) . Winger and Q-function for squeezed states . In quantum mechanics, we say that a squeezed coherent state is a quantum state that is usually described by two non-commuting observables having continuous spectra of eigenvalues. In other words, any state that can be described by just two of its canonically conjugated variables (take as an exemple the position $x$ and momentum $p$ of a particle) its a squeezed state. Logically, the product of the standard deviations of two such operators obeys the uncertainty principle. . The most general wave function that satisfies the relation mentioned is the squeezed coherent state (working with unit $ hbar = 1$) . $$ psi (x)=C exp left(-{ frac {(x-x_{0})^{2}}{2w_{0}^{2}}}+ip_{0}x right) $$ . where $C, x-0, omega_0, p_0$ are constants (a normalization constant, the center of the wavepacket, its width, and the expectation value of its momentum). The only new feature relative to a coherent state is the free value of the width $ omega_0$, which is indeed the reason why the state is called &quot;squeezed&quot;. The squeezed state above is an eigenstate of a linear operator . $${ hat x}+i{ hat p}w_{0}^{2} $$ . and the corresponding eigenvalue equals $x_{0}+ip_{0}w_{0}^{2}$. In this sense, it is a generalization of the ground state as well as the coherent state. Firt, lets define all the parameters of our squeezed state by doing: . N = 20 alpha = -1.0 # Coherent amplitude of field epsilon = 0.5j # Squeezing parameter a = destroy(N) D = (alpha * a.dag() - np.conj(alpha) * a).expm() # Displacement S = (0.5 * np.conj(epsilon) * a * a - 0.5 * epsilon * a.dag() * a.dag()).expm() # Squeezing psi = D * S * basis(N, 0) # Apply to vacuum state g = 2 . Wigner function of a squeezed state . For that and the next plot, lets try something different. The idea here is to generate the three-dimensional plot of a wigner function of a squeezed state. We will then proceed with this, and to increase our visualization of it, we will also generate two-dimensional plots in the planes formed by the intersections of the label axes: . xvec = np.arange(-40.,40.)*5./40 X,Y = np.meshgrid(xvec, xvec) W = wigner(psi, xvec, xvec) fig1 = plt.figure(figsize=(8,6)) ax = Axes3D(fig1) ax.plot_surface(X, Y, W, rstride=2, cstride=2, cmap=cm.jet, alpha=0.7) ax.contour(X, Y, W, 15,zdir=&#39;x&#39;, offset=-6) ax.contour(X, Y, W, 15,zdir=&#39;y&#39;, offset=6) ax.contour(X, Y, W, 15,zdir=&#39;z&#39;, offset=-0.3) ax.set_xlim3d(-6,6) ax.set_xlim3d(-6,6) ax.set_zlim3d(-0.3,0.4) plt.title(&#39;Wigner function of squeezed state&#39;); . The Q-function . Here, I want to recall that in quantum mechanics the Husimi Q representation, or Q-function in the context of quantum optics, was introduced by Kôdi Husimi in 1940. It is a quasiprobability distribution commonly used in quantum mechanics to represent the phase space distribution of a quantum state such as light in the phase space formulation. It is also one of the simplest distributions of quasiprobability in phase space because it&#39;s constructed in such a way that observables written in anti-normal order follow the optical equivalence theorem. This means that it is essentially the density matrix put into normal order. This makes it relatively easy to calculate compared to other quasiprobability distributions through the formula . $$ Q( alpha)= frac{1}{ pi} langle alpha| hat{ rho}| alpha rangle $$ . which is effectively a trace of the density matrix over the basis of coherent states $| alpha rangle $. It produces a pictorial representation of the state $ρ$ to illustrate several of its mathematical properties. Its relative ease of calculation is related to its smoothness compared to other quasiprobability distributions. In fact, it can be understood as the Weierstrass transform of the Wigner quasiprobability distribution, i.e. a smoothing by a Gaussian filter, . $$ Q( alpha)= frac{2}{ pi} int W( beta) e^{-2| alpha- beta|^2} , d^2 beta $$ . For a clear visualization of this distribution, we are going to use a submodule of the widget function in QuTip as it follows, . Q = qfunc(psi, xvec, xvec, g); fig2 = plt.figure(figsize=(8,6)) ax = Axes3D(fig2) ax.plot_surface(X, Y, Q, rstride=2, cstride=2, cmap=cm.jet, alpha=0.7) ax.contour(X, Y, Q,zdir=&#39;x&#39;, offset=-6) ax.contour(X, Y, Q,zdir=&#39;y&#39;, offset=6) ax.contour(X, Y, Q, 15,zdir=&#39;z&#39;, offset=-0.4) ax.set_xlim3d(-6,6) ax.set_xlim3d(-6,6) ax.set_zlim3d(-0.3,0.4) plt.title(&#39;Q function of squeezed state&#39;); . Schrodinger cat state . In quantum mechanics, the cat state, named after Schrödinger&#39;s cat, is a quantum state composed of two diametrically opposed conditions at the same time, such as the possibilities that a cat is alive and dead at the same time. . Generalizing Schrödinger&#39;s thought experiment, any other quantum superposition of two macroscopically distinct states is also referred to as a cat state. A cat state could be of one or more modes or particles, therefore it is not necessarily an entangled state. Such cat states have been experimentally realized in various ways and at various scales. . One can then imagine a cat state defined as the quantum superposition of two opposite-phase coherent states of a single optical mode (e.g., a quantum superposition of large positive electric field and large negative electric field) . $$ | mathrm {cat} _{e} rangle propto | alpha rangle +|{-} alpha rangle $$ . where . $$ | alpha rangle =e^{-{ frac {1}{2}}| alpha |^{2}} sum _{n=0}^{ infty }{ frac { alpha ^{n}}{ sqrt {n!}}}|n rangle $$ . and . $$ |{-} alpha rangle =e^{-{ frac {1}{2}}|{-} alpha |^{2}} sum _{n=0}^{ infty }{ frac {({-} alpha )^{n}}{ sqrt {n!}}}|n rangle $$ . are coherent states defined in the number (Fock) basis. Notice that if we add the two states together, the resulting cat state only contains even Fock state terms: . $$ | mathrm {cat} _{e} rangle propto 2e^{-{ frac {1}{2}}| alpha |^{2}} left({ frac { alpha ^{0}}{ sqrt {0!}}}|0 rangle +{ frac { alpha ^{2}}{ sqrt {2!}}}|2 rangle +{ frac { alpha ^{4}}{ sqrt {4!}}}|4 rangle + dots right) $$ . As a result of this property, the above cat state is often referred to as an even cat state. Alternatively, we can define an odd cat state as . $$| mathrm {cat} _{o} rangle propto | alpha rangle -|{-} alpha rangle $$ . which will only contains odd Fock states. We can also work with the ideia of a linear superposition of two coherent states with opposite phases to from a simple example of a cat state. When each state has the same weight: . $${ displaystyle { begin{aligned}| mathrm {cat} _{e} rangle &amp;={ frac {1}{ sqrt {2 left(1+e^{-2| alpha |^{2}} right)}}}{ big (}| alpha rangle +|{-} alpha rangle { big )}, | mathrm {cat} _{o} rangle &amp;={ frac {1}{ sqrt {2 left(1-e^{-2| alpha |^{2}} right)}}}{ big (}| alpha rangle -|{-} alpha rangle { big )}, | mathrm {cat} _{ theta } rangle &amp;={ frac {1}{ sqrt {2 left(1+ cos( theta )e^{-2| alpha |^{2}} right)}}}{ big (}| alpha rangle +e^{i theta }|{-} alpha rangle { big )}. end{aligned}}} $$ . The larger the value of $ alpha$, the lower the overlap between the two macroscopic classical coherent states $ text{exp}(−2 alpha^2)$, and the better it approaches an ideal cat state. However, the production of cat states with a large mean photon number $(= | alpha|^2)$ is difficult. A typical way to produce approximate cat states is through photon subtraction from a squeezed vacuum state, like the ones we saw in the previous section. This method usually is restricted to small values of $ alpha$, and such states have been referred to as Schrödinger &quot;kitten&quot; states in the literature. . Linear superposition of coherent states . N = 20; #amplitudes of coherent states alpha1=-2.0-2j alpha2=2.0+2j #define ladder oeprators a = destroy(N); #define displacement oeprators D1=(alpha1*dag(a)-np.conj(alpha1)*a).expm() D2=(alpha2*dag(a)-np.conj(alpha2)*a).expm() #sum of coherent states psi = np.sqrt(2)**-1*(D1+D2)*basis(N,0); # Apply to vacuum state . yvec = xvec = np.arange(-100.,100.)*5./100 g=2. W = wigner(psi, xvec, yvec) fig = plt.figure(figsize=(8,6)) c = plt.contourf(xvec, yvec, np.real(W), 100) plt.xlim([-5,5]) plt.ylim([-5,5]) plt.title(&#39;Wigner function of Schrodinger cat&#39;) cbar = plt.colorbar(c) cbar.ax.set_ylabel(&#39;Pseudoprobability&#39;); . And as we saw in the last session, we can once again use the Q function to smooth our plot of a quasi-probability distribution by: . Q = qfunc(psi,xvec,yvec) fig = plt.figure(figsize=(8,6)) qplt = plt.contourf(xvec, yvec, np.real(Q), 100) plt.xlim([-5,5]) plt.ylim([-5,5]) plt.title(&#39;Q function of Schrodinger cat&#39;) cbar = plt.colorbar(qplt) cbar.ax.set_ylabel(&#39;Probability&#39;); . More About . For more information about QuTiP see https://qutip.org/docs/4.0.2/ . Make sure you choose and run one of the installation paths specified in:https://qutip.org/docs/4.0.2/installation.html . For references of several of the subjects approched here, check also: . Quasiprobability functions | Wigner quasiprobability distribution | Cat state | .",
            "url": "https://resteche.github.io/REsteche_blog/probability/quantum%20mechanics/qutip/wigner/schrodinger%20cat/2022/04/25/pseudoprobabilityfunctions.html",
            "relUrl": "/probability/quantum%20mechanics/qutip/wigner/schrodinger%20cat/2022/04/25/pseudoprobabilityfunctions.html",
            "date": " • Apr 25, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Visualizing Quantum Mechanics through QuTip",
            "content": "%matplotlib inline . import matplotlib.pyplot as plt . import numpy as np from qutip import * . Introduction - The Color system . As many of you already know, quantum mechanics is a fundamental theory in physics that provides a description of the physical properties of nature at the scale of atoms and subatomic particles. In the scope of this theory, complex numbers are as natual as real numbers. And why is that? . A underlying feature of the theory is that it usually cannot predict with certainty what will happen, only probabilities. Mathematically, a probability is found by taking the square of the absolute value of a complex number, known as a probability amplitude. This is known as the Born rule, named after the renowned physicist Max Born. . Briefly mentioning then, my intention here will be to use tools from the QuTip python package that were developed specifically to assist in the visualization and construction of quantum systems in python. QuTiP is open-source software for simulating the dynamics of open quantum systems. The QuTiP library depends runs dependently on Numpy, Scipy, and Cython numerical packages. In addition, graphical output is provided by Matplotlib. . For a longer comment on coloring complex functions which comes from a more mathematical treatment of the situation, there is an IPython Notebook Visualizing complex-valued functions with Matplotlib and Mayavi that was made by Emilia Petrisor. . Before going into details of particular plots, we show how complex_array_to_rgb maps $z = x + i y$ into colors.There are two variants in this specific function, theme=&#39;light&#39; and theme=&#39;dark&#39;, that will only influence the background of the plots. For both, we use hue for phase, with red for positive numbers and aqua for negative. See how the view looks like: . compl_circ = np.array([[(x + 1j*y) if x**2 + y**2 &lt;= 1 else 0j for x in np.arange(-1,1,0.005)] for y in np.arange(-1,1,0.005)]) fig = plt.figure(figsize=(6, 3)) for i, theme in enumerate([&#39;light&#39;, &#39;dark&#39;]): ax = plt.subplot(1, 2, i + 1) ax.set_xlabel(&#39;x&#39;, fontsize=14) ax.set_ylabel(&#39;y&#39;, fontsize=14) ax.imshow(complex_array_to_rgb(compl_circ, rmax=1, theme=theme), extent=(-1,1,-1,1)) plt.tight_layout() . AS you may noticed, the RGB (Red, Green, Blue) is the basic color space. The basic geometrical model is the unit cube. Each color is represented as a triple $(r,g,b)$ of real numbers in $[0,1]$, where $r,g,b $ are quantities of the primary colors (red, green and blue) contained by that color. . . However in applications like ours it is much more appropriate to use the HSV color model, because colors in this space are specified in the same way the humans perceive the color, namely giving the hue, saturation and brightness, which are the variables in question addressed here. . . The Schmidt plot . Arguably, the easiest way to show entanglement is to plot a wavefunction against two variables. If the plot is a product of them, the state is a product state. If not - it is entangled. . As writing a wavefunction as a matrix $| psi rangle_{ij}$ is the the crucial step in Schmidt decomposition, we call such plots Schmidt plots. The motivation for this representation is quite direct from both its definitions, since a Schmidt plot is a polar plot where the angle indicates dip or drift direction and the distance from the origin indicates the dip or drift magnitude. . To make this something less abstract and more visual, let consider two simple states: . entangled: singlet state $| psi^- rangle = (|01 rangle - |10 rangle)/ sqrt{2}$, | product $(|01 rangle - |00 rangle)/ sqrt{2}$. | . They may look seamingly similar, but the later can be decomposed into a product $|0 rangle(|1 rangle - |0 rangle)/ sqrt{2}$. . singlet = (ket(&#39;01&#39;) - ket(&#39;10&#39;)).unit() separable = (ket(&#39;01&#39;) - ket(&#39;00&#39;)).unit() . plot_schmidt(singlet, figsize=(2,2)); . plot_schmidt(separable, figsize=(2,2)); . As we see, for separable state the plot is a product of x and y coordinates, while for the singlet state - is is not. in simple systems like the one in question this separation can be seen as trivial, however this visualization format is extremely useful when dealing with more complex systems . As an example, let us now consider a product of two singlet states: $| psi^- rangle| psi^- rangle$. Schmidt plot, by default, makes spliting of equal numbers of particles. . (Also, let&#39;s multiply it by the imaginary unit, to get diffeerent colors just for fun.) . plot_schmidt(1j * tensor([singlet, singlet]), figsize=(2,2)); . As we see, we have a product, as the state is a product state with the respect to the splitting of first 2 vs last 2 particles. . But what if we shift particles, getting $| psi^- rangle_{23}| psi^- rangle_{41}$? . plot_schmidt(1j * tensor([singlet, singlet]).permute([1,2,3,0]), figsize=(2,2)); . So we see that it is entangled. . plot_schmidt allows us to specify other splittings. With parameter splitting we decide how many particles we want to have as columns. In general, we can plot systems of various numbers of particles, each being of a different dimension. . For example: . plot_schmidt(1j * tensor([singlet, singlet]), splitting=1, labels_iteration=(1,3), figsize=(4,2)); . Qubism Images . As mentioned at the beginning, one of the topics to be discussed here will be the representation of qubism through QuTip. The qubism is a visualization scheme for quantum many-body wavefunctions. Its main property is its recursivity: increasing the number of qubits results in an increase in the image resolution. Thus, the plots are typically fractal. . &quot;And what&#39;s the point of that?&quot; - One may wonder . Many features of the wavefunction, such as magnetization, correlations and criticality, can be visualized as properties of the qubism generated images. In particular, factorizability can be easily spotted, and a way to estimate the entanglement entropy can be developed from this imaging method. . Let&#39;s start by arranging a system as it follows:The tensor basis is composed of four states: $|00〉$, $|01〉$, $|10〉$ and $|11〉$ . Consider also a unit square, $[0,1] × [0,1]$, and divide it into four &#39;level-1&#39; squares. . fig = plt.figure(figsize=(8, 4)) for i in [1, 2]: ax = plt.subplot(1, 2, i) plot_qubism(0 * ket(&#39;0000&#39;), legend_iteration=i, grid_iteration=i, fig=fig, ax=ax) . We can associate each of the basis states with one of the big squares with the basic mapping with all amplitudes for states starting with: . $|00 rangle$ go to the upper left quadrant, | $|01 rangle$ go to the upper right quadrant, | $|10 rangle$ go to the lower left quadrant, | $|11 rangle$ go to the lower right quadrant. | . And we proceed recursively with the next particles. So, for example: . state = ket(&#39;0010&#39;) + 0.5 * ket(&#39;1111&#39;) + 0.5j * ket(&#39;0101&#39;) - 1j * ket(&#39;1101&#39;) - 0.2 * ket(&#39;0110&#39;) plot_qubism(state, figsize=(4,4)); . Or if we want to make sure how did we map amplitudes to particular regions in the plot: . plot_qubism(state, legend_iteration=2, figsize=(4,4)); . Or how about making it dark? (E.g. to fit out slides with black background in your presentation). . plot_qubism(state, legend_iteration=2, theme=&#39;dark&#39;, figsize=(4,4)); . The most important property of Qubism is the recursive structure that I have mentioned to you before, so that we can add more particles seamlessly. For example, let&#39;s consider a plot of k copies of the singlet states, i.e. $| psi^- rangle^{ otimes k}$: . fig = plt.figure(figsize=(15, 3)) for k in range(1,6): ax = plt.subplot(1, 5, k) plot_qubism(tensor([singlet]*k), fig=fig, ax=ax) . OK, but once we can type the wavefunction by hand, plots offer little added value. . Let&#39;s see how we can plot ground states. Before doing that, we define some functions to easy make a translationally-invariant Hamiltonian. . def spinchainize(op, n, bc=&#39;periodic&#39;): if isinstance(op, list): return sum([spinchainize(each, n, bc=bc) for each in op]) k = len(op.dims[0]) d = op.dims[0][0] expanded = tensor([op] + [qeye(d)]*(n - k)) if bc == &#39;periodic&#39;: shifts = n elif bc == &#39;open&#39;: shifts = n - k + 1 shifteds = [expanded.permute([(i + j) % n for i in range(n)]) for j in range(shifts)] return sum(shifteds) def gs_of(ham): gval, gstate = ham.groundstate() return gstate . For example, let us consider Hamiltonian for $N$ particles, of the following form (a generalization of the Majumdar-Ghosh model): . $$H = sum_{i=1}^N vec{S}_i cdot vec{S}_{i+1} + J sum_{i=1}^N vec{S}_i cdot vec{S}_{i+2},$$ . where $ vec{S}_i = tfrac{1}{2} ( sigma^x, sigma^y, sigma^z)$ is the spin operator (with sigmas being Pauli matrices). . Moreover, we can set two different boundary conditions: . periodic - spin chain forms a loop ($N+1 equiv 1$ and $N+2 equiv 2$), | open - spin chain forms a line (we remove terms with $N+1$ and $N+2$). | . heis = sum([tensor([pauli]*2) for pauli in [sigmax(), sigmay(), sigmaz()]]) heis2 = sum([tensor([pauli, qeye(2), pauli]) for pauli in [sigmax(), sigmay(), sigmaz()]]) N = 10 Js = [0., 0.5, 1.] fig = plt.figure(figsize=(2*len(Js), 4.4)) for b in [0, 1]: for k, J in enumerate(Js): ax = plt.subplot(2, len(Js), b*len(Js) + k + 1) if b == 0: spinchain = spinchainize([heis, J*heis2], N, bc=&#39;periodic&#39;) elif b ==1: spinchain = spinchainize([heis, J*heis2], N, bc=&#39;open&#39;) plot_qubism(gs_of(spinchain), ax=ax) if k == 0: if b == 0: ax.set_ylabel(&quot;periodic BC&quot;, fontsize=16) else: ax.set_ylabel(&quot;open BC&quot;, fontsize=16) if b == 1: ax.set_xlabel(&quot;$J={0:.1f}$&quot;.format(J), fontsize=16) plt.tight_layout() . We are not restricted to qubits. We can have it for other dimensions, e.g. qutrits. . Let us consider AKLT model for spin-1 particles: . $$H = sum_{i=1}^N vec{S}_i cdot vec{S}_{i+1} + tfrac{1}{3} sum_{i=1}^N ( vec{S}_i cdot vec{S}_{i+1})^2.$$ . where $ vec{S}_i$ is spin operator for spin-1 particles (or for qutip: jmat(1, &#39;x&#39;), jmat(1, &#39;y&#39;) and jmat(1, &#39;z&#39;)). . ss = sum([tensor([jmat(1, s)]*2) for s in [&#39;x&#39;, &#39;y&#39;, &#39;z&#39;]]) H = spinchainize([ss, (1./3.) * ss**2], n=6, bc=&#39;periodic&#39;) plot_qubism(gs_of(H), figsize=(4,4)); . Note that qubism for qutrits works similarly as for qubits; you can visualize the base of the system once again characterizing it as: . fig = plt.figure(figsize=(10, 5)) for i in [1, 2]: ax = plt.subplot(1, 2, i) plot_qubism(0 * ket(&#39;0000&#39;, dim=3), legend_iteration=i, grid_iteration=i, fig=fig, ax=ax) . Just in this case we interpret: . 0 as $s_z=-1$, | 1 as $s_z= 0$, | 2 as $s_z=+1$. | . While qubism works best for translationally-invariants states (so in particular, all particles need to have the same dimension), we can do it for others too. . Also, there are a few other Qubism-related plotting schemes. For example how=&#39;pairs_skewed&#39;: . fig = plt.figure(figsize=(8, 4)) for i in [1, 2]: ax = plt.subplot(1, 2, i) plot_qubism(0 * ket(&#39;0000&#39;), how=&#39;pairs_skewed&#39;, legend_iteration=i, grid_iteration=i, fig=fig, ax=ax) . The one above emphasis ferromagnetic (put on the left) vs antiferromagnetic (put on the right) states, which can obviously be particularly useful for performing electromagnetic analysis of systems. . Another one how=&#39;before_after&#39; (inspired by this - Ising tartan model) works in a bit different way: it uses typical recursion, but starting from middle particles. For example, the top left quadrant correspons to $|00 rangle_{N/2,N/2+1}$: . fig = plt.figure(figsize=(8, 4)) for i in [1, 2]: ax = plt.subplot(1, 2, i) plot_qubism(0 * ket(&#39;0000&#39;), how=&#39;before_after&#39;, legend_iteration=i, grid_iteration=i, fig=fig, ax=ax) . It is very similar to the Schmidt plot (for the default splitting), with the only difference being ordering of the y axis (particle order is reversed). All entanglement properties are the same. . So how does it work on the same example? Well, let us take spin chain for (Majumdar-Ghosh model for $J=0$), i.e. $$H = sum_{i=1}^N vec{S}_i cdot vec{S}_{i+1}$$ . for qubits. Plotting then a comparison of all the models that we explain just before: . heis = sum([tensor([pauli]*2) for pauli in [sigmax(), sigmay(), sigmaz()]]) N = 10 gs = gs_of(spinchainize(heis, N, bc=&#39;periodic&#39;)) fig = plt.figure(figsize=(12, 4)) for i, how in enumerate([&#39;schmidt_plot&#39;, &#39;pairs&#39;, &#39;pairs_skewed&#39;, &#39;before_after&#39;]): ax = plt.subplot(1, 4, i + 1) if how == &#39;schmidt_plot&#39;: plot_schmidt(gs, fig=fig, ax=ax) else: plot_qubism(gs, how=how, fig=fig, ax=ax) ax.set_title(how) plt.tight_layout() . Analyzing entanglement . As we have already dealt with here, entanglement is the physical phenomenon that occurs when a group of particles are generated, interact, or share spatial proximity in a way such that the quantum state of each particle of the group cannot be described independently of the state of the others, including when the particles are separated by a large distance. . To better analyze this property of particles and systems using what we have seen so far, we will define a series of different partitions as: . product_1 = ket(&#39;0000&#39;) product_2 = tensor([(ket(&#39;0&#39;) + ket(&#39;1&#39;)).unit()]*4) w = (ket(&#39;0001&#39;) + ket(&#39;0010&#39;) + ket(&#39;0100&#39;) + ket(&#39;1000&#39;)).unit() dicke_2of4 = (ket(&#39;0011&#39;) + ket(&#39;0101&#39;) + ket(&#39;0110&#39;) + ket(&#39;1001&#39;) + ket(&#39;1010&#39;) + ket(&#39;1100&#39;)).unit() ghz = (ket(&#39;0000&#39;) + ket(&#39;1111&#39;)).unit() . states = [&#39;product_1&#39;, &#39;product_2&#39;, &#39;w&#39;, &#39;dicke_2of4&#39;, &#39;ghz&#39;] fig = plt.figure(figsize=(2 * len(states), 2)) for i, state_str in enumerate(states): ax = plt.subplot(1, len(states), i + 1) plot_qubism(eval(state_str), fig=fig, ax=ax) ax.set_title(state_str) plt.tight_layout() . Then entanglement (or exactly: Schmidt rank) for a given partition is equal to number to different, non-zero squares. (We don&#39;t allow rotations, we do allow multiplication by a factor and, what may be more tricky, linear superposition.) . Here we use partition of first 2 particles vs last 2, as indicated by lines. . That is, . product_1 - only 1 non-zero square: Schmidt rank 1, | product_2 - 4 non-zero squares, but they are the same: Schmidt rank 1, | w - 3 non-zero quares, but two of them are the same: Schmidt rank 2, | dicke_2of4 - 4 non-zero squares, but two of them are the same: Schmidt rank 3, | ghz - 2 non-zero squares, each one different: Schmidt rank 2. | . This is basis-independent, but it may be easier to work in one basis rather than another. . And for a comparision, let us see product states: . $$ left( cos( theta/2) |0 rangle + sin( theta/2) e^{i varphi} |1 rangle right)^N $$ . def product_state(theta, phi=0, n=1): single = Qobj([[np.cos(theta/2.)], [np.sin(theta/2.) * np.exp(1j*phi)]]) return tensor([single]*n) thetas = 0.5 * np.pi * np.array([0., 0.5, 0.75, 1.]) phis = np.pi * np.array([0., 0.1, 0.2, 0.3]) fig, axes2d = plt.subplots(nrows=len(phis), ncols=len(thetas), figsize=(6,6)) for i, row in enumerate(axes2d): for j, cell in enumerate(row): plot_qubism(product_state(thetas[j], phi=phis[i], n=8), grid_iteration=1, ax=cell) if i == len(axes2d) - 1: cell.set_xlabel(&quot;$ theta={0:s} pi$&quot;.format([&quot;0&quot;, &quot;(1/4)&quot;, &quot;(3/8)&quot;, &quot;(1/2)&quot;][j]), fontsize=16) if j == 0: cell.set_ylabel(&quot;$ varphi={0:.1f} pi$&quot;.format(phis[i] / np.pi), fontsize=16) plt.tight_layout() . In each plot squares are the same, up to a factor (which is visualized as intensity and hue). . You can lookup previous plots. Setting grid_iteration=2 would show splitting of the first 4 particles vs N-4 others. And for how=&#39;before_after&#39; it is the middle particles vs all others. . More about . For more information about QuTiP: . http://qutip.org. | . For more information about Qubism: . J. Rodriguez-Laguna, P. Migdał, M. Ibanez Berganza, M. Lewenstein, G. Sierra, Qubism: self-similar visualization of many-body wavefunctions, New J. Phys. 14 053028 (2012), arXiv:1112.3560, | its video abstract, | C++/Mathematica codes for qubism on GitHub . | .",
            "url": "https://resteche.github.io/REsteche_blog/plot_schmidt/plot_qubism/complex_array_to_rgb/entanglement/2022/04/24/qubism.html",
            "relUrl": "/plot_schmidt/plot_qubism/complex_array_to_rgb/entanglement/2022/04/24/qubism.html",
            "date": " • Apr 24, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Using Machine Learning principles to predict cryptocurrency prices",
            "content": "Introduction . Cryptocurrency is a digital currency designed to work as a medium of exchange through a computer network (usually through a blockchain, but we&#39;ll talk about that later) that is not reliant on any central authority, such as a government or bank, to uphold or maintain it. The main idea is to have a currency/business medium that works in a encrypted and decentralized way. Many people use cryptocurrencies as a form of investing because it gives great returns even in a short period,among the popular cryptocurrencies today we have Bitcoin, Ethereum, and Binance Coin, as the best known examples. . Our main goal in this notebook project will be to use machine learning principles to predict the price of cryptocurrencies based on a deep analysis of an external and independent database, which is one of the popular case studies in the data science community. The prices of stocks and cryptocurrencies don’t just depend on the number of people who buy or sell them. Today, the change in the prices of these investments also depends on the changes in the financial policies of the government regarding any cryptocurrency. The feelings of people towards a particular cryptocurrency or personality who directly or indirectly endorse a cryptocurrency also result in a huge buying and selling of a particular cryptocurrency, resulting in a change in prices. . In short, buying and selling result in a change in the price of any cryptocurrency, but buying and selling trends depend on many factors. Using machine learning for cryptocurrency price prediction can only work in situations where prices change due to historical prices that people see before buying and selling their cryptocurrency. Here, let&#39;s see how you can predict the bitcoin prices (which is one of the most popular cryptocurrencies) for the next 30 days. . Data collect . We will start the task of Cryptocurrency price prediction by importing the necessary Python libraries and the dataset we need. For this task, lets collect the latest Bitcoin prices data from Yahoo Finance, using the latest yfinance API. For that, you can proceed by using the following pip comand . pip install yfinance==0.1.70 . To help you collect the latest data each time you run this code, you can run the following commands and print the head of the table data to visualize if everything is working properly . import pandas as pd import yfinance as yf import datetime from datetime import date, timedelta today = date.today() d1 = today.strftime(&quot;%Y-%m-%d&quot;) end_date = d1 d2 = date.today() - timedelta(days=730) d2 = d2.strftime(&quot;%Y-%m-%d&quot;) start_date = d2 data = yf.download(&#39;BTC-USD&#39;, start=start_date, end=end_date, progress=False) data[&quot;Date&quot;] = data.index data = data[[&quot;Date&quot;, &quot;Open&quot;, &quot;High&quot;, &quot;Low&quot;, &quot;Close&quot;, &quot;Adj Close&quot;, &quot;Volume&quot;]] data.reset_index(drop=True, inplace=True) print(data.head()) . Date Open High Low Close Adj Close 0 2020-04-21 6879.784180 6934.551758 6834.442383 6880.323242 6880.323242 1 2020-04-22 6879.440430 7145.865723 6867.781738 7117.207520 7117.207520 2 2020-04-23 7121.306152 7491.785156 7081.594727 7429.724609 7429.724609 3 2020-04-24 7434.181641 7574.195801 7434.181641 7550.900879 7550.900879 4 2020-04-25 7550.482910 7641.363770 7521.672363 7569.936035 7569.936035 Volume 0 32589741511 1 33249153866 2 43500782316 3 34636526286 4 32941541447 . There, we have collected the latest data of Bitcoin prices for the past 730 days, and then prepared it for any data science task. Now, let’s have a look at the shape of this dataset to see if we are working with 730 rows or not: . data.shape . (731, 7) . So the dataset contains 731 rows, where the first row contains the names of each column. Now let’s visualize the change in bitcoin prices till today by using a candlestick chart provided by the plotly pyhton library ( which again can be installed in case you don&#39;t use it yet by pip install plotly ) . import plotly.graph_objects as go figure = go.Figure(data=[go.Candlestick(x=data[&quot;Date&quot;], open=data[&quot;Open&quot;], high=data[&quot;High&quot;], low=data[&quot;Low&quot;], close=data[&quot;Close&quot;])]) figure.update_layout(title = &quot;Bitcoin Price Analysis&quot;, xaxis_rangeslider_visible=False) figure.show() . . In case this plot doesn&#39;t render in the blog, you can view it in detail in my github later! So, the Close column in the dataset contains the values we need to predict. So, let’s have a look at the correlation of all the columns in the data concerning the Close column . correlation = data.corr() print(correlation[&quot;Close&quot;].sort_values(ascending=False)) . Close 1.000000 Adj Close 1.000000 High 0.998517 Low 0.998208 Open 0.996582 Volume 0.280780 Name: Close, dtype: float64 . The Price Prediction Model . To predict the behavior of such a sensitive system using only previous statistical data, we need a mathematical model that is efficient and robust enough to convey confidence in our results. For our analysis in question, the future prices of cryptocurrency can be based on the problem of Time series analysis. The AutoTS library in Python is one of the best libraries for time series analysis, designed for rapidly deploying high-accuracy forecasts at scale. So here I will be using the AutoTS library to predict the bitcoin prices for the next 30 days . from autots import AutoTS model = AutoTS( forecast_length=30, frequency=&#39;infer&#39;, ensemble=&#39;simple&#39;, model_list=&quot;default&quot; # &quot;fast&quot;, &quot;superfast&quot;, &quot;fast_parallel&quot; ) model = model.fit( data, date_col=&#39;Date&#39;, value_col=&#39;Close&#39;, id_col=None ) prediction = model.predict() forecast = prediction.forecast print(forecast) . After a good run time to analyze all the data we embedded, the final result of the model will return something like the following image format (result of my execution on 04/21/2022) . . But always keep in mind: Buying and selling result in a change in the price of any cryptocurrency, but buying and selling trends depend on many factors! Using machine learning for cryptocurrency price prediction can only work in situations where prices change due to historical pr ices that people see before buying and selling their cryptocurrency which is an educational assumption that we took in this project with the aim of achieving a better understanding of the tools used here . Keep in mind that we can also speed up our process by selecting more efficient/simplified ways of learning within the autoTS library, as I suggested in the code comment above. Just by changing the learning mode from &quot;default&quot; to &quot;fast&quot;, we see an absurd addition of functionality in terms of training time and plotted result, but with a large variation in the outcome results, as can be seen below . . Last tips for Speed and Large Data . Use appropriate model lists, especially the predefined lists: . superfast (simple naive models) and fast (more complex but still faster models, optimized for many series) | fast_parallel (a combination of fast and parallel) or parallel, given many CPU cores are available | see a dict of predefined lists (some defined for internal use) with from autots.models.model_list import model_lists | . | Use the subset parameter when there are many similar series, subset=100 will often generalize well for tens of thousands of similar series. . If using subset, passing weights for series will weight subset selection towards higher priority series. | if limited by RAM, it can be distributed by running multiple instances of AutoTS on different batches of data, having first imported a template pretrained as a starting point for all. | . | For datasets with many records, upsampling (for example, from daily to monthly frequency forecasts) can reduce training time if appropriate. . this can be done by adjusting frequency and aggfunc but is probably best done before passing data into AutoTS. | . | . For even more details, check out the documentation of the librarie .",
            "url": "https://resteche.github.io/REsteche_blog/deep%20learning/pytorch/data%20science/neural%20network/crypto/2022/03/28/crypto.html",
            "relUrl": "/deep%20learning/pytorch/data%20science/neural%20network/crypto/2022/03/28/crypto.html",
            "date": " • Mar 28, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Text Classifier Network",
            "content": "introduction . The main goal of this projesct is to develop a deep learning project using Pytorch and Fast.ai. After PyTorch came out in 2017 it has become the world&#39;s fastest-growing deep learning library and is already used for most research papers at top conferences. This is generally a leading indicator of usage in industry, because these are the papers that end up getting used in products and services commercially. PyTorch is considered nowadays the most flexible and expressive library for deep learning. It does not trade off speed for simplicity, but provides both. . PyTorch works best as a low-level foundation library, providing the basic operations for higher-level functionality, the fast.ai library is the most popular library for adding this higher-level functionality on top of PyTorch (there&#39;s even a peer-reviewed academic paper about this layered API) and bearing in mind these reasons, these were the tools that I found most appropriate for this project. . Here, we will see how we can train a model to classify text (here based on their sentiment). First we will see how to do this quickly in a few lines of code, then how to get state-of-the art results using the approach of the ULMFit paper, using the IMDb dataset from the paper Learning Word Vectors for Sentiment Analysis, containing a few thousand movie reviews. In addition , I am planning to add later also a mid-level API for data collection fromn the wikitext dataset, inspired by the teaching tutorial of fast.ai from github. . Installation of the packages . First, it is important to clarify that you can use fastai without any installation by using Google Colab.If you want to run things locally, you can install fastai on your own machines with conda (highly recommended), as long as you&#39;re running Linux or Windows (NB: Mac is not supported). . to proceed with installation in Anaconda, run the following command line: . conda install -c fastchan fastai anaconda . To install with pip, use: pip install fastai. If you install with pip, you should install PyTorch first by following the PyTorch installation instructions, which for my setup design consists on only running the following command line: . conda install pytorch torchvision torchaudio cpuonly -c pytorch . but you can use the mentioned website to configure your installation command line according to your setup settings. . Train a text classifier from a pretrained model . Here, we will try to train a classifier using a pretrained model. To get our data ready, we will first use the high-level API. First, we can download the data and decompress it with the following command: . from fastai.text.all import * path = untar_data(URLs.IMDB) path.ls() . (#5) [Path(&#39;/home/sgugger/.fastai/data/imdb/unsup&#39;),Path(&#39;/home/sgugger/.fastai/data/imdb/models&#39;),Path(&#39;/home/sgugger/.fastai/data/imdb/train&#39;),Path(&#39;/home/sgugger/.fastai/data/imdb/test&#39;),Path(&#39;/home/sgugger/.fastai/data/imdb/README&#39;)] . (path/&#39;train&#39;).ls() . (#4) [Path(&#39;/home/sgugger/.fastai/data/imdb/train/pos&#39;),Path(&#39;/home/sgugger/.fastai/data/imdb/train/unsupBow.feat&#39;),Path(&#39;/home/sgugger/.fastai/data/imdb/train/labeledBow.feat&#39;),Path(&#39;/home/sgugger/.fastai/data/imdb/train/neg&#39;)] . The data follows an ImageNet-style organization, in the train folder, we have two subfolders, pos and neg (for positive reviews and negative reviews). We can gather it by using the TextDataLoaders.from_folder method. The only thing we need to specify is the name of the validation folder, which is &quot;test&quot; (and not the default &quot;valid&quot;). . dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid=&#39;test&#39;) . We can then have a look at the data with the show_batch method: . dls.show_batch() . text category . 0 xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules of the match , both opponents have to go through tables in order to get the win . xxmaj benoit and xxmaj guerrero heated up early on by taking turns hammering first xxmaj spike and then xxmaj bubba xxmaj ray . a xxmaj german xxunk by xxmaj benoit to xxmaj bubba took the wind out of the xxmaj dudley brother . xxmaj spike tried to help his brother , but the referee restrained him while xxmaj benoit and xxmaj guerrero | pos | . 1 xxbos xxmaj warning : xxmaj does contain spoilers . n n xxmaj open xxmaj your xxmaj eyes n n xxmaj if you have not seen this film and plan on doing so , just stop reading here and take my word for it . xxmaj you have to see this film . i have seen it four times so far and i still have n&#39;t made up my mind as to what exactly happened in the film . xxmaj that is all i am going to say because if you have not seen this film , then stop reading right now . n n xxmaj if you are still reading then i am going to pose some questions to you and maybe if anyone has any answers you can email me and let me know what you think . n n i remember my xxmaj grade 11 xxmaj english teacher quite well . xxmaj | pos | . 2 xxbos i thought that xxup rotj was clearly the best out of the three xxmaj star xxmaj wars movies . i find it surprising that xxup rotj is considered the weakest installment in the xxmaj trilogy by many who have voted . xxmaj to me it seemed like xxup rotj was the best because it had the most profound plot , the most suspense , surprises , most xxunk the ending ) and definitely the most episodic movie . i personally like the xxmaj empire xxmaj strikes xxmaj back a lot also but i think it is slightly less good than than xxup rotj since it was slower - moving , was not as episodic , and i just did not feel as much suspense or emotion as i did with the third movie . n n xxmaj it also seems like to me that after reading these surprising reviews that | pos | . 3 xxbos xxup the xxup shop xxup around xxup the xxup corner is one of the sweetest and most feel - good romantic comedies ever made . xxmaj there &#39;s just no getting around that , and it &#39;s hard to actually put one &#39;s feeling for this film into words . xxmaj it &#39;s not one of those films that tries too hard , nor does it come up with the oddest possible scenarios to get the two protagonists together in the end . xxmaj in fact , all its charm is innate , contained within the characters and the setting and the plot … which is highly believable to boot . xxmaj it &#39;s easy to think that such a love story , as beautiful as any other ever told , * could * happen to you … a feeling you do n&#39;t often get from other romantic comedies | pos | . 4 xxbos xxmaj the premise of this movie has been tickling my imagination for quite some time now . xxmaj we &#39;ve all heard or read about it in some kind of con - text . xxmaj what would you do if you were all alone in the world ? xxmaj what would you do if the entire world suddenly disappeared in front of your eyes ? xxmaj in fact , the last part is actually what happens to xxmaj dave and xxmaj andrew , two room - mates living in a run - down house in the middle of a freeway system . xxmaj andrew is a nervous wreck to say the least and xxmaj dave is considered being one of the biggest losers of society . xxmaj that alone is the main reason to why these two guys get so well along , because they simply only have each | pos | . 5 xxbos xxrep 3 * xxup spoilers xxrep 3 * xxrep 3 * xxup spoilers xxrep 3 * xxmaj continued … n n xxmaj from here on in the whole movie collapses in on itself . xxmaj first we meet a rogue program with the indication we &#39;re gon na get ghosts and vampires and werewolves and the like . xxmaj we get a guy with a retarded accent talking endless garbage , two &#39; ghosts &#39; that serve no real purpose and have no character what - so - ever and a bunch of henchmen . xxmaj someone &#39;s told me they &#39;re vampires ( straight out of xxmaj blade 2 ) , but they &#39;re so undefined i did n&#39;t realise . n n xxmaj the funny accented guy with a ridiculous name suffers the same problem as the xxmaj oracle , only for far longer and far far worse . | neg | . 6 xxbos xxmaj i &#39;ve rented and watched this movie for the 1st time on xxup dvd without reading any reviews about it . xxmaj so , after 15 minutes of watching xxmaj i &#39;ve noticed that something is wrong with this movie ; it &#39;s xxup terrible ! i mean , in the trailers it looked scary and serious ! n n i think that xxmaj eli xxmaj roth ( mr . xxmaj director ) thought that if all the characters in this film were stupid , the movie would be funny … ( so stupid , it &#39;s funny … ? xxup wrong ! ) xxmaj he should watch and learn from better horror - comedies such xxunk xxmaj night &quot; , &quot; the xxmaj lost xxmaj boys &quot; and &quot; the xxmaj return xxmaj of the xxmaj living xxmaj dead &quot; ! xxmaj those are funny ! n n &quot; | neg | . 7 xxbos xxup myra xxup breckinridge is one of those rare films that established its place in film history immediately . xxmaj praise for the film was absolutely nonexistent , even from the people involved in making it . xxmaj this film was loathed from day one . xxmaj while every now and then one will come across some maverick who will praise the film on philosophical grounds ( aggressive feminism or the courage to tackle the issue of xxunk ) , the film has not developed a cult following like some notorious flops do . xxmaj it &#39;s not hailed as a misunderstood masterpiece like xxup scarface , or trotted out to be ridiculed as a camp classic like xxup showgirls . n n xxmaj undoubtedly the reason is that the film , though outrageously awful , is not lovable , or even likable . xxup myra xxup breckinridge is just | neg | . 8 xxbos xxmaj after reading the previous comments , xxmaj i &#39;m just glad that i was n&#39;t the only person left confused , especially by the last 20 minutes . xxmaj john xxmaj carradine is shown twice walking down into a grave and pulling the lid shut after him . i anxiously awaited some kind of explanation for this odd behavior … naturally i assumed he had something to do with the evil goings - on at the house , but since he got killed off by the first rising corpse ( hereafter referred to as xxmaj zombie # 1 ) , these scenes made absolutely no sense . xxmaj please , if someone out there knows why xxmaj carradine kept climbing down into graves -- let the rest of us in on it ! ! n n xxmaj all the action is confined to the last 20 minutes so xxmaj | neg | . We can see that the library automatically processed all the texts to split then in tokens, adding some special tokens like: . xxbos to indicate the beginning of a text | xxmaj to indicate the next word was capitalized | . Then, we can define a Learner suitable for text classification in one line . learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy) . Here, we have used the AWD LSTM architecture, drop_mult is a parameter that controls the magnitude of all dropouts in that model, and we use accuracy to track down how well we are doing. We can then fine-tune our pretrained model as can be seen below . learn.fine_tune(4, 1e-2) . epoch train_loss valid_loss accuracy time . 0 | 0.587251 | 0.386230 | 0.828960 | 01:35 | . epoch train_loss valid_loss accuracy time . 0 | 0.307347 | 0.263843 | 0.892800 | 03:03 | . 1 | 0.215867 | 0.226208 | 0.911800 | 02:55 | . 2 | 0.155399 | 0.231144 | 0.913960 | 03:12 | . 3 | 0.129277 | 0.200941 | 0.925920 | 03:01 | . As you can see from the progress of the last line, our model is doing pretty good until now! In order to quantize through objective criteria how well the model is performing, we can use the show_results method as it follows . learn.show_results() . text category category_ . 0 xxbos xxmaj there &#39;s a sign on xxmaj the xxmaj lost xxmaj highway that says : n n * major xxup spoilers xxup ahead * n n ( but you already knew that , did n&#39;t you ? ) n n xxmaj since there &#39;s a great deal of people that apparently did not get the point of this movie , xxmaj i &#39;d like to contribute my interpretation of why the plot makes perfect sense . xxmaj as others have pointed out , one single viewing of this movie is not sufficient . xxmaj if you have the xxup dvd of xxup md , you can &quot; cheat &quot; by looking at xxmaj david xxmaj lynch &#39;s &quot; top 10 xxmaj hints to xxmaj unlocking xxup md &quot; ( but only upon second or third viewing , please . ) ;) n n xxmaj first of all , xxmaj mulholland xxmaj drive is | pos | pos | . 1 xxbos ( some spoilers included : ) n n xxmaj although , many commentators have called this film surreal , the term fits poorly here . xxmaj to quote from xxmaj encyclopedia xxmaj xxunk &#39;s , surreal means : n n &quot; fantastic or incongruous imagery &quot; : xxmaj one need n&#39;t explain to the unimaginative how many ways a plucky ten - year - old boy at large and seeking his fortune in the driver &#39;s seat of a red xxmaj mustang could be fantastic : those curious might read xxmaj james xxmaj kincaid ; but if you asked said lad how he were incongruous behind the wheel of a sports car , he &#39;d surely protest , &quot; no way ! &quot; xxmaj what fantasies and incongruities the film offers mostly appear within the first fifteen minutes . xxmaj thereafter we get more iterations of the same , in an | pos | neg | . 2 xxbos xxmaj hearkening back to those &quot; good xxmaj old xxmaj days &quot; of 1971 , we can vividly recall when we were treated with a whole xxmaj season of xxmaj charles xxmaj chaplin at the xxmaj cinema . xxmaj that &#39;s what the promotional guy called it when we saw him on somebody &#39;s old talk show . ( we ca n&#39;t recall just whose it was ; either xxup merv xxup griffin or xxup woody xxup woodbury , one or the other ! ) xxmaj the guest talked about xxmaj sir xxmaj charles &#39; career and how his films had been out of circulation ever since the 1952 exclusion of the former &quot; little xxmaj tramp &#39; from xxmaj los xxmaj xxunk xxmaj xxunk on the grounds of his being an &quot; undesirable xxmaj alien &quot; . ( no xxmaj schultz , he &#39;s xxup not from another | pos | pos | . 3 xxbos &quot; buffalo xxmaj bill , xxmaj hero of the xxmaj far xxmaj west &quot; director xxmaj mario xxmaj costa &#39;s unsavory xxmaj spaghetti western &quot; the xxmaj beast &quot; with xxmaj klaus xxmaj kinski could only have been produced in xxmaj europe . xxmaj hollywood would never dared to have made a western about a sexual predator on the prowl as the protagonist of a movie . xxmaj never mind that xxmaj kinski is ideally suited to the role of &#39; crazy &#39; xxmaj johnny . xxmaj he plays an individual entirely without sympathy who is ironically dressed from head to toe in a white suit , pants , and hat . xxmaj this low - budget oater has nothing appetizing about it . xxmaj the typically breathtaking xxmaj spanish scenery around xxmaj almeria is nowhere in evidence . xxmaj instead , xxmaj costa and his director of photography | pos | pos | . 4 xxbos xxmaj if you &#39;ve seen the trailer for this movie , you pretty much know what to expect , because what you see here is what you get . xxmaj and even if you have n&#39;t seen the previews , it wo n&#39;t take you long to pick up on what you &#39;re in for-- specifically , a good time and plenty of xxunk from this clever satire of ` reality xxup tv &#39; shows and ` buddy xxmaj cop &#39; movies , ` showtime , &#39; directed by xxmaj tom xxmaj dey , starring xxmaj robert xxmaj de xxmaj niro and xxmaj eddie xxmaj murphy . n n t xxmaj mitch xxmaj preston ( de xxmaj niro ) is a detective with the xxup l.a.p.d . , and he &#39;s good at what he does ; but working a case one night , things suddenly go south when another cop | pos | pos | . 5 xxbos * xxmaj some spoilers * n n xxmaj this movie is sometimes subtitled &quot; life xxmaj everlasting . &quot; xxmaj that &#39;s often taken as reference to the final scene , but more accurately describes how dead and buried this once - estimable series is after this sloppy and illogical send - off . n n xxmaj there &#39;s a &quot; hey kids , let &#39;s put on a show air &quot; about this telemovie , which can be endearing in spots . xxmaj some fans will feel like insiders as they enjoy picking out all the various cameo appearances . xxmaj co - writer , co - producer xxmaj tom xxmaj fontana and his pals pack the goings - on with friends and favorites from other shows , as well as real xxmaj baltimore personages . n n xxmaj that &#39;s on top of the returns of virtually all the members | neg | neg | . 6 xxbos ( caution : several spoilers ) n n xxmaj someday , somewhere , there &#39;s going to be a post - apocalyptic movie made that does n&#39;t stink . xxmaj unfortunately , xxup the xxup postman is not that movie , though i have to give it credit for trying . n n xxmaj kevin xxmaj costner plays somebody credited only as &quot; the xxmaj postman . &quot; xxmaj he &#39;s not actually a postman , just a wanderer with a mule in the wasteland of a western xxmaj america devastated by some unspecified catastrophe . xxmaj he trades with isolated villages by performing xxmaj shakespeare . xxmaj suddenly a pack of bandits called the xxmaj holnists , the self - declared warlords of the xxmaj west , descend upon a village that xxmaj costner &#39;s visiting , and their evil leader xxmaj gen . xxmaj bethlehem ( will xxmaj patton | neg | neg | . 7 xxbos xxmaj in a style reminiscent of the best of xxmaj david xxmaj lean , this romantic love story sweeps across the screen with epic proportions equal to the vast desert regions against which it is set . xxmaj it &#39;s a film which purports that one does not choose love , but rather that it &#39;s love that does the choosing , regardless of who , where or when ; and furthermore , that it &#39;s a matter of the heart often contingent upon prevailing conditions and circumstances . xxmaj and thus is the situation in ` the xxmaj english xxmaj patient , &#39; directed by xxmaj anthony xxmaj minghella , the story of two people who discover passion and true love in the most inopportune of places and times , proving that when it is predestined , love will find a way . n n xxmaj it &#39;s xxup | pos | pos | . 8 xxbos xxmaj no one is going to mistake xxup the xxup squall for a good movie , but it sure is a memorable one . xxmaj once you &#39;ve taken in xxmaj myrna xxmaj loy &#39;s performance as xxmaj nubi the hot - blooded gypsy girl you &#39;re not likely to forget the experience . xxmaj when this film was made the exotically beautiful xxmaj miss xxmaj loy was still being cast as foreign vixens , often xxmaj asian and usually sinister . xxmaj she &#39;s certainly an eyeful here . xxmaj it appears that her skin was darkened and her hair was curled . xxmaj in most scenes she &#39;s barefoot and wearing little more than a skirt and a loose - fitting peasant blouse , while in one scene she wears nothing but a patterned towel . i suppose xxmaj i &#39;m focusing on xxmaj miss xxmaj loy | neg | neg | . And we can predict on new texts quite easily from now on as our model has already undergone more advanced training . learn.predict(&quot;I really liked that movie!&quot;) . (&#39;pos&#39;, tensor(1), tensor([0.0092, 0.9908])) . Here we can see the model has considered the review to be positive. The second part of the result is the index of &quot;pos&quot; in our data vocabulary and the last part is the probabilities attributed to each class (99.1% for &quot;pos&quot; and 0.9% for &quot;neg&quot;). . Using what has been done so far, you can even write your own mini movie review, or copy one from the Internet, and we can see what this model thinks about it by performing the same command that we run before! . For the curious ones: Using the data block API . We can also use the data block API to get our data in a DataLoaders. This is a bit more advanced, The intention with this is just to refine our work a little more so far in order to learn details about the structure of the data and the way it will be stored. . A datablock is built by giving the fastai library a bunch of information: . the types used, through an argument called blocks: here we have images and categories, so we pass TextBlock and CategoryBlock. To inform the library our texts are files in a folder, we use the from_folder class method. | how to get the raw items, here our function get_text_files. | how to label those items, here with the parent folder. | how to split those items, here with the grandparent folder. | . imdb = DataBlock(blocks=(TextBlock.from_folder(path), CategoryBlock), get_items=get_text_files, get_y=parent_label, splitter=GrandparentSplitter(valid_name=&#39;test&#39;)) . This only gives a blueprint on how to assemble the data. To actually create it, we need to use the dataloaders method: . dls = imdb.dataloaders(path) . Train a text classifier using the ULMFit approach . The pretrained model we used in the first stage of this project is called a language model. It was pretrained on Wikipedia on the task of guessing the next word, after reading all the words before. We got great results by directly fine-tuning this language model to a movie review classifier, but with one extra step, we can do even better: the Wikipedia English is slightly different from the IMDb English. So instead of jumping directly to the classifier, we could fine-tune our pretrained language model to the IMDb corpus and then use that as the base for our classifier. . One reason, of course, is that it is helpful to understand the foundations of the models that you are using. But there is another very practical reason, which is that you get even better results if you fine tune the (sequence-based) language model prior to fine tuning the classification model. For instance, in the IMDb sentiment analysis task, the dataset includes 50,000 additional movie reviews that do not have any positive or negative labels attached in the unsup folder. We can use all of these reviews to fine tune the pretrained language model — this will result in a language model that is particularly good at predicting the next word of a movie review. In contrast, the pretrained model was trained only on Wikipedia articles. . The whole ideia of this stage can be summarized by this picture: . . We can get our texts in a DataLoaders suitable for language modeling very easily: . dls_lm = TextDataLoaders.from_folder(path, is_lm=True, valid_pct=0.1) . We need to pass something for valid_pct otherwise this method will try to split the data by using the grandparent folder names. By passing valid_pct=0.1, we tell it to get a random 10% of those reviews for the validation set. . We can have a look at our data using show_batch. Here the task is to guess the next word, so we can see the targets have all shifted one word to the right. . dls_lm.show_batch(max_n=5) . text text_ . 0 xxbos xxmaj about thirty minutes into the film , i thought this was one of the weakest &quot; xxunk ever because it had the usual beginning ( a murder happening , then xxmaj columbo coming , inspecting everything and interrogating the main suspect ) squared ! xxmaj it was boring because i thought i knew everything already . n n xxmaj but then there was a surprising twist that turned this episode into | xxmaj about thirty minutes into the film , i thought this was one of the weakest &quot; xxunk ever because it had the usual beginning ( a murder happening , then xxmaj columbo coming , inspecting everything and interrogating the main suspect ) squared ! xxmaj it was boring because i thought i knew everything already . n n xxmaj but then there was a surprising twist that turned this episode into a | . 1 yeon . xxmaj these two girls were magical on the screen . i will certainly be looking into their other films . xxmaj xxunk xxmaj jeong - ah is xxunk cheerful and hauntingly evil as the stepmother . xxmaj finally , xxmaj xxunk - su xxmaj kim gives an excellent performance as the weary , broken father . n n i truly love this film . xxmaj if you have yet to see | . xxmaj these two girls were magical on the screen . i will certainly be looking into their other films . xxmaj xxunk xxmaj jeong - ah is xxunk cheerful and hauntingly evil as the stepmother . xxmaj finally , xxmaj xxunk - su xxmaj kim gives an excellent performance as the weary , broken father . n n i truly love this film . xxmaj if you have yet to see &#39; | . 2 tends to be tedious whenever there are n&#39;t any hideous monsters on display . xxmaj luckily the gutsy killings and eerie set designs ( by no less than xxmaj bill xxmaj paxton ! ) compensate for a lot ! a nine - headed expedition is send ( at hyper speed ) to the unexplored regions of space to find out what happened to a previously vanished spaceship and its crew . xxmaj | to be tedious whenever there are n&#39;t any hideous monsters on display . xxmaj luckily the gutsy killings and eerie set designs ( by no less than xxmaj bill xxmaj paxton ! ) compensate for a lot ! a nine - headed expedition is send ( at hyper speed ) to the unexplored regions of space to find out what happened to a previously vanished spaceship and its crew . xxmaj bad | . 3 movie just sort of meanders around and nothing happens ( i do n&#39;t mean in terms of plot - no plot is fine , but no action ? xxmaj come on . ) xxmaj in hindsight , i should have expected this - after all , how much can really happen between 4 teens and a bear ? xxmaj so although special effects , acting , etc are more or less on | just sort of meanders around and nothing happens ( i do n&#39;t mean in terms of plot - no plot is fine , but no action ? xxmaj come on . ) xxmaj in hindsight , i should have expected this - after all , how much can really happen between 4 teens and a bear ? xxmaj so although special effects , acting , etc are more or less on par | . 4 greetings again from the darkness . xxmaj writer / xxmaj director ( and xxmaj wes xxmaj anderson collaborator ) xxmaj noah xxmaj baumbach presents a semi - autobiographical therapy session where he unleashes the anguish and turmoil that has carried over from his childhood . xxmaj the result is an amazing insight into what many people go through in a desperate attempt to try and make their family work . n n xxmaj | again from the darkness . xxmaj writer / xxmaj director ( and xxmaj wes xxmaj anderson collaborator ) xxmaj noah xxmaj baumbach presents a semi - autobiographical therapy session where he unleashes the anguish and turmoil that has carried over from his childhood . xxmaj the result is an amazing insight into what many people go through in a desperate attempt to try and make their family work . n n xxmaj the | . Then we have a convenience method to directly grab a Learner from it, using the AWD_LSTM architecture like before. We use accuracy and perplexity as metrics (the later is the exponential of the loss) and we set a default weight decay of 0.1. to_fp16 puts the Learner in mixed precision, which is going to help speed up training on GPUs that have Tensor Cores. . learn = language_model_learner(dls_lm, AWD_LSTM, metrics=[accuracy, Perplexity()], path=path, wd=0.1).to_fp16() . By default, a pretrained Learner is in a frozen state, meaning that only the head of the model will train while the body stays frozen. We show you what is behind the fine_tune method here and use a fit_one_cycle method to fit the model: . learn.fit_one_cycle(1, 1e-2) . epoch train_loss valid_loss accuracy perplexity time . 0 | 4.120048 | 3.912788 | 0.299565 | 50.038246 | 11:39 | . This model takes a while to train, so it&#39;s a good opportunity to talk about saving intermediary results. . One should know that the state of your model can easily be saved like so: . learn.save(&#39;1epoch&#39;) . It will create a file in learn.path/models/ named &quot;1epoch.pth&quot;. If you want to load your model on another machine after creating your Learner the same way, or resume training later, you can load the content of this file with: . learn = learn.load(&#39;1epoch&#39;) . We can them fine-tune the model after unfreezing: . learn.unfreeze() learn.fit_one_cycle(10, 1e-3) . epoch train_loss valid_loss accuracy perplexity time . 0 | 3.893486 | 3.772820 | 0.317104 | 43.502548 | 12:37 | . 1 | 3.820479 | 3.717197 | 0.323790 | 41.148880 | 12:30 | . 2 | 3.735622 | 3.659760 | 0.330321 | 38.851997 | 12:09 | . 3 | 3.677086 | 3.624794 | 0.333960 | 37.516987 | 12:12 | . 4 | 3.636646 | 3.601300 | 0.337017 | 36.645859 | 12:05 | . 5 | 3.553636 | 3.584241 | 0.339355 | 36.026001 | 12:04 | . 6 | 3.507634 | 3.571892 | 0.341353 | 35.583862 | 12:08 | . 7 | 3.444101 | 3.565988 | 0.342194 | 35.374371 | 12:08 | . 8 | 3.398597 | 3.566283 | 0.342647 | 35.384815 | 12:11 | . 9 | 3.375563 | 3.568166 | 0.342528 | 35.451500 | 12:05 | . Once this is done, we save all of our model except the final layer that converts activations to probabilities of picking each token in our vocabulary. The model not including the final layer is called the encoder. We can save it with save_encoder: . learn.save_encoder(&#39;finetuned&#39;) . Jargon:Encoder: The model not including the task-specific final layer(s). It means much the same thing as body when applied to vision CNNs, but tends to be more used for NLP and generative models. . Before using this to fine-tune a classifier on the reviews, we can use our model to generate random reviews: since it&#39;s trained to guess what the next word of the sentence is, we can use it to write new reviews: . TEXT = &quot;I liked this movie because&quot; N_WORDS = 40 N_SENTENCES = 2 preds = [learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)] . print(&quot; n&quot;.join(preds)) . i liked this movie because of its story and characters . The story line was very strong , very good for a sci - fi film . The main character , Alucard , was very well developed and brought the whole story i liked this movie because i like the idea of the premise of the movie , the ( very ) convenient virus ( which , when you have to kill a few people , the &#34; evil &#34; machine has to be used to protect . Finally training a text classifier . We can gather our data for text classification almost exactly like before: . dls_clas = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid=&#39;test&#39;, text_vocab=dls_lm.vocab) . The main difference is that we have to use the exact same vocabulary as when we were fine-tuning our language model, or the weights learned won&#39;t make any sense. We pass that vocabulary with text_vocab. . Then we can define our text classifier like before: . learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, metrics=accuracy) . The difference is that before training it, we load the previous encoder: . learn = learn.load_encoder(&#39;finetuned&#39;) . The last step is to train with discriminative learning rates and gradual unfreezing. In computer vision, we often unfreeze the model all at once, but for NLP classifiers, we find that unfreezing a few layers at a time makes a real difference. . learn.fit_one_cycle(1, 2e-2) . epoch train_loss valid_loss accuracy time . 0 | 0.347427 | 0.184480 | 0.929320 | 00:33 | . In just one epoch we get the same result as our training in the first section, not too bad! We can pass -2 to freeze_to to freeze all except the last two parameter groups: . learn.freeze_to(-2) learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2)) . epoch train_loss valid_loss accuracy time . 0 | 0.247763 | 0.171683 | 0.934640 | 00:37 | . Then we can unfreeze a bit more, and continue training: . learn.freeze_to(-3) learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3)) . epoch train_loss valid_loss accuracy time . 0 | 0.193377 | 0.156696 | 0.941200 | 00:45 | . And finally, the whole model! . learn.unfreeze() learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3)) . epoch train_loss valid_loss accuracy time . 0 | 0.172888 | 0.153770 | 0.943120 | 01:01 | . 1 | 0.161492 | 0.155567 | 0.942640 | 00:57 | . Conclusion of the project . So far, I hope the power of artificial intelligences that use predefined python libraries like pytorch has become clear. With simple scripts like the one developed here for didactic purposes, we can create models capable of predicting market behavior, human writing patterns, and even performing detailed analyzes on databases that would cost us many hours of relatively automated work, which can be saved with a simple implementation like the one shown here. . Ihope you like it, and feel free to try on your own machine and tell me later your thoughts on this project, as well as any suggested modifications, everything can be discussed in the comments of this push on github or through private conversations in my email: . ruben.esteche@ufpe.br . or . rubenesteche@hotmail.com .",
            "url": "https://resteche.github.io/REsteche_blog/deep%20learning/pytorch/text%20classification/neural%20network/2022/03/01/DL_text_project.html",
            "relUrl": "/deep%20learning/pytorch/text%20classification/neural%20network/2022/03/01/DL_text_project.html",
            "date": " • Mar 1, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "A temporal Gaussian wave packet",
            "content": "Our intention now turns to proposing an analytical solution to equation (5.6) of Ricardo&#39;s thesys. This problem has already been analyzed in different ways, and it has been shown that for a free particle problem, there is a closed form for its solution as in ref. session 2.4. Now, let&#39;s show that for a wide range of physical systems, which are under the action of a potential independent of time, it is possible to use a similar technique to obtain a general solution. Recalling that: . $$ sqrt{2m} hat{ sigma_z} [i hbar frac{ partial}{ partial t} - V(T,x)]^{1/2} phi (t|x) = -i hbar frac{ partial}{ partial x} phi (t|x) tag{1}$$ . Analogously to the reference for free particle solution, we can use a separation of variables in the form $ phi(t|x) = e^{-i epsilon t/ hbar} phi_ epsilon (x)$, and by the same argument of series expansion it&#39;s clear that . $$ hat{ sigma_z} sqrt{2m [ epsilon - V(x)]} phi_ epsilon (x) = -i hbar frac{d}{dx} phi_ epsilon (x) tag{2}$$ . This is a first-order differential equation whose solution is given by . $$ phi_ epsilon (x) = frac{1}{ sqrt{2 pi hbar}} e^{(i/ hbar) hat{ sigma_z} int_{0}^{x} sqrt{2m [ epsilon - V(x)]} dx^{ prime}} tag{3}$$ . So, since (3) is still a linear equation, we can write the general solution so that . $$ phi (t|x) = frac{1}{ sqrt{2 pi hbar}} int d epsilon C_ epsilon e^{(i/ hbar) hat{ sigma}_z int_{0}^{x} sqrt{2m [ epsilon - V(x)]} dx^{ prime}-i epsilon t / hbar} tag{4}$$ . Or in terms of the classical momentum&#39;s $P( epsilon, x) = pm sqrt{2m epsilon} $ . $$ phi (t|x) = frac{1}{ sqrt{2 pi hbar}} int d epsilon C_ epsilon e^{(i/ hbar) int_{0}^{x} P( epsilon; x^{ prime}) dx^{ prime}-i epsilon t / hbar} tag{5}$$ . In order to solve then analytically for any potential $V(x)$ one need only to perform the integration in the exponential argument to obtain the general solution of the problem. At first glance, this simplicity can seem strange, after all, for the Schrödinger equation we don&#39;t have a uniquely shaped solution for every $V(x)$. However, this result is somewhat expected: first, analyzing Eq. (5.6) of the thesys, we see that the potential $V(x,t) = V(x)$ we choose only depends on the conditional parameter of this equation, i.e., of the $x$ position. Thus, if we want to obtain an equivalent result in the Schrödinger equation, we must choose a potential $V(x,t)$ that only depends on the conditional parameter t of this equation, so we will find the same result in the symmetric construction of quantum mechanics. . Proceeding with the solution, let us first consider the simplest case to be that of a free particle, where the potential will then be given by $V(x) = 0$. If we then apply these conditions to equation (4) and consider only the positive branch of the solution matrix $ hat{ sigma_z}$ we will reduce it to . $$ phi (t|0) = frac{1}{ sqrt{2 pi hbar}} int d epsilon C_ epsilon e^{(-i epsilon t/ hbar)} tag{6} $$ . Note here that we are now going to proceed in very similar to what is done in the traditional construction of quantum theory, with the aim now to evolve spatially through the fourier transform the conditional space wave function of our theory . Taking $$ phi (t|0) = frac{1}{ sqrt{2 pi hbar}} exp{ left[ frac{-(t-t_0)^2}{2 sigma^2} right]} exp{ left[ frac{i epsilon_0 t}{ hbar} right]} tag{7}$$ . As an initial condition, where you notice that I set a Gaussian in $x=0$ with standard deviation $ sigma$, we follow applying the Fourier transform already substituting for the coordinate $ epsilon$ so that . $$ phi(t|0) = int_{- infty}^{ infty} frac{d epsilon}{ sqrt{2 pi hbar}} psi( epsilon) e^{i epsilon t} tag{8} $$ . $$ psi( epsilon) equiv C_ epsilon = int_{- infty}^{ infty} frac{1}{ sqrt{2 pi hbar} } left ( frac{1} { sqrt{2 pi hbar}} e^{-(t-t_0)^2/(2 sigma^2)} e^{(i epsilon_0 t)/ hbar} right ) e^{ -i epsilon t} dt$$ . $$= frac{1}{ 2 pi hbar} int_{- infty}^{ infty} left ( e^{-(t-t_0)^2/(2 sigma^2)} e^{(i epsilon_0 t)/ hbar} right ) e^{-i epsilon t} dt$$ . $$ = frac{1}{ 2 pi hbar} frac{ exp{ left ( - frac{( epsilon_0 + hbar epsilon) left ( sigma^2( epsilon_0 + hbar epsilon)-2i hbar t_0 right ) }{2 hbar^2} right )}}{ sqrt{ frac{1}{ sigma^2}}}$$ . $$= frac{ sigma^2}{ 2 pi hbar} exp{ left ( - frac{( epsilon_0 + hbar epsilon) left ( sigma^2( epsilon_0 + hbar epsilon)-2i hbar t_0 right ) }{2 hbar^2} right )} tag{9}$$ . What this is telling us is that the Gaussian time wave function is a superposition of different energies, with the probability of finding these energies between $ epsilon_1$ and $ epsilon_1 + d epsilon$ being proportional to $ exp{ left ( - frac{( epsilon_0 + hbar epsilon) left ( sigma^2( epsilon_0 + hbar epsilon)-2i hbar t_0 right ) }{2 hbar^2} right ) } d epsilon$. In order to recap, it is worth recalling the development of this problem within the scope of traditional theory, proceeding in the same way and applying a Fourier transform into a Gaussian and obtaining another Gaussian. Note here that the results obtained are, as well as those proposed, symmetrical and equivalent. We will also point out that our initial wave function (5) which represents the solution found to the Schroedinger equation conditional space represents a superposition of different plane waves associated with different coefficients (which we usually refer to as amplitudes). . Now, finally in possession of the coefficients $C_ epsilon$ we can substitute back the results in (7.5) and obtain the result of the space-conditional wavefunction given an initial Gaussian space-boundary condition. . As we also have the general result for $ phi(t|x)$ in the case of the free particle, we can substitute in (7.4) the value found for $C_ epsilon$ and understand how the wave function will propagate in all instants of time t, given that we are going to observe it in a known space x. . Evolved space result | . $$ phi (t|x) = frac{1}{ sqrt{2 pi hbar}} int d epsilon frac{ sigma^2}{ 2 pi hbar} exp{ left ( - frac{( epsilon_0 + hbar epsilon) left ( sigma^2( epsilon_0 + hbar epsilon)-2i hbar t_0 right ) }{2 hbar^2} right )} e^{(i/ hbar) sqrt{2m epsilon} x-i epsilon t / hbar } tag{10}$$ . $$= frac{ sigma^4}{(2 hbar pi )^{3/2}} int d epsilon exp{ left ( - frac{( epsilon_0 + hbar epsilon) left ( sigma^2( epsilon_0 + hbar epsilon)-2i hbar t_0 right ) }{2 hbar^2} right )} e^{(i/ hbar) sqrt{2m epsilon} x-i epsilon t / hbar} tag{11} $$ . $$ = text{integral não tem solução analítica} blacksquare $$ $$ text{(note que ela não divergiu, apenas não tem solução analítica. Integrando numericamente para todas as constantes = 1 obtemos para a condição inicial: 1.390521824852123 + 0.2928372699906291 i)} $$ . In order to eliminate the root of the exponent that makes this integration impossible, we can then use the construction of $ epsilon$ that as $P = pm sqrt{2 m epsilon}$, that is, $ epsilon_p = P^ 2/2m$. By carrying out this replacement, we will obtain that: . $$ epsilon_p = frac{P^2}{2m} Rightarrow d epsilon = frac{P}{m} dP tag{12}$$ . Then, proceeding with the integration from 0 to $ infty$ (by replacing the $P^2$ of the exponent of $C_P$) . $$ phi (t|x) = frac{1}{ sqrt{2 pi hbar}} int dP frac{P }{ m} frac{ sigma^2}{ 2 pi hbar} exp{ left ( - frac{(P_0 + frac{ hbar P^2}{2 m}) left ( sigma^2(P_0 + frac{ hbar P^2}{2 m})-2i hbar t_0 right ) }{2 hbar^2} right )} e^{(i/ hbar) P x - i P^2 t /2 m hbar } tag{13} $$ . $$ phi (t|x) = frac{ sigma^2}{ (2 pi hbar)^{3/2} m} int dP P exp{ left ( - frac{(P_0 + frac{ hbar P^2}{2 m}) left ( sigma^2(P_0 + frac{ hbar P^2}{2 m})-2i hbar t_0 right ) }{2 hbar^2} right )} e^{(i/ hbar) P x - i P^2 t /2 m hbar } tag{14}$$ . *Note: I was unable to continue analytically here, however I was able to computationally solve/plot the integral above; I can then work on additional computational graphics from there . Numerical evolution for integrals without analytical solution . import quadpy import math as math from math import e from math import pi from math import sqrt import numpy as np from scipy import integrate import matplotlib.pyplot as plt import cmath #definindo unidade imaginária: i = 0 + 1j #criando a função para o plot def plot_results(sigma = 1, Xp = 1, m=1, P_0 = 1, hbar = 1): #preparando a função que vai ser integrada para a integração def f(x, t): res = (sigma**2)/(2*pi*hbar)**(3/2)*x*e**(-(P_0 + hbar*(x**2)/(2*m))*(sigma**2)*(P_0 + hbar*(x**2)/(2*m))/(2*hbar**2))*cmath.exp((i/hbar)*x*Xp - i*(x**2)*t/(2*m*hbar)) return res integral = np.vectorize(f) #efetuando a integral def F(t): xmin = 0 xmax = 10 y,err = quadpy.quad(lambda x: integral(x,t), xmin, xmax) return y Fvec = np.vectorize(F) t = np.linspace(-5,80,200) plt.plot(t, abs(Fvec(t))**2) plt.title(&#39;Numerical evolution of the probability density of Gaussian wave packet&#39;) plt.ylabel(&#39;$| phi(t|x)|^2$&#39;) plt.xlabel(&#39;$t$&#39;) from ipywidgets import interact, fixed interact(plot_results, sigma=(0.0,1.5), Xp=(0,40), m=fixed(1), hbar=fixed(1), P_0=fixed(1)); . Above, the automated scheme of the numerical evolution of the most general form of the Gaussian package, where I can put as an interactive variable any interesting quantity for discussion. For a static view I suggest the following view template . . Now, with the above script, we can use the following result for the exponential integral that contains the potential; as a souvenir, eq. (4) tells us that: . $$ phi (t|x) = frac{1}{ sqrt{2 pi hbar}} int d epsilon C_ epsilon e^{(i/ hbar) hat{ sigma_z} int_{0}^{x} sqrt{2m [ epsilon - V(x)]} dx^{ prime}-i epsilon t / hbar} tag{4}$$ . So, proceeding with an arbitrary phisycal potential, let&#39;s say the potential of a harmonic oscillator $ V(x) = -kx^2 $, we can solve the above as . $$I = int_0^x sqrt{2m left[ epsilon - kx^{ prime 2} right]} dx^ prime Rightarrow sqrt{2m} int_0^x sqrt{ epsilon - kx^{ prime 2}} dx^ prime tag{19}$$ Replacing as $x^ prime = frac{ sqrt{ epsilon} tan u}{ sqrt{k}} rightarrow u = arctan left( sqrt{ frac{k}{ epsilon}} u right); dx^ prime = sqrt{ frac{k}{ epsilon}} sec^2(u) $, so $$ = int frac{ sqrt{ epsilon} sec^2 (u) sqrt{ epsilon tan^2 (u) + epsilon}}{ sqrt{-k}} du tag{20}$$ with $ epsilon tan^2 u + epsilon = epsilon sec^2 u$ $$ = int frac{ sqrt{ epsilon} sec^2 (u) sqrt{ epsilon tan^2 (u) + epsilon}}{ sqrt{-k}} du= sqrt{ frac{2 m}{-k}} epsilon int sec^3 u du tag{21}$$ that is, by the formula $$ int sec^n (u) du = frac{n - 2}{n - 1} int sec^{n-2} (u) du + frac{sec^{n-2}(u) tan(u)}{n - 1} tag{21}$$ With n = 3, $$ = frac{ sec(u) tan(u)}{2} + frac{1}{2} int sec(u) du tag{23}$$ our initial integral then equates to . $$ = frac{ epsilon}{2} sqrt{ frac{2m}{k}} left[ sec (u) tan (u) + ln left( sec(u) tan(u) right) right] tag{24}$$ . removing u from the expression, where $ tan left( arctan left[ sqrt{ frac{k}{ epsilon}} x^{ prime} right] right) = sqrt{ frac{k}{ epsilon}} x^{ prime}$ also $ sec left( arctan left[ sqrt{ frac{k}{ epsilon}} x^{ prime} right] right) = sqrt{ frac{k x^{ prime2}}{ epsilon} + 1}$ we obtain that . $$ = sqrt{2m} left[ frac{ epsilon ln left( sqrt{1 - frac{kx^2}{ epsilon}} + sqrt{ frac{-k}{ epsilon}} x^{ prime} right) }{2 sqrt{-k}} + frac{ sqrt{ epsilon} x^{ prime} sqrt{1 - frac{kx^2}{ epsilon}}}{2} right] tag{25}$$ . plugging integration boundaries back, . $$ int_0^x sqrt{2m left[ epsilon - kx^{ prime 2} right]} dx^ prime= frac{ sqrt{2m}}{2 k} left[x k sqrt{ epsilon - x^2 k} + epsilon arcsin left( sqrt{ frac{k}{ epsilon}} x right) sqrt{k} right] tag{26}$$ . Implementing this result in code to evolve the conditional time-space probability density, it will then be possible to understand how the probability distribution will be affected while under the actuation of the potential . import quadpy import math as math from math import e from math import pi from math import sqrt import numpy as np from scipy import integrate import matplotlib.pyplot as plt import cmath #definindo unidade imaginária: i = 0 + 1j #criando a função para o plot def plot_results(sigma = 1, Xp = 1, m=1, P_0 = 1, hbar = 1, k = 1): #preparando a função que vai ser integrada para a integração def f(x, t): res = (sigma**2)/(2*pi*hbar)**(3/2)*x*e**(-(P_0 + hbar*(x**2)/(2*m))*(sigma**2)*(P_0 + hbar*(x**2)/(2*m))/(2*hbar**2))*cmath.exp((i/hbar)*sqrt(2*m)/(2*k)*(Xp*k*cmath.sqrt((Xp**2)/(2*m) -(Xp**2)*k)+ (x**2)/(2*m)*np.arcsin(Xp*cmath.sqrt(k*2*m/(x**2)))*sqrt(k)) - i*(x**2)*t/(2*m*hbar)) #parte do potencial = (Xp*k*cmath.sqrt((Xp**2)*k + (Xp**2)/(2*m))+ (x**2)/(2*m)*np.arcsin(Xp*cmath.sqrt(k*2*m/(x**2)))*sqrt(k)) return res integral = np.vectorize(f) #efetuando a integral def F(t): xmin = 0 xmax = 10 y,err = quadpy.quad(lambda x: integral(x,t), xmin, xmax) return y Fvec = np.vectorize(F) t = np.linspace(-10,50,200) plt.plot(t, abs(Fvec(t))**2) plt.title(&#39;Numerical evolution of the probability density of Gaussian wave packet under action of potential&#39;) plt.ylabel(&#39;$| phi(t|x)|^2$&#39;) plt.xlabel(&#39;$t$&#39;) from ipywidgets import interact, fixed interact(plot_results, sigma=(0.0,1.5), Xp=(0,10), m=fixed(1), hbar=fixed(1), P_0=fixed(1), k=fixed(1)); . Static visualization: . . Following the same procedure, it is trivial to also solve the problem for an arbitrary square potential of width $a in [0,5] $. Consider the following potential configuration shown in the image below, . . In order to perform the integral of expression (4) once again, we will proceed by dividing the wavefunction integral into 3 different regions referring to the potential barrier proposed above. starting with the integral inside the potential barrier, for a $V_0 &gt; epsilon$, we will obtain from . $$ phi (t|x) = frac{1}{ sqrt{2 pi hbar}} int d epsilon C_ epsilon e^{(i/ hbar) hat{ sigma_z} int_{0}^{x} sqrt{2m [ epsilon - V(x)]} dx^{ prime}-i epsilon t / hbar} tag{4}$$ . Inside the potential barrier, . $$ phi (t|x) = frac{1}{ sqrt{2 pi hbar}} int d epsilon C_ epsilon e^{(i/ hbar) int_{0}^{x} sqrt{2m [ epsilon - V_0]} dx^{ prime}-i epsilon t / hbar} tag{27}$$ . Solving the exponent&#39;s integral analytically again . $$ int_{0}^{x} sqrt{2m [ epsilon - V_0]} dx^{ prime} = sqrt{2m left({ epsilon}-V_0 right)}x tag{28}$$ . Which leads us, following the same procedure as before, to . import quadpy import math as math from math import e from math import pi from math import sqrt import numpy as np from scipy import integrate import matplotlib.pyplot as plt import cmath #definindo unidade imaginária: i = 0 + 1j #criando a função para o plot def plot_results(sigma = 1, Xp = 1, m=1, P_0 = 1, hbar = 1, V0 = 5): #preparando a função que vai ser integrada para a integração def f(x, t): res = (sigma**2)/(2*pi*hbar)**(3/2)*x*e**(-(P_0 + hbar*(x**2)/(2*m))*(sigma**2)*(P_0 + hbar*(x**2)/(2*m))/(2*hbar**2))*cmath.exp((i/hbar)*cmath.sqrt(2*m*((x**2)/(2*m) - V0))*Xp - i*(x**2)*t/(2*m*hbar)) return res integral = np.vectorize(f) #efetuando a integral def F(t): xmin = 0 xmax = 10 y,err = quadpy.quad(lambda x: integral(x,t), xmin, xmax) return y Fvec = np.vectorize(F) t = np.linspace(-5,40,200) plt.plot(t, abs(Fvec(t))**2) plt.title(&#39;Numerical evolution of the probability density of Gaussian wave packet under action of potential&#39;) plt.ylabel(&#39;$| phi(t|x)|^2$&#39;) plt.xlabel(&#39;$t$&#39;) from ipywidgets import interact, fixed interact(plot_results, sigma=(0.0,1.5), Xp=(0,5), m=fixed(1), hbar=fixed(1), P_0=fixed(1), V0=fixed(5)); . Static view before normalization . . Static view after normalization . . In order to provide a more detailed analysis of the problem, we will continue our construction observing other parameters of this system. As we know from traditional quantum mechanics theory, we can calculate the scattering width of a wave packet using the statistical variance and the expected value of its operators, then setting up a parallel for our construction leads us to . $$ Delta T(x) rightarrow Delta T = left[ langle T^2 rangle - langle T rangle^2 right]^{1/2} tag{29}$$ . where, $$ langle T rangle = int_{- infty}^{ infty} t | phi(t|x)|^2 dt; langle T^2 rangle = int_{- infty}^{ infty} t^2 | phi(t|x)|^2 dt tag{30}$$ so, . $$ Delta T = Bigg { int_{- infty}^{ infty} t^2 | phi(t|x)|^2 dt - left[ int_{- infty}^{ infty} t | phi(t|x)|^2 dt right]^2 Bigg } tag{31}$$ . and, as you can remember for the case of the free particle, $$| phi(t|x)|^2 = left| frac{ sigma^2}{(2 pi hbar)^{3/2}m} int dP C_P P e^{(i/ hbar)Px - iP^2t/2m hbar} right|$$ . and, of course, the reason we started this calculation, . $$ C_P = exp{ left ( - frac{(P_0 + frac{ hbar P^2}{2 m}) left ( sigma^2(P_0 + frac{ hbar P^2}{2 m})-2i hbar t_0 right ) }{2 hbar^2} right )} $$ . So, we can proceed to plot $ Delta T$ vs $x$ using the formulas above doing as . from scipy.integrate import quad from math import exp, cos, sin, pi, sqrt import numpy as np import matplotlib.pyplot as plt #defina aqui as constantes: sigma = 0.5 hbar = 1 m = 1 P_0 = 0 i = 0 + 1j #preparando a função que vai ser integrada para a integração, dividida em real e imaginária #parte real: def re(p, t, x): RE = (sigma**2)/(2*pi*hbar)**(3/2)*p*exp(-(P_0 + hbar*(p**2)/(2*m))*(sigma**2)*(P_0 + hbar*(p**2)/(2*m))/(2*hbar**2))*cos((1/hbar)*p*x - 1*(p**2)*t/(2*m*hbar)) return RE integral1 = np.vectorize(re) #parte imaginária def im(p,t,x): IM = (sigma**2)/(2*pi*hbar)**(3/2)*p*exp(-(P_0 + hbar*(p**2)/(2*m))*(sigma**2)*(P_0 + hbar*(p**2)/(2*m))/(2*hbar**2))*sin((1/hbar)*p*x - 1*(p**2)*t/(2*m*hbar)) return IM integral2 = np.vectorize(im) #efetuando a integral em p e passando t para ser o argumento da função de onda def F(t,x): min = 0 max = 10 y, err = quad(integral1, min, max, args=(t,x)) return y def M(t,x): min = 0 max = 10 y, err = quad(integral2, min, max, args=(t,x)) return y def OP(t,x): return F(t,x) + M(t,x)*i OPvec = np.vectorize(OP) #definindo as integrais 1 e 2 do meu DeltaT #integral de &lt;T^2&gt; def R(t, x): return (t**2)*abs(OPvec(t,x))**2 Rvec = np.vectorize(R) #integral de &lt;T&gt; para fazer &lt;T&gt;^2 def L(t, x): return t*abs(OPvec(t,x))**2 Lvec = np.vectorize(L) #efetuando a integral em t e passando Xp para ser o argumento da função 1 e 2 def Q(x): min = -10 max = 10 y, err = quad(Rvec, min, max, args=(x)) return y def D(x): min = -10 max = 10 y, err = quad(Lvec, min, max, args=(x)) return y #calculando agora DeltaT como (&lt;T^2&gt; - &lt;T&gt;^2)^-(1/2) def Sol(x): return sqrt(Q(x) - D(x)**2) Sol = np.vectorize(Sol) #organizando o plot da função em questão X = np.linspace(-9, 9, 200) plt.plot(X, Sol(X)) plt.title(&#39;Width of the wave packet as a function of position&#39;) plt.ylabel(&#39;$ Delta T$&#39;) plt.xlabel(&#39;$x$&#39;) plt.show() . . Just for the sake of comparison, I will leave here the same graph constructed for the problem from the perspective of traditional quantum mechanics from COHEN-TANNOUDJI, C., DIU, B., &amp; LALOË, F. (2005). Quantum mechanics. New York, Wiley. . . Finally, to further improve our visualization of the system studied, I suggest that we also observe a last plot of the maximum of the probability density function as a function of x. For this, note that in our graph we had the following distribution: $| phi (t|x)|^2$ vs $t$ for x=1, 2, 3,..... . So now, we&#39;re going to take the value of the maximum of $| phi (t)|^2$ in all values it assumes fot different $t$&#39;s, and plot it in function of $x$ . In order to calculate the maximum individually, we will perform a numerically point-by-point derivative using the formal definition of the limit where $ delta = 0$ in: . $$ frac{d f}{d t} = frac{f(t + delta/2)+f(t - delta/2)}{ delta} $$ . $$ frac{d^2 f}{d t^2} = frac{f(t + delta)+f(t - delta)- 2f(t)}{ delta^2} $$ . Where we will analyze the behavior of the first and second derivatives in $| psi (t|x)|^2$ to measure the maximum point accurately for each iteration over the spatial parameter $x$. Take the following graph as an example, and notice that I took in my algorithm the value to be plotted during an iteration in $x$ that met the conditions: $ frac{d }{d t}| psi|^2 = 0$ and also $ frac{d^2 }{d t^2} | psi|^2 &lt; 0$ . . from scipy.integrate import quad from math import exp, cos, sin, pi import numpy as np import matplotlib.pyplot as plt #defina aqui as constantes: sigma = 1 hbar = 1 m = 1 P_0 = 0 i = 0 + 1j n = 20 piu = np.zeros(n) valor_x = np.zeros(n) #definindo uma derivada para o algortimo de máximos def derivative(f,a,method=&#39;central&#39;,h=1e-4): if method == &#39;central&#39;: return (f(a + h) - f(a - h))/(2*h) elif method == &#39;forward&#39;: return (f(a + h) - f(a))/h elif method == &#39;backward&#39;: return (f(a) - f(a - h))/h else: raise ValueError(&quot;Method must be &#39;central&#39;, &#39;forward&#39; or &#39;backward&#39;.&quot;) #executando o algoritmo que calcular psi e tira seus maximos for x in range (n): #preparando a função que vai ser integrada para a integração, dividida em real e imaginária #parte real: def re(p, t): RE = (sigma**2)/(2*pi*hbar)**(3/2)*p*exp(-(P_0 + hbar*(p**2)/(2*m))*(sigma**2)*(P_0 + hbar*(p**2)/(2*m))/(2*hbar**2))*cos((1/hbar)*p*x - 1*(p**2)*t/(2*m*hbar)) return RE integral1 = np.vectorize(re) #parte imaginária def im(p, t): IM = (sigma**2)/(2*pi*hbar)**(3/2)*p*exp(-(P_0 + hbar*(p**2)/(2*m))*(sigma**2)*(P_0 + hbar*(p**2)/(2*m))/(2*hbar**2))*sin((1/hbar)*p*x - 1*(p**2)*t/(2*m*hbar)) return IM integral2 = np.vectorize(im) #efetuando a integral em p e passando t para ser o argumento da função de onda def F(t): min = 0 max = 10 y, err = quad(integral1, min, max, args=(t)) return y def M(t): min = 0 max = 10 y, err = quad(integral2, min, max, args=(t)) return y def OP(t): return abs(F(t) + M(t)*i)**2 OPvec = np.vectorize(OP) def der1(t): deu = derivative(OP, t, method=&#39;central&#39;,h=1e-5) return deu der1vec = np.vectorize(der1) def der2(t): deu = derivative(der1, t, method=&#39;central&#39;,h=1e-5) return deu der2vec = np.vectorize(der2) for k in range(2000000): if (der1vec(k) &lt; 0 and der2vec(k)&lt;0): piu[x] = OP(k-1) valor_x[x] = x break #organizando o plot da função em questão plt.plot(valor_x,piu) plt.title(&#39;Maximum of $| phi (t)|^2$&#39;) plt.ylabel(&#39;$MAX(| phi(t)|^2$)&#39;) plt.xlabel(&#39;$x$&#39;) . . Redoing the same process for the point-to-point renormalized free particle wave function, we&#39;re going to find out not a too different result, as it follows . . also proceeding with the research group&#39;s suggestion made during the last presentation, it is of extreme interest to repeat this same script now plotting the value of $t$ that maximizes the probability density of our free particle in respect to the position x. it is important to make it clear that, later, I can do the same for the particle inside the square potential to plot the tunneling time graph, in the same way that it was done here for the free particle, just by substituting the expression of the integrand. . . At the level of comparison, I have already plotted in advance for the non-normalized case . .",
            "url": "https://resteche.github.io/REsteche_blog/quantum%20mechanics/quantum%20foundations/time%20distributions/2022/01/28/Pacote_gaussiano.html",
            "relUrl": "/quantum%20mechanics/quantum%20foundations/time%20distributions/2022/01/28/Pacote_gaussiano.html",
            "date": " • Jan 28, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Uma extensão espaço tempo simétrica da mecânica quântica",
            "content": "Futuras aplica&#231;&#245;es no problema do tempo de travessia . 1. Introdu&#231;&#227;o . Heisenberg introduziu em 1924 as seguintes relações de incerteza, referentes a Heisenberg Picture of quantum mechanics: . $$ Delta x Delta p geqslant frac{ hbar}{2} tag{1.1} $$ . $$ Delta E Delta t geqslant frac{ hbar}{2} tag{1.2} $$ . Dos nossos conhecimentos prévios de mecânica quântica, sabemos que da equação $(1.1)$ existem operadores $x$ e $p$ tais que . $$ X left | x right rangle = x left | x right rangle tag{1.3} $$ . $$ P left | p right rangle = p left | p right rangle tag{1.4} $$ . também que tanto $ Delta x$ como $ Delta p$ podem ser interpretados como o desvio padrão dos valores esperados de seus respectivos operadores: . $$ Delta x = sqrt{ left langle x^2 right rangle - left langle x right rangle^2} tag{1.5} $$ . $$ Delta p = sqrt{ left langle p^2 right rangle - left langle p right rangle^2} tag{1.6} $$ . Mas, reside na equação $(1.3)$ um problema a anos apontado pelo próprio pauli. Sabemos dos postulados de Heisenberg na mecânica quântica que a relação de comutação entre $x$ e $p$ é: . $$[X,P]= i hbar tag{1.7} $$ . Daí, surge a pergunta então: se interpretarmos $ Delta E$ como o desvio padrão do valor esperado do operador Hamiltoniano, teremos . $$ H left | E right rangle = E left | E right rangle tag{1.8} $$ . $$ Delta E = sqrt{ left langle H^2 right rangle - left langle H right rangle^2} tag{1.9} $$ . Então, será possível então expressar T em forma de uma variável dinâmica, ou seja, introduzir um operador T de modo que $ Delta t$ passaria a ser interpretado como o desvio padrão do valor esperado de T? . Como já referênciamos, Pauli demonstrou que a implementação de um operador temporal na mecânica quântica faria com que a Heisenberg Picture levasse a absurdos físicos (e.g ausência de valores limitantes inferiores a um Hamiltoniano) como demonstraremos a seguir: . 2. Breve demonstra&#231;&#227;o do Argumento de Pauli . No regime proposto na sessão anterior, descrevemos as seguintes condições: . $$ H left | E right rangle = E left | E right rangle $$ . $$ T left | t right rangle = t left | t right rangle $$ . $$ [H, T] = i hbar $$ . Vamos então agora mostrar que o estado $ exp[i E^ prime T/ hbar] left | E right rangle $ é um autoestado do operador Hamiltoniano H, com autovalor de energia $ E - E^ prime $. Comecemos expandindo o operador exponencial em formato de série: . $$ e^{iE^ prime T/ hbar} = sum_{n=0}^{ infty} frac{1}{n!} left( frac{iE^ prime T}{ hbar} right)^n tag{2.1} $$ . Aplicando então o operador hamiltoniano pela esquerda vamos obter que, . $$ H e^{iE^ prime T/ hbar} = sum_{n=0}^{ infty} frac{1}{n!} H left( frac{iE^ prime T}{ hbar} right)^n tag{2.2} $$ . Agora, precisamos passar o operador hamiltoniano para a direita do operador tempo. Para isso, note que . $$HT = TH + [H,T] = TH + i hbar tag{2.3}$$ . Onde usamos então o fato de que o teorema de Pauli supoem que $[H,T] = i hbar$. Assim aplicando então o operador $T$ pela direita em $(2.3)$ vamos obter que . $$HT^2 = THT + i hbar T tag{2.4}$$ . E utilizando recursivamente a equação $(2.3)$ agora no primeiro termo de $(2.4)$ é fácil ver que . $$HT^2 = T^2H + 2i hbar T tag{2.5}$$ . De maneira similar, podemos seguir então indutivamente tal . $$HT^3 = T^3H + 2i hbar T^2 tag{2.6}$$ . Até obtermos a forma generalizada, que a esta altura espero já estar clara como sendo . $$HT^n = T^nH + 2i hbar T^{n-1} tag{2.7}$$ . Portanto, substituindo $(2.7)$ de volta em $(2.2)$ , . $$ H e^{iE^ prime T/ hbar} = sum_{n=0}^{ infty} frac{1}{n!} H left( frac{iE^ prime T}{ hbar} right)^n T^nH + 2i hbar T^{n-1} $$ . $$ = e^{iE^ prime T/ hbar}H - sum_{n=1}^{ infty} frac{E^ prime}{n-1!} left( frac{iE^ prime T}{ hbar} right)^{n-1} tag{2.8} $$ . logo, . $$ H e^{iE^ prime T/ hbar} = e^{iE^ prime T/ hbar}(H - E^ prime) tag{2.9}$$ . que nos permite concluir finalmente que: . $$ H e^{iE^ prime T/ hbar} left | E right rangle = e^{iE^ prime T/ hbar} (E - E^ prime) left | E right rangle $$ . $$= (E - E^ prime)e^{iE^ prime T/ hbar} left | E right rangle tag{2.10}$$ . Onde podemos então concluir o seguinte absurdo: a aplicação do operador unitário exponencial sugerido (que físicamente representa um deslocamento de $-E^ prime$ na energia apenas) com um $E^ prime in mathbb{R}$ no autoestado de energia $ left | E right rangle$ do operador $H$ produz um novo autoestado de energia com autovalor $E - E^ prime$. Portanto, $E - E^ prime$ pode tomar qualquer valor em $ mathbb{R}$, i.e. dado um autovalor de energia $E$, podemos sempre escolher um valor para $E^ prime$ de forma a acessar qualquer autoestado com energia $E - E^ prime in mathbb{R}$ desejado. Assim, vemos que se este operador existir o espectro de energias do hamiltoniano se estenderia continuamente em $(- infty, infty)$ independente do sistema físico, permitindo assim a criação de moto-perpetuos de energia. . 3.introduzindo nota&#231;&#245;es &#250;teis . Sobre distribui&#231;&#245;es de probabilidade . Seja $P_I (i) equiv$ densidade de probabilidade da variável $I$ ter um valor entre $i$ e $i + di$ . Desta forma, a probabilidade da variável $I$ tervalor entre $(a,b)$ será de: . $$p(a,b) = int_{a}^{b} P_I (i) di tag{3.1} $$ . Clique aqui para mais informações . Probabilidade condicional . Vamos tratar como sendo a probabilidade de obter $i$ dado que $j$ aconteceu como sendo $P_I(i|j) equiv$ Função de distribuição de probabilidade (F.D.P.) da variável $I in [i, i + di]$ quando $ J = j $. Clique aqui para mais informações . No caso de tratarmos um sistema de duas variáveis, faremos tal que $P_{I,J}(i,j) equiv $ F.D.P das variáveis $I in [i, i + di]$ e $J in [j, j + dj]$ , então, . $$p(a,b;c,d) = int_{a}^{b} di int_{c}^{d} dj P_{I,J}(i,j) tag{3.2} $$ . Teorema de Bayes . Do famoso e tão usado Teorema de Bayes, vamos usar que em condições de ergodicidade temos que: . $$ P_{I,J}(i,j) = P_I(i|j) P_J(j) = P_J(j|i) P_I(i) tag{3.3}$$ . Note aqui que o fato de propormos definições baseadas em ergodicidade impõem um simetria completa entre $I$ e $J$. Note aqui que, usando as 3 ferramentas estatísticas apresentadas, já podemos criar um gancho de raciocínio para a próxima sessão. Como vamos finalmente aplicar esses conteúdos na mecânica quântica? . Vamos traduzir agora para palavras a nossa famosa equação da mecânica quântica: . $$| psi(x,t)|^2 = | langle x | psi (t) rangle|^2 tag{3.4}$$ . Como sendo a densidade de probabilidade de detectarmos uma particula na posição x dado que a medição ocorreu no instante t. Em outras palavras . $$P_{X,T}(x,t) equiv mathcal{P} (x,t)$$ . $$P_{X}(x|t) equiv | psi(x,t)|^2$$ . $$P_{T}(t) equiv f(t)$$ . $$ Rightarrow mathcal{P}(x,t) = | psi(x,t)|^2 f(t) tag{3.5}$$ . Será que podemos então construir uma simetriz através do Teorema de Bayes? . $$ mathcal{P}(x,t) = P_T(t|x)P_X(x) = P_X(x|t)P_T(t) $$ . Vamos então na sessão que segue, no intuito de contornar o argumento de Pauli, sair do espaço de Hilbert $ mathcal{H_x}$ que define a posição como variável dinâmica e o tempo como parâmetro apenas, que é o espaço onde ocorre a catástrofe da energia quando tentamos introduzir um operador tempo que seja canônicamente conjugado com $H$, e vamos agora para um espaço $ mathcal{H_t}$ onde a criação de um operador $T$ não é mais catastrófica. A intenção é que os espaços futuramente se relacionem através da seguinte transformação . $$ mathcal{H} equiv mathcal{H_X} otimes mathcal{H_T} tag{3.6}$$ . Pretendemos enxergar então através da relação $ Delta E Delta t geqslant frac{ hbar}{2}$ que o termo $ Delta t$ representa então a incerteza de sucessivas medições do operador $T$. . 4. Teoria de Extens&#227;o da MQ espa&#231;o-tempo-sim&#233;trica . Constru&#231;&#227;o via Hamilton-Jacobi . No intuito de prover uma fundamentação teórica mais completa, vamos construir esse desenvolvimento a partir do zero. Uma das operações binárias mais importantes no tocante a teoria da mecânica clássica Hamiltoniana são os conhecidos Colchetes de Poisson. Relembre que os definimos da seguinte forma: . $$ { lambda, omega } = sum_{k}^{N} left( frac{ partial lambda}{ partial q_k} frac{ partial omega}{ partial p_k} - frac{ partial lambda}{ partial p_k} frac{ partial omega}{ partial q_k} right) Rightarrow tag{4.1}$$ . $$ Rightarrow {q_i, p_j } = sum_{k}^{N} left( frac{ partial q_i}{ partial q_k} frac{ partial p_j}{ partial p_k} - frac{ partial q_i}{ partial p_k} frac{ partial p_j}{ partial q_k} right) = sum_{k}^{N} delta_{ik} delta_{jk} = delta_{ij}$$ . $$ Rightarrow {q_i, q_j } = sum_{k}^{N} left( frac{ partial q_i}{ partial q_k} frac{ partial q_j}{ partial p_k} - frac{ partial q_i}{ partial p_k} frac{ partial q_j}{ partial q_k} right) = 0 $$ . $$ Rightarrow {p_i, p_j } = 0 $$ . Onde temos então definidos os colchetes de poisson, e pontuadas suas propriedades mais diretas: . $$ {q_i, p_j } = delta_{ij} ; {q_i, q_j } = {p_i, p_j } = 0; tag{4.2} $$ . Transforma&#231;&#245;es Can&#244;nicas (TC) . E fácil então agora reescrever a partir dos colchetes de poisson as equações de Hamilton, de forma que: . $$ dot{q_i} = {q_i, mathcal{H} }; tag{4.3}$$ . $$ dot{p_i} = {p_i, mathcal{H} }; tag{4.4}$$ . Sejam então $(q,p) = (q_1,...,q_n; p_1,...,p_n)$ o conjunto de coordenadas necessárias para descrever um sistema, existe então também um $( bar{q}, bar{p}) = ( bar{q_1},..., bar{q_n}; bar{p_1},..., bar{p_n})$ que o descreve. . $$ begin{matrix} (q,p) &amp; rightarrow &amp; ( bar{q}, bar{p}) downarrow &amp; &amp; downarrow mathcal{H}(q,p) &amp; rightarrow &amp; mathbb{K}( bar{q}, bar{p}) end{matrix}$$ . Se tivermos então as equações de Hamilton para este novo sistema de coordenadas, isto é, . $$ dot{ bar{q_i}} = { bar{q}, mathbb{K} }; tag{4.5}$$ . $$ dot{ bar{p_i}} = { bar{p}, mathbb{K} }; tag{4.6}$$ . Essa transformação então é dita canônica, o que acarreta diretamente em . $$ { bar{q_i}, bar{p_j} } = delta_{ij} ; { bar{q_i}, bar{q_j} } = { bar{p_i}, bar{p_j} } = 0; tag{4.7} $$ . Podemos então agora escolher uma função geradora $ delta$ de modo a nova hamiltoniana $ mathbb{K}$ seja identicamente nula, e isso nos leva a equção de Hamilton-Jacobi: . $$ mathcal{H}(q,p) = - frac{ partial S}{ partial t} tag{4.8}$$ . onde temos que . $$ p = frac{ partial S}{ partial q} tag{4.9}$$ . Buscando uma motivação de visão ondulatória da já desenvolvida e consolidada mecânica clássico (como em paraleleo desenvolvido para a ótica), vamos buscar enxergar $S$ como uma fase de um processo ondulatório de um regime físico qualquer, isto é, . $$ psi = exp left({ frac{iS}{ hbar}} right) Rightarrow ln( psi) = frac{iS}{ hbar} Rightarrow S = -i hbar ln psi $$ . Isto é, . $$ frac{ partial S}{ partial t} =-i hbar left( frac{1}{ psi} frac{ partial psi}{ partial t} right) , frac{ partial S}{ partial q} =-i hbar left( frac{1}{ psi} frac{ partial psi}{ partial q} right) tag{4.10}$$ . Substituindo esses resultados nas equações (4.8) e (4.9) vamos obter . $$ mathcal{H} psi = i hbar frac{ partial psi}{ partial t} tag{4.11}$$ . $$p psi = i hbar frac{ partial psi}{ partial q} tag{4.12}$$ . Especificamente essas relações são de nosso grande interesse. Note que elas se assemelham ás equações da dinâmica de evolução de $| psi rangle$ . Equação de Schroedinger: $$ mathcal{H} | psi rangle = i hbar frac{ partial | psi rangle}{ partial t} tag{4.13}$$ . Operador momento atuando em $ mathcal{H_x}$: $$ langle psi | p | psi rangle = -i hbar frac{ partial psi}{ partial t} tag{4.14}$$ . E elas assim o fazem pelo postulado de coerência entre a MQ e a MC direto do Heisenberg picture. . $$ textrm{Formulação tradicional}$$ $$ hat{X}, hat{p}, H, t $$ . $$ mathcal{H} | psi(t) rangle = i hbar frac{ partial | psi (t) rangle}{ partial t} $$ . $$ langle x | hat{p} | psi(t) rangle = -i hbar frac{ partial psi (x,t)}{ partial x} $$ . $$ textrm{Formulação alternativa}$$ $$ x, P, hat{h}, hat{T}$$ . $$ mathcal{P} | phi(x) rangle = -i hbar frac{ partial | phi (x) rangle}{ partial x} $$ . $$ langle t | hat{h} | phi(t) rangle = i hbar frac{ partial phi (x,t)}{ partial t} tag{4.14}$$ . Na nossa formulação alternativa, o equivalente a equação de Schroedinger vai ser dito o &quot;pamiltoneano&quot;, sendo ela uma espécie de segunda lei de Newton para nossa nova formulação. . $x$ agora será tratado como um parâmetro, $t$ como um autovalor associado a $ hat{t}$, e as variáveis canônicamente conjugadas agora fazem com que . $$[ hat{h}, hat{t} ]= i hbar$$ . $$ begin{pmatrix} x hat{p} mathcal{H} | psi rangle end{pmatrix} Longleftrightarrow begin{pmatrix} -t hat{h} mathcal{P} | phi rangle end{pmatrix}$$ Essa formulação alternativa se mostra então tão poderosa quanto a mecânica quântica tradicional, mas é capaz de possibilitar a obtenção de resultados antes inacessiveís; Nada mais justo então do que reafirmar a transformação que propuzemos anteriormente em (3.6): . $$ mathcal{H} equiv mathcal{H_X} otimes mathcal{H_T}$$ . de forma que: . $$ mathcal{H_x} rightarrow | psi rangle $$ . $$ mathcal{H_t} rightarrow | phi rangle $$ . $$ mathcal{H} rightarrow | Psi rangle $$ . Desta forma, vamos tratar o produto interno dentro desse espaço como sendo: . $$|| Psi rangle = int_{ mathcal{}H_T} dt int_{ mathcal{}H_X} dx |x t rangle Psi(x,t) tag{4.15}$$ . Onde de $ Psi(x,t) rightarrow | Psi|^2 = mathcal{P}(x,t)$. . OBSERVA&#199;&#195;O . usando o teorema de Bayes: $$ mathcal{P}(x,t) = P_X(x|t)P_T(t) = P_T(t|x)P_X(x) $$ . $$ | Psi (x,t)|^2 = | psi (x,t)|^2 f(t) = | phi (t,x)|^2 g(x) $$ . $$ Psi (x,t) = psi (x,t) sqrt{f(t)} = phi (t,x) sqrt{g(x)} $$ . $$ Psi^* (x,t) = psi^* (x,t) sqrt{f(t)} = phi^* (t,x) sqrt{g(x)} $$ . Isto é, finalmente, o que buscavamos, . $$ |x t rangle equiv |x rangle otimes |t rangle $$ . No mais, já que sabemos que $ mathcal{H} equiv mathcal{H_X} otimes mathcal{H_T}$, devemos saber como atuam os operadores num autoket de $ mathcal{H}$ . $$ langle xt | hat{p} | Psi rangle = langle xt | hat{p} int dx^ prime int dt^ prime Psi(x^ prime,t^ prime) |x^ prime t^ prime rangle $$ . $$ = int dx^ prime int dt^ prime Psi(x^ prime,t^ prime) langle xt | hat{p}|x^ prime t^ prime rangle $$ . $$ = int dx^ prime int dt^ prime Psi(x^ prime,t^ prime) langle t | t^ prime rangle langle x | hat{p}|x^ prime rangle $$ . $$ = int dt^ prime langle t | t^ prime rangle int dx^ prime Psi(x^ prime,t^ prime) langle x | hat{p}|x^ prime rangle $$ . $$ = sqrt{f(t)} int dx^ prime Psi(x^ prime,t^ prime) langle x | hat{p}|x^ prime rangle $$ . $$ = sqrt{f(t)} langle x| hat{p} int dx^ prime Psi(x^ prime,t^ prime) |x^ prime rangle $$ . $$ Rightarrow langle xt | hat{p} | Psi rangle = sqrt{f(t)} langle x| hat{p} | psi(t) rangle $$ . Analogamente, também é idêntico ver que, . $$ Rightarrow langle xt | hat{h} | Psi rangle = sqrt{g(x)} langle t| hat{h} | phi(x) rangle $$ . 5. Deriva&#231;&#227;o de uma equa&#231;&#227;o din&#226;mica para $ phi(t,x)$ . Dada a construção de raciocinio até então, temos em nosso conhecimento que: . Equação de Schroedinger tempo-condicional: | . $$ mathcal{H} | psi(t) rangle = i hbar frac{ partial | psi (t) rangle}{ partial t} tag{5.1}$$ . temos aqui o tempo como parâmetro, e a equação trata de uma evolução temporal do ket $ psi$. . Equação de Schroedinger espaço-condicional: | . $$ mathcal{P} | phi(x) rangle = -i hbar frac{ partial | phi (x) rangle}{ partial x} tag{5.2}$$ . Agora, a posição é que será tratada como parâmetro, e a equação trata de uma evolução espacial do ket $ phi$. . Resta agora explicitar as transformações que envolvem o nosso novo operador Pamiltoniano $( mathcal{P})$, para isso iremos utilizar mais uma vez o formalismo Hamiltoniano da mecânica clássica como uma analogia para a construção da teoria. . Do fomrmalismo tradicional, temos que $$ mathcal{H}(q, p; t) = frac{p^2}{2m} + V(x,t) Rightarrow H (X, hat{p};t) = frac{ hat{p}^2}{2m} + V(x,t) tag{5.3}$$ . Podemos então isolar $p$ na equação acima e explicitá-lo em função de $ mathcal{H}$ e $V$ . $$p(t, mathcal{H};x) = pm sqrt{2m} ( mathcal{H} - V(t,x))^{1/2}$$ . $$ Rightarrow P(T, hat{h};x) = hat{ sigma_z} sqrt{2m} ( hat{h} - V(T,x))^{1/2} tag{5.4}$$ . Aplicando na equação de Schroedinger espaço condicional que desenvolvemos anteriormente, temos que . $$ sqrt{2m} hat{ sigma_z} [ hat{h} - V(T,x)]^{1/2} | phi (x) rangle = -i hbar frac{d}{dx} | phi (x) rangle tag{5.5}$$ . Note que ao projetarmos em $ langle t |$, vamos obter que . $$ sqrt{2m} hat{ sigma_z} [i hbar frac{ partial}{ partial t} - V(T,x)]^{1/2} phi (t,x) = -i hbar frac{ partial}{ partial x} phi (t,x) tag{5.6}$$ . Observa&#231;&#227;o: . O caráter matricial de $ hat{ sigma_z}$ é resultado apenas da definição dos dois polos da raíz $$ phi(t,x) = begin{pmatrix} phi^+ (t,x) phi^- (t,x) end{pmatrix} tag{5.7}$$ . Onde $ phi^+$ é a amplitude de porbabilidade da partícula incidir em um sentido no aparato localizado $x$, e $ phi^-$ no outro sentido. Claro que se $ rho (t,x)$ for a densidade total . $$ rho (x,t) = | phi^+|^2 + | phi^-|^2 = phi^{ dagger} phi tag{5.8}$$ . Da formulação tradicional, sabemos que $| psi (x,t)|^2 = | langle x | psi(t) rangle|^2 rightarrow$ equivale a densidade de probabilidade de detectarmos a partícula na posição x, dado que a medição ocorreu no instante t. Agora, nossa proposta chegou ao ponto de nos fornecer o ansatz de que $| phi (t,x)|^2 = | langle t | phi(x) rangle|^2 $, é a relação complementar dentro da nossa teoria estendida que nos fornece a densidade de probabilidade de detectarmos a partícula num instante de tempo t, dado que a medição ocorreu no espaço x. . Esse desenvolvimento nos permite então evoluir temporalmente a equação de Schroedinger Bayes-simétrica no espaço $ mathcal{H_t}$, e obter então uma previsão analítica do comportamento do tempo de tunelamento em um experimento para determinar o tempo de tunelamento, como faremos computacionalmente na sessão à seguir, e compararemos com alguns artigos já publicado que se propõe a efetuar esse experimento. . trabalho 1 . | trabalho 2 . | trabalho 3 . | . 6. Primeira tentativa de evolu&#231;&#227;o num&#233;rica com experimento de tempo de tunelamento . Tomando como partida o exercício 4.10 sugerido pelo livro do Foot de física atômica, vamos evoluir a equação de Schroedinger a partir do seguinte script em python . import numpy as np import matplotlib.pyplot as plt import scipy as sp . Chamando as seguintes constantes para parametrizar nosso problema, e definindo as condições iniciais para determinar o tempo de travessia . N = 100 Am = 1 step = 0.2 E = -0.25 . t = [0 for x in range(N)] t[0] = 0.2 Vt = [0 for x in range(N)] phi = [0 for x in range(N)] phi[0] = 5 . Usando como método de aproximação para o calculo numérico o seguinte procedimento para resolver a EDO e finalmente evoluir a nossa equação de Schroedinger como desejado: $$ frac{d f}{d t} = frac{f(t + delta/2)+f(t - delta/2)}{ delta} tag{6.1}$$ . $$ frac{d^2 f}{d t^2} = frac{f(t + delta)+f(t - delta)- 2f(t)}{ delta^2} tag{6.2}$$ . onde implementamos então esse algoritmo na nossa relação para $ phi(t,x)$ desenvolvido na sessão anterior . $$ sqrt{2m} hat{ sigma_z} [ hat{h} - V(T,x)]^{1/2} | phi (x) rangle = -i hbar frac{d}{dx} | phi (x) rangle$$ . de forma que a dependência no espaço que vai estar relacionada na solução ao fator $ exp[ frac{-iEx}{ hbar}]$ vai sempre se cancelar quando construirmos quantidades de relvância física ($| phi|^2$) logo todas as probabilidades e valres esperados são constantes neessa grandeza . for n in range (1,N): t[n] = t[n-1] + step Vt[n] = -(2/t[n-1]) + (Am*(Am+1)/(t[n-1]*t[n-1])) phi[n] = (2*phi[n-1]+(Vt[n-1] - E)*phi[n-1]*step*step - (1-step/t[n-1])*phi[n-2])/(1+step/t[n-1]) . Plotando agora para diferentes condições através da biblioteca matplotlib, podemos observar os seguintes resultados á se comparar com resultados de medições diretas de tempos de tunelamento, como prometemos no final da sessão anterior: . plt.plot(t,phi) plt.title(&quot;Analitycal tunneling time&quot;,fontsize = 16) plt.xlabel(&quot;$t$&quot;, fontsize = 12) plt.ylabel(&quot;$| phi(t) |^2$&quot;, fontsize = 12) plt.show() . 7. Desenvolvimento anal&#237;tico para solu&#231;&#227;o da equa&#231;&#227;o de Schroedinger espa&#231;o condicional . Nossa inteção agora se volta para propor uma solução analítica para a equação (5.6). Esse problema já foi analisado de diferentes formas, e mostrou-se que para problema de uma partícula livre, existe uma forma fechada para sua solução ref. sessão 2.4. Agora, vamos mostrar que para uma vasta gama de sistemas físicos, os quais estão sob ação de um potencial independente do tempo, é possível usar uma técnica similar para obter uma solução geral. Relembrando que: . $$ sqrt{2m} hat{ sigma_z} [i hbar frac{ partial}{ partial t} - V(T,x)]^{1/2} phi (t|x) = -i hbar frac{ partial}{ partial x} phi (t|x) $$ . De maneira análoga a referência para solução da partícula livre, podemos usar uma separação de variáveis $ phi(t|x) = e^{-i epsilon t/ hbar} phi_ epsilon (x)$, e pelo mesmo argumento da expansão em série vemos que . $$ hat{ sigma_z} sqrt{2m [ epsilon - V(x)]} phi_ epsilon (x) = -i hbar frac{d}{dx} phi_ epsilon (x) tag{7.1}$$ . Esta é uma equação diferencial de primeira ordem cuja solução é dada por . $$ phi_ epsilon (x) = frac{1}{ sqrt{2 pi hbar}} e^{(i/ hbar) hat{ sigma_z} int_{0}^{x} sqrt{2m [ epsilon - V(x)]} dx^{ prime}} tag{7.2}$$ . Portanto, como (7.2) ainda é uma equação linear, podemos escrever a solução geral de forma que . $$ phi (t|x) = frac{1}{ sqrt{2 pi hbar}} int d epsilon C_ epsilon e^{(i/ hbar) hat{ sigma_z} int_{0}^{x} sqrt{2m [ epsilon - V(x)]} dx^{ prime}-i epsilon t / hbar} tag{7.3}$$ . Ou, em termos do momento clássico $P( epsilon, x) = pm sqrt{2m epsilon} $ . $$ phi (t|x) = frac{1}{ sqrt{2 pi hbar}} int d epsilon C_ epsilon e^{(i/ hbar) int_{0}^{x} P( epsilon; x^{ prime}) dx^{ prime}-i epsilon t / hbar} tag{7.4}$$ . No intuito de resolver então analíticamente para qualquer potencial $V(x)$ basta realizar a integração no argumento da exponencial para obter a solução geral do problema. À primeira vista, essa simplicidade pode parecer estranha, afinal, para a equação de Schrödinger não temos uma solução com formato único para todo $V(x)$. Entretanto, este resultado é de certa forma esperado: primeiramente, analisando a Eq. (5.6), vemos que o potencial $V(x,t) = V(x)$ que escolhemos só depende do parâmetro condicional dessa equação, ou seja, da posição $x$. Dessa forma, se desejamos obter um resultado equivalente na equação de Schrödinger devemos escolher um potencial $V(x,t)$ que só dependa do parametro condicional t dessa equação, assim vamos encontrar o mesmo resultado na construção simétrica da mecânica quântica. . Prosseguindo na solução, vamos considerar primeiramente o caso mais simples como sendo o de uma partícula livre, onde o potencial será então dado por $V(x) = 0$. Se aplicarmos então essas condições a equação (7.3) e considerarmos apenas o ramo positivo da matriz de soluções $ hat{ sigma_z}$ vamos reduzir a mesma para . $$ phi (t|0) = frac{1}{ sqrt{2 pi hbar}} int d epsilon C_ epsilon e^{(-i epsilon t/ hbar)} tag{7.5} $$ . Note aqui que agora vamos prosseguir de forma muito semelhante ao que é feito na construção tradicional da teoria quântica, com o objetivo agora de evoluir espacialmente através da transformada de fourier a função de onda espaço condicional da nossa teoria . Tomando $$ phi (t|0) = frac{1}{ sqrt{2 pi hbar}} exp{ left[ frac{-(t-t_0)^2}{2 sigma^2} right]} exp{ left[ frac{i epsilon_0 t}{ hbar} right]} tag{7.6}$$ . Como condição inicial, onde perceba que setei uma gaussiana de desvio padrão $ sigma$ em $x=0$, seguimos aplicando a transformada de Fourier já substituindo para a coordenada $ epsilon$ de forma que . $$ phi(t|0) = int_{- infty}^{ infty} frac{d epsilon}{ sqrt{2 pi hbar}} psi( epsilon) e^{i epsilon t} tag{7.7} $$ . $$ psi( epsilon) equiv C_ epsilon = int_{- infty}^{ infty} frac{1}{ sqrt{2 pi hbar} } left ( frac{1}{ sqrt{2 pi hbar}} e^{-(t-t_0)^2/(2 sigma^2)} e^{(i epsilon_0 t)/ hbar} right ) e^{-i epsilon t} dt$$ . $$= frac{1}{ 2 pi hbar} int_{- infty}^{ infty} left ( e^{-(t-t_0)^2/(2 sigma^2)} e^{(i epsilon_0 t)/ hbar} right ) e^{-i epsilon t} dt$$ . $$ = frac{1}{ 2 pi hbar} frac{ exp{ left ( - frac{( epsilon_0 + hbar epsilon) left ( sigma^2( epsilon_0 + hbar epsilon)-2i hbar t_0 right ) }{2 hbar^2} right )}}{ sqrt{ frac{1}{ sigma^2}}}$$ . $$= frac{ sigma^2}{ 2 pi hbar} exp{ left ( - frac{( epsilon_0 + hbar epsilon) left ( sigma^2( epsilon_0 + hbar epsilon)-2i hbar t_0 right ) }{2 hbar^2} right )} tag{7.8}$$ . O que isso nos diz é que a função de onda temporal da gaussiana é uma superposição de diferentes energias, com a probabilidade de achar essas energias entre $ epsilon_1$ e $ epsilon_1 + d epsilon$ sendo proporcional a $ exp{ left ( - frac{( epsilon_0 + hbar epsilon) left ( sigma^2( epsilon_0 + hbar epsilon)-2i hbar t_0 right ) }{2 hbar^2} right )} d epsilon$. A nível de recordação, vale relembrar o desenvolvimento desse problema dentro do escopo da teoria tradicional, procedendo da mesma forma e aplicando uma transformada de fourier numa gaussiana e obtendo outra gaussiana. Perceba aqui que os resultados obtidos são, assim como propostos, simétricos e equivalentes. Vamos pontuar também que a nossa função de onda inicial (7.4) que representa a solução encontrada para a equação de Schroedinger espaço condicional representa uma superposição de diferentes ondas planas associadas a diferentes coeficientes (que usualmente nos referimos como amplitudes). . Agora, finalmente de posse dos coeficientes $C_ epsilon$ podemos substituir de volta os resultados em (7.5) e obter o resultado da função de onda espaço-condicional dada uma condição de contorno espaço-inicial gaussiana. . Como também temos o resultado geral para $ phi(t|x)$ no caso da partícula livre, podemos substituir em (7.4) o valor encontrado para $C_ epsilon$ e entender como a função de onda irá se propagar em um instante de tempo t, dado que iremos observar ela em um espaço conhecido x. . Resultado na condição inicial | . $$ phi (t|0) = frac{1}{ sqrt{2 pi hbar}} int d epsilon frac{ sigma^2}{ 2 pi hbar} exp{ left ( - frac{( epsilon_0 + hbar epsilon) left ( sigma^2( epsilon_0 + hbar epsilon)-2i hbar t_0 right ) }{2 hbar^2} right )} e^{(-i epsilon t/ hbar)}$$ . $$= frac{ sigma^2}{(2 hbar pi )^{3/2}} int d epsilon exp{ left ( - frac{( epsilon_0 + hbar epsilon) left ( sigma^2( epsilon_0 + hbar epsilon)-2i hbar t_0 right ) }{2 hbar^2} right )} e^{(-i epsilon t/ hbar)} $$ . $$= frac{ sigma^2}{(2 hbar pi )^{3/2}} frac{ sqrt{ pi} e^{ frac{-2 h t { epsilon_0}+ frac{ left(-t+i h^2 text{t0} right)^2}{ sigma ^2}}{ 2 h^4}} text{erf} left( frac{-t+h left( sigma ^2 (h epsilon +{ epsilon_0})-i h {t_0} right)}{ sqrt{2} h^2 sigma } right)}{ sqrt{2} sigma } tag{7.9}$$ onde $ text{erf}$ representa a função erro da gaussiana. Substituindo os limites de integração de menos infinito(?) até mais infinito, vamos obter . $$ phi (t|0) = frac{ sigma^2}{(2 hbar pi )^{3/2}} left[ dfrac{ sqrt{2} sqrt{{ pi}} mathrm{i} mathrm{e}^{- frac{t^2}{2 sigma^2 hbar^2}+ frac{t_0 t}{ sigma^2 hbar}- frac{t_0^2}{2 sigma^2}} sin left( frac{ epsilon_0 t}{ hbar^2} right)}{ sigma}+ dfrac{ sqrt{2} sqrt{{ pi}} mathrm{e}^{- frac{t^2}{2 sigma^2 hbar^2}+ frac{t_0 t}{ sigma^2 hbar}- frac{t_0^2}{2 sigma^2}} cos left( frac{ epsilon_0 t}{ hbar^2} right)}{ sigma} right]$$ . $$ = frac{ sigma}{(2 hbar pi )^{3/2}} sqrt{2 pi} mathrm{e}^{- frac{t^2}{2 sigma^2 hbar^2}+ frac{t_0 t}{ sigma^2 hbar}- frac{t_0^2}{2 sigma^2}} left( mathrm{i} sin left( frac{ epsilon_0 t}{ hbar^2} right)+ cos left( frac{ epsilon_0 t}{ hbar^2} right) right) tag{7.10}$$ . Para auxiliar na visualização do sistema, segue o plot dessa função de onda em x = 0: . . Com respectiva densidade de probabilidade . $$| phi(t|0)|^2 = frac{ sigma^2}{(2 hbar pi )^{3}} dfrac{2{ pi} mathrm{e}^{2 mathcal{Re} left(- frac{t^2}{2 sigma^2 hbar^2}+ frac{t_0t}{ sigma^2 hbar}- frac{t_0^2}{2 sigma^2} right)} left| mathrm{i} sin left( frac{ epsilon_0 t}{ hbar^2} right)+ cos left( frac{ epsilon_0 t}{ hbar^2} right) right|^2}{ sigma^2} = frac{2{ pi} mathrm{e}^{2 mathcal{Re} left(- frac{t^2}{2 sigma^2 hbar^2}+ frac{t_0t}{ sigma^2 hbar}- frac{t_0^2}{2 sigma^2} right)} left| mathrm{i} sin left( frac{ epsilon_0 t}{ hbar^2} right)+ cos left( frac{ epsilon_0 t}{ hbar^2} right) right|^2}{(2 hbar pi )^{3}} tag{7.11}$$ . Observe que até aqui, o resultado segue coerente visto que nossa expressão continua fazendo juz a gaussiana da condição inicial: . . Resultado espaço evoluído | . $$ phi (t|x) = frac{1}{ sqrt{2 pi hbar}} int d epsilon frac{ sigma^2}{ 2 pi hbar} exp{ left ( - frac{( epsilon_0 + hbar epsilon) left ( sigma^2( epsilon_0 + hbar epsilon)-2i hbar t_0 right ) }{2 hbar^2} right )} e^{(i/ hbar) sqrt{2m epsilon} x-i epsilon t / hbar } $$ . $$= frac{ sigma^4}{(2 hbar pi )^{3/2}} int d epsilon exp{ left ( - frac{( epsilon_0 + hbar epsilon) left ( sigma^2( epsilon_0 + hbar epsilon)-2i hbar t_0 right ) }{2 hbar^2} right )} e^{(i/ hbar) sqrt{2m epsilon} x-i epsilon t / hbar}$$ . $$ = text{integral não tem solução analítica} blacksquare $$ $$ text{(note que ela não divergiu, apenas não tem solução analítica. Integrando numericamente para todas as constantes = 1 obtemos para a condição inicial: 1.390521824852123 + 0.2928372699906291 i)} $$ . No intuito de eliminar a raíz do expoente que nos impossibilita essa integração, podemos então utilizar da construção de $ epsilon$ que como $P = pm sqrt{2 m epsilon}$, isto é, $ epsilon_p = P^2/2m $. Efetuando essa substituição vamos obter que: . $$ epsilon_p = frac{P^2}{2m} Rightarrow d epsilon = frac{P}{m} dP tag{7.12}$$ . Então, procedendo com a integração de 0 até $ infty$ (ao substituir o $P^2$ do expoente de $C_P$ . $$ phi (t|x) = frac{1}{ sqrt{2 pi hbar}} int dP frac{P }{ m} frac{ sigma^2}{ 2 pi hbar} exp{ left ( - frac{(P_0 + frac{ hbar P^2}{2 m}) left ( sigma^2(P_0 + frac{ hbar P^2}{2 m})-2i hbar t_0 right ) }{2 hbar^2} right )} e^{(i/ hbar) P x - i P^2 t /2 m hbar } $$ . $$ phi (t|x) = frac{ sigma^2}{ (2 pi hbar)^{3/2} m} int dP P exp{ left ( - frac{(P_0 + frac{ hbar P^2}{2 m}) left ( sigma^2(P_0 + frac{ hbar P^2}{2 m})-2i hbar t_0 right ) }{2 hbar^2} right )} e^{(i/ hbar) P x - i P^2 t /2 m hbar } $$ . Observação: continuarei de forma analítica aqui, todavia fui capaz de resolver/plotar computacionalmente a integral acima; posso então trabalhar em gráficos complementares analiticos/computacionais a partir disso . Setando o instante de tempo inicial $t_0$ como zero da nossa condição inicial gaussiana temporal, podemos simplificar nossa integral sem perder nenhum tipo de generalidade ou aplicação do problema, podendo então escrever a expressão como . $$ phi (t|x) = frac{ sigma^2}{ (2 pi hbar)^{3/2} m} int dP P exp{ left ( - frac{ sigma^2(P_0 + frac{ hbar P^2}{2 m})^2 }{ 2 hbar^2} right )} e^{(i/ hbar) P x - i P^2 t /2 m hbar } tag{7.13}$$ . Considerando a energia inicial como sendo $ epsilon_0 = 0$, isso nos leva de $ epsilon_0 = P_0^2/2m$ em $P_0 = 0$, que simplifica nossa integral mais uma vez em: . $$ phi (t|x) = frac{ sigma^2}{ (2 pi hbar)^{3/2} m} int dP P exp{ left ( - frac{ sigma^2 hbar^4 P^4}{ m} right )}e^{ - i P^2 t /2 m hbar } e^{(i/ hbar) P x} tag{7.14}$$ . $$ textrm{erro: expoente com fator de 2 faltando}$$ Podemos reescrever o argumento das exponenciais que vão com $P^2$ da seguinte maneira: . $$ left ( - frac{ sigma^2 P^2}{ 2m hbar} right ) - frac{i P^2 t }{2 m} = - frac{b P^2}{2} $$ . tomando $ b = frac{1}{m hbar} left ( sigma^2 + i t right )$, podemos simplificar ambas as exponenciais para . $$ phi (t|x) = frac{ sigma^2}{ (2 pi hbar)^{3/2} m} int dP P e^{- frac{b P^2}{2}} e^{(i/ hbar) P x} $$ . cujo resultado da integral acima sabemos de integrais tabeladas como sendo: . $$ I = frac{1}{b} - frac{ sqrt{ frac{ pi}{2}} x e^{-x^2/(2 b^2)} left( text{erfi} left( frac{x}{ sqrt{2 b}} right) - i right) }{b^{3/2}}$$ . Fazendo então com que o resultado da função de onda espaço evoluída seja . $$ phi (t|x) = frac{ sigma^2}{ (2 pi hbar)^{3/2} m} left[ frac{m hbar}{ sigma^2 + it} - frac{ sqrt{ frac{ pi}{2}} (m hbar)^{3/2} x e^{- frac{x^2 (m hbar)^2}{(2 ( sigma^2+it)^2)}} left( text{erfi} left( x sqrt{ frac{ sigma^2 + it }{2 m hbar}} right) - i right) }{( sigma^2 + it)^{3/2}} right] tag{7.14-1}$$ . (Tenho plots para mostrar tanto da função de onda quanto da densidade de probabilidade - posso mostrar 1 a 1, a animação ainda não tive tempo de implementar para vocês poderem visualizar a evolução) . Com respectiva densidade de probabilidade : . $$| phi(t|x)|^2 = frac{ sigma^4}{ (2 pi hbar)^{3}m^2 } left[ dfrac{ left(bx mathrm{e}^ frac{x^2}{ mathrm{i}t+ sigma^2} operatorname{erfi} left(x sqrt{ mathrm{i}t+ sigma^2} right)-hm sqrt{ mathrm{i}t+ sigma^2}- mathrm{i} right)^2}{ left| mathrm{i}t+ sigma^2 right|^3} right] tag{7.15}$$ . Para efeito de visualização, segue um plot do formato geral da função de onda evoluída em x = 1, utilizando 1 para todas as constantes e evoluindo ela numericamente em t: . . segue também o plot da densidade de probabilidade na posição x = 1, . . Note aqui que utilizamos embutido no resultado espacialmente evoluído a ideia de um propagador espacial para o sistema no intuito de obter $ phi (t|x)$. A nível de curiosidade, vou desenvolver de forma geral a ideia de um propagador no escopo da teoria espelhada, e obter um resultado coerente com o que podemos observar em (7.10): para $x_0 = 0$, o propagador se torna $e^{(i/ hbar) sqrt{2m epsilon} x} $. . $$ textrm{Curiosidade: Desenvolvendo um propagador}$$ Perceba também que o termo da transformada $ exp (i epsilon t)$, está associado para cada coeficiente $C_ epsilon$ com uma autofunção do nosso novo Pamiltoniano com autovalor em forma de $E_ epsilon = sqrt{2 m epsilon}/ hbar$. Logo, nós sabemos que $ exp (i epsilon t)$ deve evoluir no espaço com uma dependência espacial dada por $e^{-iE_ epsilon x / hbar}$, devido ao formato do nosso Pamiltoniano, como mostraremos à seguir . Na intenção de examinar como um estado irá evoluir espacialmente, vamos restabelecer a definição de um propagador em nossa nova teoria como sendo um operador linear escrito como $ hat{U}(x,x_0)$, tal que . $$| phi(x) rangle = hat{U}(x,x_0) | phi(x_0) rangle tag{a.1}$$ . podemos também inferir a seguinte propriedade: . $$ hat{U}(x_0,x_0) = hat{I} tag{a.2}$$ . onde $ hat{I}$ é o operador unidade (identidade) . No propósito de avançar no que haviamos proposto anteriormente, precismos encontrar a forma analítica desse operador $ hat{U}(x,x_0)$, para isso, vamos simplesmente substituí-lo (7.9) na relação do Pamiltoniano de (4.14), isto é . $$ mathcal{ hat{P}} left ( hat{U}(x,x_0)| phi(x_0) rangle right ) = -i hbar frac{ partial }{ partial x} left ( hat{U}(x,x_0)| phi(x_0) rangle right ) tag {a.3}$$ . ou, . $$ frac{ partial hat{U}(x,x_0)}{ partial x} = frac{i}{ hbar} hat{P} hat{U}(x,x_0) tag{a.4}$$ . Assumindo o Pamiltoniano espaço independente podemos proceder de forma parecida a MQ tradicional e integrar facilmente essa equação diferencial de forma a obter que . $$ boxed{ hat{U}(x,x_0) = e^{i(x-x_0) hat{P}/ hbar} longmapsto e^{i(x-x_0) sqrt{2m epsilon}/ hbar}} tag{a.5}$$ . A partir de agora, usaremos esse operador propagador de forma equivalente ao que fazemos para operar evoluções temporais na MQ tradicional, todavia, o faremos agora para evoluções espaciais no escopo da teoria espelhada. . $$ textrm{Fim}$$ Agora, seguindo em diante com o resultado encontrado, vamos verificar se o mesmo encontra-se tempo normalizado. Para isso, vamos analisar . $$ int_{- infty}^{ infty} | phi(t|x)|^2 dt = 1 tag{7.16}$$ . Como ainda não resolvemos a questão da integração para a função de onda espaço evoluída, vamos analisar o caso condicionla de $x = 0$ que já podemos normalizar no tempo. Prosseguindo com: . $$ int_{- infty}^{ infty} | phi(t|0)|^2 dt = 1 tag{7.17}$$ . $$1 = int_{- infty}^{ infty} frac{2{ pi} mathrm{e}^{2 mathcal{Re} left(- frac{t^2}{2 sigma^2 hbar^2}+ frac{t_0t}{ sigma^2 hbar}- frac{t_0^2}{2 sigma^2} right)} left| mathrm{i} sin left( frac{ epsilon_0 t}{ hbar^2} right)+ cos left( frac{ epsilon_0 t}{ hbar^2} right) right|^2}{(2 hbar pi )^{3}} dt tag{7.18}$$ . assumindo então $ sigma, hbar, epsilon_0, t, t_0$ como reais podemos escrever nossa integral para a forma: . $$ frac{ sigma^2}{(2 hbar pi )^{3}} int_{- infty}^{ infty} dfrac{2{ pi} mathrm{e}^{- frac{t^2}{ sigma^2 hbar^2}+ frac{2t_0t}{ sigma^2 hbar}- frac{t_0^2}{ sigma^2}}}{ sigma^2} dt tag{7.19}$$ . Contraindo os termos do expoente no formato de quadrado da soma, pode convenientemente nos ajudar a enchergar uma função de erro de gauss, isto é: . $$ frac{ sigma^2}{(2 hbar pi )^{3}} int_{- infty}^{ infty} dfrac{2{ pi} mathrm{e}^{- frac{ left(t-t_0 hbar right)^2}{ sigma^2 hbar^2}}}{ sigma^2} dt Rightarrow frac{ sigma^4}{(2 hbar pi )^{3}} dfrac{{ pi}^ frac{3}{2} hbar operatorname{erf} left( frac{t}{ sigma hbar}- frac{t_0}{ sigma} right)}{ sigma} tag{7.20}$$ . Substituindo os limites de integração, podemos então facilmente ver que . $$ int_{- infty}^{ infty} | phi(t|0)|^2 dt = frac{ sigma^2}{(2 hbar pi )^{3}} dfrac{2{ pi}^ frac{3}{2} hbar}{ sigma} = frac{ sigma}{4 hbar^2 pi^{3/2}} tag{7.21}$$ . Logo, a constante de normalização pode ser simplesmente expressada através de $4 hbar^2 pi^{3/2}/ sigma$. . import cmath i = 0 + 1j print(&#39;e^i =&#39;, cmath.exp(i)) print (i) . e^i = (0.5403023058681398+0.8414709848078965j) 1j . def f(x): return x**2 print(&#39;f&#39;, x) . (-1) ** 0.5 . (6.123233995736766e-17+1j) . import quadpy import math as math from math import e from math import pi from math import sqrt import numpy as np from scipy import integrate import matplotlib.pyplot as plt import cmath from scipy.optimize import fmin #definindo unidade imaginária: i = 0 + 1j #criando a função para o plot def plot_results(sigma = 1, Xp = 1, m=1, P_0 = 1, hbar = 1): #preparando a função que vai ser integrada para a integração def f(x, t): res = (sigma**2)/(2*pi*hbar)**(3/2)*x*e**(-(P_0 + hbar*(x**2)/(2*m))*(sigma**2)*(P_0 + hbar*(x**2)/(2*m))/(2*hbar**2))*cmath.exp((i/hbar)*x*Xp - i*(x**2)*t/(2*m*hbar)) return res integral = np.vectorize(f) #efetuando a integral def F(t): xmin = 0 xmax = 10 y,err = quadpy.quad(lambda x: integral(x,t), xmin, xmax) return y Fvec = np.vectorize(F) &quot;&quot;&quot; def piu(x): return fmin(Fvec, 0, args=(x)) #print(&#39;piu&#39;, x) &quot;&quot;&quot; return Fvec X0 = plot_results(1,0,1,1,1) X1 = plot_results(1,5,1,1,1) X2 = plot_results(1,10,1,1,1) X3 = plot_results(1,15,1,1,1) t = np.linspace(-5,40,200) plt.plot(t, abs(X0(t))**2, label=&#39;x=0&#39;) plt.plot(t, abs(X1(t))**2, label=&#39;x=5&#39;) plt.plot(t, abs(X2(t))**2, label=&#39;x=10&#39;) plt.plot(t, abs(X3(t))**2, label=&#39;x=15&#39;) plt.legend() plt.title(&#39;Numerical evolution of the probability density of Gaussian wave packet&#39;) plt.ylabel(&#39;$| phi(t|x)|^2$&#39;) plt.xlabel(&#39;$t$&#39;) . Text(0.5, 0, &#39;$t$&#39;) . from scipy.integrate import quad from math import exp, cos, sin, pi, sqrt import numpy as np import matplotlib.pyplot as plt #defina aqui as constantes: sigma = 1 hbar = 1 m = 1 P_0 = 0 i = 0 + 1j x = 8 #preparando a função que vai ser integrada para a integração, dividida em real e imaginária #parte real: def re(p, t): RE = (sigma**2)/(2*pi*hbar)**(3/2)*p*exp(-(P_0 + hbar*(p**2)/(2*m))*(sigma**2)*(P_0 + hbar*(p**2)/(2*m))/(2*hbar**2))*cos((1/hbar)*p*x - 1*(p**2)*t/(2*m*hbar)) return RE integral1 = np.vectorize(re) #parte imaginária def im(p, t): IM = (sigma**2)/(2*pi*hbar)**(3/2)*p*exp(-(P_0 + hbar*(p**2)/(2*m))*(sigma**2)*(P_0 + hbar*(p**2)/(2*m))/(2*hbar**2))*sin((1/hbar)*p*x - 1*(p**2)*t/(2*m*hbar)) return IM integral2 = np.vectorize(im) #efetuando a integral em p e passando t para ser o argumento da função de onda def F(t): min = 0 max = 10 y, err = quad(integral1, min, max, args=(t)) return y def M(t): min = 0 max = 10 y, err = quad(integral2, min, max, args=(t)) return y def OP(t): return abs(F(t) + M(t)*i)**2 OPvec = np.vectorize(OP) t = np.linspace(-0,20,200) plt.plot(t, OPvec(t)) plt.title(&#39;Numerical evolution of the probability density of Gaussian wave packet&#39;) plt.ylabel(&#39;$| phi(t|x)|^2$&#39;) plt.xlabel(&#39;$t$&#39;) . Text(0.5, 0, &#39;$t$&#39;) . from scipy.integrate import quad from math import exp, cos, sin, pi import numpy as np import matplotlib.pyplot as plt #defina aqui as constantes: sigma = 1 hbar = 1 m = 1 P_0 = 0 i = 0 + 1j n = 15 irio = 100 piu = np.zeros(irio) valor_x = np.zeros(n) maximum = np.zeros(n) tuntime = np.zeros(n) x = 5 #preparando a função que vai ser integrada para a integração, dividida em real e imaginária #parte real: def re(p, t): RE = (sigma**2)/(2*pi*hbar)**(3/2)*p*exp(-(P_0 + hbar*(p**2)/(2*m))*(sigma**2)*(P_0 + hbar*(p**2)/(2*m))/(2*hbar**2))*cos((1/hbar)*p*x - 1*(p**2)*t/(2*m*hbar)) return RE integral1 = np.vectorize(re) #parte imaginária def im(p, t): IM = (sigma**2)/(2*pi*hbar)**(3/2)*p*exp(-(P_0 + hbar*(p**2)/(2*m))*(sigma**2)*(P_0 + hbar*(p**2)/(2*m))/(2*hbar**2))*sin((1/hbar)*p*x - 1*(p**2)*t/(2*m*hbar)) return IM integral2 = np.vectorize(im) #efetuando a integral em p e passando t para ser o argumento da função de onda def F(t): min = 0 max = 20 y, err = quad(integral1, min, max, args=(t)) return y def M(t): min = 0 max = 20 y, err = quad(integral2, min, max, args=(t)) return y def OP(t): return abs(F(t) + M(t)*i)**2 OPvec = np.vectorize(OP) #integrando em t para obter a função de normalização def norm(t): min = -20 max = 20 y, err = quad(OPvec, min, max) return y #normalizando def graph(t): return OP(t)/norm(t) graph_vec = np.vectorize(graph) for k in range (irio): piu[k] = graph(k) tuntime = np.argmax(piu) print(tuntime) maximum = np.amax(piu) print(maximum) #organizando o plot da função em questão t = np.linspace(-5,15,200) plt.scatter(t, graph_vec(t), s = 2) plt.ylabel(&#39;$| phi(t|x)|^2$&#39;) plt.xlabel(&#39;$t$&#39;) . 4 0.21089492298047324 . Text(0.5, 0, &#39;$t$&#39;) .",
            "url": "https://resteche.github.io/REsteche_blog/quantum%20mechanics/quantum%20foundations/time%20distributions/2021/12/19/Teoria_STS.html",
            "relUrl": "/quantum%20mechanics/quantum%20foundations/time%20distributions/2021/12/19/Teoria_STS.html",
            "date": " • Dec 19, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Animating the Lorenz Attractor with Python",
            "content": "Edward Lorenz, the father of chaos theory, once described chaos as “when the present determines the future, but the approximate present does not approximately determine the future.” . Lorenz first discovered chaos by accident while developing a simple mathematical model of atmospheric convection, using three ordinary differential equations. He found that nearly indistinguishable initial conditions could produce completely divergent outcomes, rendering weather prediction impossible beyond a time horizon of about a fortnight. . In 1963, Lorenz developed a simple mathematical model for the way air moves around in the atmosphere, governed by the following equations: . $$ frac{dx}{dt} = sigma left ( y - x right )$$ . $$ frac{dy}{dt} =x left ( rho - z right ) - y$$ . $$ frac{dz}{dt} =xy - beta z tag{1}$$ . Now known as the Lorenz System, this model demonstrates chaos at certain parameter values and its attractor is fractal. The animation we gone develop here depicts this system’s behavior over time in Python, using scipy to integrate the differential equations, matplotlib to draw the 3D plots, and pillow to create the animated GIF. . In three dimensions, these trajectories never overlap and the system never lands on the same point twice, due to its fractal geometry. We can also look at this attractor in two dimensions with matplotlib as I will show next . On the construction of the visualization of differente planes of the Lorenz attractor . import numpy as np import matplotlib.pyplot as plt def lorenz(x, y, z, sigma=10, rho=28, beta=2.667): x_dot = sigma*(y - x) y_dot = rho*x - y - x*z z_dot = x*y - beta*z return x_dot, y_dot, z_dot . Given that: . x, y, z: a point of interest in three dimensional space sigma, rho, beta: parameters defining the lorenz attractor | this function wil returns: x_dot, y_dot, z_dot: values of the lorenz attractor&#39;s partial derivatives at the point x, y, z . | . dt = 0.01 num_steps = 2000 . We need one more to set the initial values . xs = np.empty(num_steps + 1) ys = np.empty(num_steps + 1) zs = np.empty(num_steps + 1) xs[0], ys[0], zs[0] = (0., 1., 1.05) . Step through &quot;time&quot;, calculating the partial derivatives at the current point and using them to estimate the next point . for i in range(num_steps): x_dot, y_dot, z_dot = lorenz(xs[i], ys[i], zs[i]) xs[i + 1] = xs[i] + (x_dot * dt) ys[i + 1] = ys[i] + (y_dot * dt) zs[i + 1] = zs[i] + (z_dot * dt) . Now, proceeding to plot: . ax = plt.figure().add_subplot(projection=&#39;3d&#39;) ax.scatter(xs, ys, zs, s = 2, c = plt.cm.jet(zs/max(zs))) ax.plot(xs, ys, zs, color = &#39;grey&#39;) ax.set_xlabel(&quot;X Axis&quot;) ax.set_ylabel(&quot;Y Axis&quot;) ax.set_zlabel(&quot;Z Axis&quot;) ax.set_title(&quot;Lorenz Attractor&quot;) plt.show() . Now we plot two-dimensional cuts of the three-dimensional phase space, redefining iteration to make graphs even more beautiful . dt = 0.01 num_steps = 10000 fig, ax = plt.subplots(1, 3, sharex=False, sharey=False, figsize=(17, 6)) # plot the x values vs the y values ax[0].plot(xs, ys, color=&#39;r&#39;, alpha=0.7, linewidth=0.3) ax[0].set_title(&#39;x-y phase plane&#39;) # plot the x values vs the z values ax[1].plot(xs, zs, color=&#39;m&#39;, alpha=0.7, linewidth=0.3) ax[1].set_title(&#39;x-z phase plane&#39;) # plot the y values vs the z values ax[2].plot(ys, zs, color=&#39;b&#39;, alpha=0.7, linewidth=0.3) ax[2].set_title(&#39;y-z phase plane&#39;) plt.show() . Animating the Lorenz Attractor . %matplotlib inline import numpy as np, matplotlib.pyplot as plt, glob, os import IPython.display as IPdisplay, matplotlib.font_manager as fm from scipy.integrate import odeint from mpl_toolkits.mplot3d.axes3d import Axes3D from PIL import Image . defining fonts to use for plots just beacuse we can . family = &#39;Myriad Pro&#39; title_font = fm.FontProperties(family=family, style=&#39;normal&#39;, size=20, weight=&#39;normal&#39;, stretch=&#39;normal&#39;) . save_folder = &#39;images/lorenz-animate&#39; if not os.path.exists(save_folder): os.makedirs(save_folder) . Define the initial system state, initial parameters and the time points to solve for, evenly spaced between the start and end times . initial_state = [0.1, 0, 0] # sigma, rho, and beta sigma = 10. rho = 28. beta = 8./3. # time points start_time = 1 end_time = 60 interval = 100 time_points = np.linspace(start_time, end_time, end_time * interval) . Now for the Lorenz system: . def lorenz_system(current_state, t): x, y, z = current_state dx_dt = sigma * (y - x) dy_dt = x * (rho - z) - y dz_dt = x * y - beta * z return [dx_dt, dy_dt, dz_dt] . Plot the system line simplificated in three dimensions . def plot_lorenz(xyz, n): fig = plt.figure(figsize=(12, 9)) ax = fig.gca(projection=&#39;3d&#39;) ax.xaxis.set_pane_color((1,1,1,1)) ax.yaxis.set_pane_color((1,1,1,1)) ax.zaxis.set_pane_color((1,1,1,1)) x = xyz[:, 0] y = xyz[:, 1] z = xyz[:, 2] ax.plot(x, y, z, color=&#39;g&#39;, alpha=0.7, linewidth=0.7) ax.set_xlim((-30,30)) ax.set_ylim((-30,30)) ax.set_zlim((0,50)) ax.set_title(&#39;Lorenz system attractor&#39;, fontproperties=title_font) plt.savefig(&#39;{}/{:03d}.png&#39;.format(save_folder, n), dpi=60, bbox_inches=&#39;tight&#39;, pad_inches=0.1) plt.close() . return a list in iteratively larger chunks, and handle them to facilitate future animation . def get_chunks(full_list, size): size = max(1, size) chunks = [full_list[0:i] for i in range(1, len(full_list) + 1, size)] return chunks chunks = get_chunks(time_points, size=20) # get the points to plot, one chunk of time steps at a time, by integrating the system of equations points = [odeint(lorenz_system, initial_state, chunk) for chunk in chunks] # plot each set of points, one at a time, saving each plot for n, point in enumerate(points): plot_lorenz(point, n) . Animation: . Create an animated gif of all the plots then display it inline . first_last = 100 #show the first and last frames for 100 ms standard_duration = 5 #show all other frames for 5 ms durations = tuple([first_last] + [standard_duration] * (len(points) - 2) + [first_last]) . # load all the static images into a list images = [Image.open(image) for image in glob.glob(&#39;{}/*.png&#39;.format(save_folder))] gif_filepath = &#39;images/animated-lorenz-attractor.gif&#39; . gif = images[0] gif.info[&#39;duration&#39;] = durations #ms per frame gif.info[&#39;loop&#39;] = 0 #how many times to loop (0=infinite) gif.save(fp=gif_filepath, format=&#39;gif&#39;, save_all=True, append_images=images[1:]) . Image.open(gif_filepath).n_frames == len(images) == len(durations) . IPdisplay.Image(url=gif_filepath) . .",
            "url": "https://resteche.github.io/REsteche_blog/chaos%20theory/butterfly%20effect/python%20animation/2021/10/20/Lorenz_animation.html",
            "relUrl": "/chaos%20theory/butterfly%20effect/python%20animation/2021/10/20/Lorenz_animation.html",
            "date": " • Oct 20, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Statistical mechanics",
            "content": "Compilado de atividades computacionais executadas durante a discipina de mecânica estatística ministradas pelo professor Paulo Campos. . Atividade 1 . Objetivo: Simular computacionalmente um número M de caminhantes aleatórios unidimensionais capazes de realizar N passos, e plotar histogramas variados de cada uma dessas simulações, assim como calcular média e variância para cada uma delas. . | Sobre a minha simulação: Foram utilizados como parâmetros aqui todos os caminhantes centrados na posição 0, com uma probabilidade de 50% de tomar um passo para a direita e 50% de tomar um passo para esquerda. Para o algoritmo de decisão implementei um método de Monte Carlo a partir da função geradora de números aleatórios (rand()) tomando passos temporais no intervalo de 0.1. Conduzi as simulações em C usando o script referenciado a seguir, e gerei arquivos para plotar os histogramas em um software externo. . | . #include&lt;math.h&gt; #include&lt;stdio.h&gt; #include&lt;time.h&gt; int main(){ int i,j,N=1000,M=1000; double p,t,r,x=0,xm, xd; FILE*random; random = fopen(&quot;caminhante.txt&quot;,&quot;w+&quot;); printf(&quot;GERADOR DE ARQUIVO CAMINHANTE ALEATÓRIO n n&quot;); printf(&quot;escolha uma probabilidade de passo: n&quot;); scanf(&quot;%lf&quot;, &amp;p); printf(&quot;escolha um tamanho para os passos: n&quot;); scanf(&quot;%lf&quot;, &amp;t); srand(time(NULL)); //lembrar: M caminhantes realizando N passos! for(j=0;j&lt;M;j++){ x=0; xm=0; xd=0; for (i=0;i&lt;N;i++){ r = rand()/((double)RAND_MAX); if(p&lt;r){ x=x+t; } else{ x=x-t; } } fprintf(random,&quot;%d %lf n&quot;, j, x); xm = (xm + x); xd = (xd + x*x); } xm = xm/M; xd = xd/M; printf(&quot; n%lf n&quot;,xm); printf(&quot;%lf n&quot;,xd); fclose(random); printf(&quot;seu arquivo foi criado com sucesso! :) n&quot;); return (0); } . Simula&#231;&#227;o 1 . Parâmetros: M = 1000, N = 100, p = 0.5, t = 0.1 | . . Aqui, podemos observar os cálculos para média e variância como sendo: $&lt;x&gt;$ = 0.0012 e $ sigma^2$ = 0.00144 . Histograma: | . . Simula&#231;&#227;o 2 . Parâmetros: M = 1000, N = 200, p = 0.5, t = 0.1 | . . Aqui, podemos observar os cálculos para média e variância como sendo: $&lt;x&gt;$ = 0.0006 e $ sigma^2$ = 0.00036 . Histograma: | . . Simula&#231;&#227;o 3 . Parâmetros: M = 1000, N = 1000, p = 0.5, t = 0.1 | . . Aqui, podemos observar os cálculos para média e variância como sendo: $&lt;x&gt;$ = −0.002200 e $ sigma^2$ = 0.0048 . Histograma: | . . Atividade 2 . Objetivo: Dada uma distribuição de probabilidade específica, selecione um número M de pontos arbitrários (onde fixamos M = 1000) onde cada ponto corresponde a média de N números aleatórios (N = 20, 100, 500); plote de forma gráfica a média móvel e observe que, conforme o número N cresce, a distribuição fica mais bem definida e a largura da distribuição também muda. Distribuições a serem analisadas: Gaussiana, Gamma, exponencial e uniforme. | Sobre a minha simulação: Primeiramente, elaborei um código em C para gerar números aleatórios como usualmente através da função rand() e da biblioteca &lt;time.h&gt; . Ao calcular as médias dos valores uando a função geradora de números aleatórios (double)RAND_MAX) implementei funções para cada uma das funções de distribuição de probabilidade e iterei ela 1000 vezes como requisitado, salvando os resultados num arquivo .txt com o título da função na qual estava chamando naquela execução. Refiz o procedimento para cada função 3 vezes criando arquivos com N = 20, 100 e 1000. Com os arquivos em mãos, utilizei um software externo de plotagem para gerar cada um dos gráficos exibidos no decorrer do trabalho. Código: segue como exemplo o meu código usado com parâmetros fixados para a função de distribuição de probabilidade gaussiana com N = 20. | . #include&lt;math.h&gt; #include&lt;stdio.h&gt; #include&lt;time.h&gt; double gaussdp(double g){ g = exp(pow(-g,2)); return g; } double gammadp(double m){ if(m &lt;= 0.0){ return 0; } if(m &gt; 0.0){ m = tgamma(m); return m; } } double expdp(double e){ if(e &lt;= 0.0){ return 0; } if(e &gt; 0.0){ e = exp(-e); return e; } } double uniformdp(double u){ //usando como parametros a = 0.10 e b = 0.75 if(u &lt; 0.10){ return 0; } if(u &gt; 0.75){ return 1; } if(0.10 &lt;= u &amp;&amp; u &lt;= 0.75){ u = (u-0.1)/0.65; return u; } } int main(){ int i,j,N=20,M=1000; double r,x,xm, Xm; FILE*random; random = fopen(&quot;fdp_gaussianaN20.txt&quot;,&quot;w+&quot;); //lembrar: calcular média de N num aleatorios para gerar M pontos da distribuição! srand(time(NULL)); for(j=0;j&lt;M;j++){ xm = 0; for(j=0;j&lt;N;j++){ r = rand()/((double)RAND_MAX); x = (gaussdp(r)); xm = xm + x; } xm = xm/N; Xm = Xm + xm; fprintf(random,&quot;%d %lf n&quot;, j, xm); } Xm = Xm/M; printf(&quot;a média e igual a: %lf&quot;, Xm); fclose(random); printf(&quot;seu arquivo foi criado com sucesso! :) n&quot;); return (0); } . . . . . . . Atividade 3 - Modelo de Ising . Objetivo: Elaborar um programa para simular o modelo de Ising em uma rede quadrada de duas dimensões, implementando o algoritmo de Monte Carlo, usando um sistema ferromagnético com condições de contorno periódicas. Devemos iniciar a partir de diferentes configurações iniciais, isto é, os spins devem ser aleatoriamente distribuídos; todos de mesmos valores; parte com valor 𝑠1 e parte com valor 𝑠2. Plote gráficos para analisar o comportamento do calor específico e da susceptibilidade magnética em diferentes regimes de temperatura para um dado tamanho de rede N (n x n), mantendo registro de quantos passos de Monte Carlo são executados. (utilizado como parâmetro para essa construção uma parte da atividade proposta em [2]). . | Sobre a simulação : Como sabemos, o modelo de Ising consiste numa rede de sítios interconectados, cada sítio podendo assumir dois valores distintos $𝑠_1$ e $𝑠_2$. Note que temos aqui apenas uma simples representação de um sistema magnético onde o spin s, pode assumir valores -1 e 1. A energia (E) do sistema e a sua magnetização (M) são estabelecidas através das seguintes relações . | . $$ E = - J sum^N_{&lt;i,j&gt;} s_i s_j - B sum^N_{i = 1} s_j tag{1}$$ . $$ M = sum^N_{i = 1} s_i tag{2}$$ . Onde &lt; 𝑖,𝑗 &gt; representam as interações somente entre os primeiros vizinhos, J é a constante de troca do sistema (para sistemas ferromagnéticos temos 𝐽 &gt; 0 e para antiferromagnéticos 𝐽 &lt; 0), e B um campo magnético externo qualquer. O sistema é uma das variações mais simples que apresentam uma transição de fase do tipo ordem-desordem, o que permite que usemos ele para representar inclusive outros sistemas físicos que possuem esse mesmo tipo de transição, como gás de rede, interações sociais, economia, neurociência etc. [4] . #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;string.h&gt; #include &lt;time.h&gt; #include &lt;math.h&gt; #define N 30 //tamanho da rede -1 linha e -1 coluna que vão ser perididas nas iterações #define Nt 784 //constante pra preencher as bordas periódicamente #define J 0.5 //constante ferromagnetismo(+/-); dividimos por 2 para evitar repeticao energias #define B 0 #define NE 2 //Vetor para guardar as energias do passo atual e posterior #define T 5 #define C 10000 //Número de iterações using namespace std; void periodico (int M[N][N]){ //Tornando uma matriz quadrada periódica int i,j; for(j=1;j&lt;(N-1);j++){ M[0][j] = M[N-2][j]; M[N-1][j] = M[1][j]; } for(i=1;i&lt;(N-1);i++){ M[i][0] = M[i][N-2]; M[i][N-1] = M[i][1]; } M[0][0] = M[N-2][N-2]; M[0][N-1] = M[N-2][1]; M[N-1][0] = M[1][N-2]; M[N-1][N-1] = M[1][1]; } void energia (int M[N][N], double Energias[NE], double MEM[4], double t){ //Função onde ocorre as mudanças de energia int i, j, prob, prob2, de, O[N][N]; double parametro, sorteio, beta, Eprov; for(i=0;i&lt;N;i++){ for(j=0;j&lt;N;j++){ O[i][j] = M[i][j]; } } prob = rand()%(N-2) + 1; //escolhendo um spin aleatório da rede prob2 = rand()%(N-2) + 1; Energias[0] = 0; Energias[1] = 0; for(i=0;i&lt;4;i++){ MEM[i] = 0; } for(i=1;i&lt;(N-1);i++){ //Calculando a energia atual for(j=1;j&lt;(N-1);j++){ Energias[0] += -J*((M[i][j]*M[i][j+1]) + (M[i][j]*M[i+1][j]) + (M[i][j]*M[i][j-1]) + (M[i][j]*M[i-1][j])) - B*M[i][j]; } } O[prob][prob2] = -O[prob][prob2]; //Alterando o spin escolhido for(i=1;i&lt;(N-1);i++){ //Calculando a nova energia for(j=1;j&lt;(N-1);j++){ Energias[1] += -J*((O[i][j]*O[i][j+1]) + (O[i][j]*O[i+1][j]) + (O[i][j]*O[i][j-1]) + (O[i][j]*O[i-1][j])) - B*O[i][j]; } } de = Energias[1] - Energias[0]; //Diferença de energia entre os estados if(de&gt;0){ //Analisando o caso onde a energia aumenta sorteio = rand()%RAND_MAX; sorteio = sorteio/RAND_MAX; parametro = exp(-de/t); if(sorteio&lt;=parametro){ M[prob][prob2] = O[prob][prob2]; } } for(i=1;i&lt;(N-1);i++){ //Iterando E, E^2, M e M^2 for(j=1;j&lt;(N-1);j++){ Eprov = -J*((M[i][j]*M[i][j+1]) + (M[i][j]*M[i+1][j]) + (M[i][j]*M[i][j-1]) + (M[i][j]*M[i-1][j])) - B*M[i][j]; MEM[0] += Eprov; MEM[1] += Eprov*Eprov; MEM[2] += M[i][j]; MEM[3] += M[i][j]*M[i][j]; } } } int main(){ int i, j, M[N][N], k; //k é o contador de iterações double prob, t, Energias[NE], MEM[4]; //MEM guarda as grandezas desejadas (E, E^2, M e M^2) FILE* data = fopen(&quot;Dados.txt&quot;, &quot;w+&quot;); srand((unsigned)time(NULL)); for(t=0.5;t&lt;=T;t+=0.1){ //Loop para analisar diferentes temperaturas for(i=0;i&lt;NE;i++){ //Zerando os dados Energias[i] = 0; } for(i=0;i&lt;4;i++){ MEM[i] = 0; } for(i=1;i&lt;(N-1);i++){ //Definindo a população inicial for(j=1;j&lt;(N-1);j++){ /*prob = rand()%RAND_MAX; prob = prob/RAND_MAX; if(prob&lt;=0.5){ M[i][j] = 1; //isso daqui foi de quando estavamos gerando condição incial aleatoria, antes da dica do professor de começar todos em +/-1 } else { M[i][j] = -1; }*/ M[i][j] = 1; } } periodico(M); for(k=0;k&lt;C;k++){ // Loop das iterações energia(M,Energias,MEM,t); periodico(M); } //fprintf(data,&quot;%g %g %g %g %g&quot;,t,MEM[0]/Nt,fabs(MEM[2])/Nt, (float)((MEM[1]/Nt)-(MEM[0]*MEM[0])/(Nt*Nt))/(t*t), (float)((MEM[3]/Nt)- (MEM[2]*MEM[2])/(Nt*Nt))/t); //desistimos desse printf por conta de correlação fprintf(data,&quot;%g %g &quot;,t,MEM[0]/Nt); for(k=0;k&lt;500;k++){ // Loop das iterações energia(M,Energias,MEM,t); periodico(M); } fprintf(data,&quot;%g &quot;,fabs(MEM[2])/Nt); for(k=0;k&lt;500;k++){ // Loop das iterações energia(M,Energias,MEM,t); periodico(M); } fprintf(data,&quot;%g &quot;,(float)((MEM[1]/Nt)-(MEM[0]*MEM[0])/(Nt*Nt))/(t*t)); for(k=0;k&lt;500;k++){ // Loop das iterações energia(M,Energias,MEM,t); periodico(M); } fprintf(data,&quot;%g&quot;,(float)((MEM[3]/Nt)-(MEM[2]*MEM[2])/(Nt*Nt))/t); fprintf(data,&quot; n&quot;); cout &lt;&lt; t &lt;&lt; &quot; ok&quot; &lt;&lt; endl; } fclose(data); } . Trabalho gráfico : Nessa sessão podemos observar como prometido os plots referentes as propriedades de susceptibilidade magnética e calor específico para fazermos uma análise quantitativa mais eficiente do nosso sistema. Para isso, consideramos que o sistema precisa rodar o algoritmo de metropolis várias vezes até estar termalizado, algoritmo esse que consiste no seguinte passo a passo: | . Testamos uma modificação aleatória no sistema; | Calculamos a diferença de energia devido a esta modificação; | Se ∆𝐸 ≤ 0, a nova configuração é aceita; | Se ∆𝐸 &gt; 0, é preciso calcular 𝜔 = exp(−𝛽∆𝐸) e gerar um número aleatório 𝑟 ∈ [0,1] uniformemente. Se 𝑟 ≤ 𝜔 a nova configuração é aceita; se 𝑟 &gt; 𝜔 a modificação é descartada. | Como vamos calcular as grandezas &lt; 𝐸 &gt;, &lt; $𝐸^2$ &gt;, &lt; 𝑀 &gt; 𝑒 &lt; $𝑀^2$ &gt; durante nossa simulação, precisamos garantir que um número razoável de passos de Monte Carlo ocorra para o sistema estar em um estado termalizado [1][2], para somente então começarmos a calcular essas médias. Após isso, os observáveis são calculados de acordo com as seguintes fórmulas: . Para o Calor específico (C), vamos ter . $$ C = frac{&lt;E^2&gt; - &lt;E&gt;^2}{k_b T^2}$$ . E para a Susceptibilidade magnética (𝜒) . $$ chi = = frac{&lt;M^2&gt; - &lt;M&gt;^2}{k_b T} $$ . Ambos os gráficos vão ser gerados em função da temperatura que variamos no código/sistema. . . . Dado a determinação coerente com o esperado desses observáveis, também tomamos a liberdade de introduzir os gráficos que são geralmente referenciados na literatura [1][3] para efeito de comparação do bom andamento da simulação do nosso sistema. . . . Acreditamos que esses gráficos são igualmente importantes para a visualização e um bom entendimento do funcionamento do sistema proposto. A temperatura crítica 𝑇𝑐 que caracteriza a perda das propriedades coletivas dos 𝑠𝑝𝑖𝑛𝑠 e consequentemente marca a perda do fenômeno de histerese [5], é coerente em todos os gráficos apresentados (e pode ser determinada com precisão dado um maior poder de processamento para gerar uma rede de spins maior que N=100). . Refer&#234;ncias . Antunes, Felipe. (2012). Estudo do Modelo de Ising Bidimensional Utilizando o Algorítimo de Metropolis. 10.13140/RG.2.1.2354.1840. | Cabral, Leonardo. (2018.1). Módulo 11 – Simulação de Ensembles estatísticos utilizando Monte Carlo. Notas de aula da disciplina Métodos computacionais para física. | Barry M. McCoy and Tai Tsun Wu (1973), The Two-Dimensional Ising Model. Harvard University Press, Cambridge Massachusetts, ISBN 0-674-91440-6. | «PiresMA/Ising_like_models_interdisciplinary_applications». GitHub. Consultado em 12 de julho de 2021. | Modelo Ising – Wikipédia, a enciclopédia livre (wikipedia.org) |",
            "url": "https://resteche.github.io/REsteche_blog/modelo%20de%20ising/chaos%20theory/drunk%20walker/maths/2021/07/20/Statistical_mechanics.html",
            "relUrl": "/modelo%20de%20ising/chaos%20theory/drunk%20walker/maths/2021/07/20/Statistical_mechanics.html",
            "date": " • Jul 20, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "O som do átomo de hidrogênio",
            "content": "Inspirado pelo vídeo do minute physics decidi fazer esse mini projeto para mostrar como fazer projetos envolvendo faixas de som audíveis no próprio navegador, totalmente em python através do Jupyter Notebook. . Para isso, vamos usar o espectro do átomo de hidrogênio, e vamos fazer um shift linear nele para dentro do espectro audível de frequências sonoras. O nome dado a esse processo é sonificação, você pode usar o link para saber mais caso se interesse. . %matplotlib inline . from __future__ import division, print_function, absolute_import import scipy.constants as const import numpy as np import scipy from matplotlib.pyplot import plot from scipy.io import wavfile from IPython.core.display import HTML, display . try: from IPython.display import Audio def wavPlayer(data, rate): display(Audio(data, rate=rate)) except ImportError: pass . from numpy import sin, pi rate = 44100 # freq = 44.1 khz duration = 3 # duração do audio # função para normalizar o seno na amplitude certa para um arquivo wav normedsin = lambda f,t : 2**13*sin(2*pi*f*t) time = np.linspace(0,duration, num=rate*duration) . Primeiro teste para o wav Player . Para começar, vamos começar tocando algo em 440 Hz . import numpy import struct import warnings . la = lambda t : normedsin(440,t) # plot rápido da onda nos primeiros 25 ms plot(time[0:1000], la(time)[0:1000]) ampl = la(time).astype(np.int16) # função que salva o áudio e toca ele em HTML5 wavPlayer(ampl, rate) . Your browser does not support the audio element. As diferentes frequências emitidas por um átomo de hidrogênio são analiticamente calculadas através da formula de Rydberg: . $$ frac{1}{ lambda} = R left( frac{1}{n_1} - frac{1}{n_2} right) $$ . Como sabemos, essa relação tem um formato muito similar a das freqências emitidas do hidrogênio . $$ f_{n,m} = frac{c}{ lambda} = frac{R_h}{h} left( frac{1}{n} - frac{1}{m} right) $$ . Para o caso de $n = 1$ reconhecemos a famosa série de Lyman, e para $n = 2$ a série de Balmer . f0 = const.Rydberg*const.c print(&quot;A maior frequência do hidrogênio é &quot;,f0,&quot;Hz. e corresponde a n = 1, m = ∞&quot;) fshift = 440 print(&quot;todavia odemos shiftar o espectro para 440 Hz (A)&quot;) . A maior frequência do hidrogênio é 3289841960250880.5 Hz. e corresponde a n = 1, m = ∞ todavia odemos shiftar o espectro para 440 Hz (A) . ryd = lambda n,m : fshift*(1/(n**2) -1/(m**2)) flyman = lambda x : ryd(1,x) fbalmer = lambda x : ryd(2,x) . ser = lambda t : sum( [normedsin(flyman(i),t)+normedsin(fbalmer(i+1),t) for i in range(2,8)]) # e uma função em forma de vetor para realizar operações em cada um dos seus elementos serv = scipy.vectorize(ser) . ss= serv(time) . plot(time, ss) ss = 2**15*ss/ss.max() . wavPlayer(ss.astype(np.int16),rate) . Your browser does not support the audio element.",
            "url": "https://resteche.github.io/REsteche_blog/estrutura%20da%20mat%C3%A9ria/f%C3%ADsica%20moderna/f%C3%ADsica%20atomica/musica/2021/06/10/Som_hidrog%C3%AAnio.html",
            "relUrl": "/estrutura%20da%20mat%C3%A9ria/f%C3%ADsica%20moderna/f%C3%ADsica%20atomica/musica/2021/06/10/Som_hidrog%C3%AAnio.html",
            "date": " • Jun 10, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "fastcore: An Underrated Python Library",
            "content": ". Background . I recently embarked on a journey to sharpen my python skills: I wanted to learn advanced patterns, idioms, and techniques. I started with reading books on advanced Python, however, the information didn&#39;t seem to stick without having somewhere to apply it. I also wanted the ability to ask questions from an expert while I was learning -- which is an arrangement that is hard to find! That&#39;s when it occurred to me: What if I could find an open source project that has fairly advanced python code and write documentation and tests? I made a bet that if I did this it would force me to learn everything very deeply, and the maintainers would be appreciative of my work and be willing to answer my questions. . And that&#39;s exactly what I did over the past month! I&#39;m pleased to report that it has been the most efficient learning experience I&#39;ve ever experienced. I&#39;ve discovered that writing documentation forced me to deeply understand not just what the code does but also why the code works the way it does, and to explore edge cases while writing tests. Most importantly, I was able to ask questions when I was stuck, and maintainers were willing to devote extra time knowing that their mentorship was in service of making their code more accessible! It turns out the library I choose, fastcore is some of the most fascinating Python I have ever encountered as its purpose and goals are fairly unique. . For the uninitiated, fastcore is a library on top of which many fast.ai projects are built on. Most importantly, fastcore extends the python programming language and strives to eliminate boilerplate and add useful functionality for common tasks. In this blog post, I&#39;m going to highlight some of my favorite tools that fastcore provides, rather than sharing what I learned about python. My goal is to pique your interest in this library, and hopefully motivate you to check out the documentation after you are done to learn more! . Why fastcore is interesting . Get exposed to ideas from other languages without leaving python: I’ve always heard that it is beneficial to learn other languages in order to become a better programmer. From a pragmatic point of view, I’ve found it difficult to learn other languages because I could never use them at work. Fastcore extends python to include patterns found in languages as diverse as Julia, Ruby and Haskell. Now that I understand these tools I am motivated to learn other languages. | You get a new set of pragmatic tools: fastcore includes utilities that will allow you to write more concise expressive code, and perhaps solve new problems. | Learn more about the Python programming language: Because fastcore extends the python programming language, many advanced concepts are exposed during the process. For the motivated, this is a great way to see how many of the internals of python work. | A whirlwind tour through fastcore . Here are some things you can do with fastcore that immediately caught my attention. . . Making **kwargs transparent . Whenever I see a function that has the argument **kwargs, I cringe a little. This is because it means the API is obfuscated and I have to read the source code to figure out what valid parameters might be. Consider the below example: . def baz(a, b=2, c=3, d=4): return a + b + c def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, **kwargs)&gt; . Without reading the source code, it might be hard for me to know that foo also accepts and additional parameters b and d. We can fix this with delegates: . def baz(a, b=2, c=3, d=4): return a + b + c @delegates(baz) # this decorator will pass down keyword arguments from baz def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, b=2, d=4)&gt; . You can customize the behavior of this decorator. For example, you can have your cake and eat it too by passing down your arguments and also keeping **kwargs: . @delegates(baz, keep=True) def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, b=2, d=4, **kwargs)&gt; . You can also exclude arguments. For example, we exclude argument d from delegation: . def basefoo(a, b=2, c=3, d=4): pass @delegates(basefoo, but=[&#39;d&#39;]) # exclude `d` def foo(c, a, **kwargs): pass inspect.signature(foo) . &lt;Signature (c, a, b=2)&gt; . You can also delegate between classes: . class BaseFoo: def __init__(self, e, c=2): pass @delegates()# since no argument was passsed here we delegate to the superclass class Foo(BaseFoo): def __init__(self, a, b=1, **kwargs): super().__init__(**kwargs) inspect.signature(Foo) . &lt;Signature (a, b=1, c=2)&gt; . For more information, read the docs on delegates. . . Avoid boilerplate when setting instance attributes . Have you ever wondered if it was possible to avoid the boilerplate involved with setting attributes in __init__? . class Test: def __init__(self, a, b ,c): self.a, self.b, self.c = a, b, c . Ouch! That was painful. Look at all the repeated variable names. Do I really have to repeat myself like this when defining a class? Not Anymore! Checkout store_attr: . class Test: def __init__(self, a, b, c): store_attr() t = Test(5,4,3) assert t.b == 4 . You can also exclude certain attributes: . class Test: def __init__(self, a, b, c): store_attr(but=[&#39;c&#39;]) t = Test(5,4,3) assert t.b == 4 assert not hasattr(t, &#39;c&#39;) . There are many more ways of customizing and using store_attr than I highlighted here. Check out the docs for more detail. . P.S. you might be thinking that Python dataclasses also allow you to avoid this boilerplate. While true in some cases, store_attr is more flexible.1 . 1. For example, store_attr does not rely on inheritance, which means you won&#39;t get stuck using multiple inheritance when using this with your own classes. Also, unlike dataclasses, store_attr does not require python 3.7 or higher. Furthermore, you can use store_attr anytime in the object lifecycle, and in any location in your class to customize the behavior of how and when variables are stored.↩ . . Avoiding subclassing boilerplate . One thing I hate about python is the __super__().__init__() boilerplate associated with subclassing. For example: . class ParentClass: def __init__(self): self.some_attr = &#39;hello&#39; class ChildClass(ParentClass): def __init__(self): super().__init__() cc = ChildClass() assert cc.some_attr == &#39;hello&#39; # only accessible b/c you used super . We can avoid this boilerplate by using the metaclass PrePostInitMeta. We define a new class called NewParent that is a wrapper around the ParentClass: . class NewParent(ParentClass, metaclass=PrePostInitMeta): def __pre_init__(self, *args, **kwargs): super().__init__() class ChildClass(NewParent): def __init__(self):pass sc = ChildClass() assert sc.some_attr == &#39;hello&#39; . . Type Dispatch . Type dispatch, or Multiple dispatch, allows you to change the way a function behaves based upon the input types it receives. This is a prominent feature in some programming languages like Julia. For example, this is a conceptual example of how multiple dispatch works in Julia, returning different values depending on the input types of x and y: . collide_with(x::Asteroid, y::Asteroid) = ... # deal with asteroid hitting asteroid collide_with(x::Asteroid, y::Spaceship) = ... # deal with asteroid hitting spaceship collide_with(x::Spaceship, y::Asteroid) = ... # deal with spaceship hitting asteroid collide_with(x::Spaceship, y::Spaceship) = ... # deal with spaceship hitting spaceship . Type dispatch can be especially useful in data science, where you might allow different input types (i.e. Numpy arrays and Pandas dataframes) to a function that processes data. Type dispatch allows you to have a common API for functions that do similar tasks. . Unfortunately, Python does not support this out-of-the box. Fortunately, there is the @typedispatch decorator to the rescue. This decorator relies upon type hints in order to route inputs the correct version of the function: . @typedispatch def f(x:str, y:str): return f&#39;{x}{y}&#39; @typedispatch def f(x:np.ndarray): return x.sum() @typedispatch def f(x:int, y:int): return x+y . Below is a demonstration of type dispatch at work for the function f: . f(&#39;Hello &#39;, &#39;World!&#39;) . &#39;Hello World!&#39; . f(2,3) . 5 . f(np.array([5,5,5,5])) . 20 . There are limitations of this feature, as well as other ways of using this functionality that you can read about here. In the process of learning about typed dispatch, I also found a python library called multipledispatch made by Mathhew Rocklin (the creator of Dask). . After using this feature, I am now motivated to learn languages like Julia to discover what other paradigms I might be missing. . . A better version of functools.partial . functools.partial is a great utility that creates functions from other functions that lets you set default values. Lets take this function for example that filters a list to only contain values &gt;= val: . test_input = [1,2,3,4,5,6] def f(arr, val): &quot;Filter a list to remove any values that are less than val.&quot; return [x for x in arr if x &gt;= val] f(test_input, 3) . [3, 4, 5, 6] . You can create a new function out of this function using partial that sets the default value to 5: . filter5 = partial(f, val=5) filter5(test_input) . [5, 6] . One problem with partial is that it removes the original docstring and replaces it with a generic docstring: . filter5.__doc__ . &#39;partial(func, *args, **keywords) - new function with partial application n of the given arguments and keywords. n&#39; . fastcore.utils.partialler fixes this, and makes sure the docstring is retained such that the new API is transparent: . filter5 = partialler(f, val=5) filter5.__doc__ . &#39;Filter a list to remove any values that are less than val.&#39; . . Composition of functions . A technique that is pervasive in functional programming languages is function composition, whereby you chain a bunch of functions together to achieve some kind of result. This is especially useful when applying various data transformations. Consider a toy example where I have three functions: (1) Removes elements of a list less than 5 (from the prior section) (2) adds 2 to each number (3) sums all the numbers: . def add(arr, val): return [x + val for x in arr] def arrsum(arr): return sum(arr) # See the previous section on partialler add2 = partialler(add, val=2) transform = compose(filter5, add2, arrsum) transform([1,2,3,4,5,6]) . 15 . But why is this useful? You might me thinking, I can accomplish the same thing with: . arrsum(add2(filter5([1,2,3,4,5,6]))) . You are not wrong! However, composition gives you a convenient interface in case you want to do something like the following: . def fit(x, transforms:list): &quot;fit a model after performing transformations&quot; x = compose(*transforms)(x) y = [np.mean(x)] * len(x) # its a dumb model. Don&#39;t judge me return y # filters out elements &lt; 5, adds 2, then predicts the mean fit(x=[1,2,3,4,5,6], transforms=[filter5, add2]) . [7.5, 7.5] . For more information about compose, read the docs. . . A more useful __repr__ . In python, __repr__ helps you get information about an object for logging and debugging. Below is what you get by default when you define a new class. (Note: we are using store_attr, which was discussed earlier). . class Test: def __init__(self, a, b=2, c=3): store_attr() # `store_attr` was discussed previously Test(1) . &lt;__main__.Test at 0x7ffcd766cee0&gt; . We can use basic_repr to quickly give us a more sensible default: . class Test: def __init__(self, a, b=2, c=3): store_attr() __repr__ = basic_repr(&#39;a,b,c&#39;) Test(2) . Test(a=2, b=2, c=3) . . Monkey Patching With A Decorator . It can be convenient to monkey patch with a decorator, which is especially helpful when you want to patch an external library you are importing. We can use the decorator @patch from fastcore.foundation along with type hints like so: . class MyClass(int): pass @patch def func(self:MyClass, a): return self+a mc = MyClass(3) . Now, MyClass has an additional method named func: . mc.func(10) . 13 . Still not convinced? I&#39;ll show you another example of this kind of patching in the next section. . . A better pathlib.Path . When you see these extensions to pathlib.path you won&#39;t ever use vanilla pathlib again! A number of additional methods have been added to pathlib, such as: . Path.readlines: same as with open(&#39;somefile&#39;, &#39;r&#39;) as f: f.readlines() | Path.read: same as with open(&#39;somefile&#39;, &#39;r&#39;) as f: f.read() | Path.save: saves file as pickle | Path.load: loads pickle file | Path.ls: shows the contents of the path as a list. | etc. | . Read more about this here. Here is a demonstration of ls: . from fastcore.utils import * from pathlib import Path p = Path(&#39;.&#39;) p.ls() # you don&#39;t get this with vanilla Pathlib.Path!! . (#7) [Path(&#39;2020-09-01-fastcore.ipynb&#39;),Path(&#39;README.md&#39;),Path(&#39;fastcore_imgs&#39;),Path(&#39;2020-02-20-test.ipynb&#39;),Path(&#39;.ipynb_checkpoints&#39;),Path(&#39;2020-02-21-introducing-fastpages.ipynb&#39;),Path(&#39;my_icons&#39;)] . Wait! What&#39;s going on here? We just imported pathlib.Path - why are we getting this new functionality? Thats because we imported the fastcore.utils module, which patches this module via the @patch decorator discussed earlier. Just to drive the point home on why the @patch decorator is useful, I&#39;ll go ahead and add another method to Path right now: . @patch def fun(self:Path): return &quot;This is fun!&quot; p.fun() . &#39;This is fun!&#39; . That is magical, right? I know! That&#39;s why I&#39;m writing about it! . . An Even More Concise Way To Create Lambdas . Self, with an uppercase S, is an even more concise way to create lambdas that are calling methods on an object. For example, let&#39;s create a lambda for taking the sum of a Numpy array: . arr=np.array([5,4,3,2,1]) f = lambda a: a.sum() assert f(arr) == 15 . You can use Self in the same way: . f = Self.sum() assert f(arr) == 15 . Let&#39;s create a lambda that does a groupby and max of a Pandas dataframe: . import pandas as pd df=pd.DataFrame({&#39;Some Column&#39;: [&#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, ], &#39;Another Column&#39;: [5, 7, 50, 70]}) f = Self.groupby(&#39;Some Column&#39;).mean() f(df) . Another Column . Some Column . a 6 | . b 60 | . Read more about Self in the docs). . . Notebook Functions . These are simple but handy, and allow you to know whether or not code is executing in a Jupyter Notebook, Colab, or an Ipython Shell: . from fastcore.imports import in_notebook, in_colab, in_ipython in_notebook(), in_colab(), in_ipython() . (True, False, True) . This is useful if you are displaying certain types of visualizations, progress bars or animations in your code that you may want to modify or toggle depending on the environment. . . A Drop-In Replacement For List . You might be pretty happy with Python&#39;s list. This is one of those situations that you don&#39;t know you needed a better list until someone showed one to you. Enter L, a list like object with many extra goodies. . The best way I can describe L is to pretend that list and numpy had a pretty baby: . define a list (check out the nice __repr__ that shows the length of the list!) . L(1,2,3) . (#3) [1,2,3] . Shuffle a list: . p = L.range(20).shuffle() p . (#20) [8,7,5,12,14,16,2,15,19,6...] . Index into a list: . p[2,4,6] . (#3) [5,14,2] . L has sensible defaults, for example appending an element to a list: . 1 + L(2,3,4) . (#4) [1,2,3,4] . There is much more L has to offer. Read the docs to learn more. . But Wait ... There&#39;s More! . There are more things I would like to show you about fastcore, but there is no way they would reasonably fit into a blog post. Here is a list of some of my favorite things that I didn&#39;t demo in this blog post: . Utilities . The Basics section contain many shortcuts to perform common tasks or provide an additional interface to what standard python provides. . mk_class: quickly add a bunch of attributes to a class | wrap_class: add new methods to a class with a simple decorator | groupby: similar to Scala&#39;s groupby | merge: merge dicts | fasttuple: a tuple on steroids | Infinite Lists: useful for padding and testing | chunked: for batching and organizing stuff | . Multiprocessing . The Multiprocessing section extends python&#39;s multiprocessing library by offering features like: . progress bars | ability to pause to mitigate race conditions with external services | processing things in batches on each worker, ex: if you have a vectorized operation to perform in chunks | . Functional Programming . The functional programming section is my favorite part of this library. . maps: a map that also composes functions | mapped: A more robust map | using_attr: compose a function that operates on an attribute | . Transforms . Transforms is a collection of utilities for creating data transformations and associated pipelines. These transformation utilities build upon many of the building blocks discussed in this blog post. . Further Reading . It should be noted that you should read the main page of the docs first, followed by the section on tests to fully understand the documentation. . The fastcore documentation site. | The fastcore GitHub repo. | Blog post on delegation. | . Shameless plug: fastpages . This blog post was written entirely in a Jupyter Notebook, which GitHub automatically converted into to a blog post! Sound interesting? Check out fastpages. .",
            "url": "https://resteche.github.io/REsteche_blog/fastcore/",
            "relUrl": "/fastcore/",
            "date": " • Sep 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hello again 👋, . In case you want to know more about my recent work, you can check my github here . I promiss I’ll do my best to keep it updated! . Remember you can contact me through my institutional or professional email anytime for doubts or sugestions. . Make sure you check out everything that interests you on my site again! I hope you like my content. . Reference link to my Lattes platform . | My current department website . | .",
          "url": "https://resteche.github.io/REsteche_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://resteche.github.io/REsteche_blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}