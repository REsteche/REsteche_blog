{
  
    
        "post0": {
            "title": "O som do átomo de hidrogênio",
            "content": "Inspirado pelo vídeo do minute physics decidi fazer esse mini projeto para mostrar como fazer projetos envolvendo faixas de som audíveis no próprio navegador, totalmente em python através do Jupyter Notebook. . Para isso, vamos usar o espectro do átomo de hidrogênio, e vamos fazer um shift linear nele para dentro do espectro audível de frequências sonoras. O nome dado a esse processo é sonificação, você pode usar o link para saber mais caso se interesse. . %matplotlib inline . from __future__ import division, print_function, absolute_import import scipy.constants as const import numpy as np import scipy from matplotlib.pyplot import plot from scipy.io import wavfile from IPython.core.display import HTML, display . try: from IPython.display import Audio def wavPlayer(data, rate): display(Audio(data, rate=rate)) except ImportError: pass . from numpy import sin, pi rate = 44100 # freq = 44.1 khz duration = 3 # duração do audio # função para normalizar o seno na amplitude certa para um arquivo wav normedsin = lambda f,t : 2**13*sin(2*pi*f*t) time = np.linspace(0,duration, num=rate*duration) . Primeiro teste para o wav Player . Para começar, vamos começar tocando algo em 440 Hz . import numpy import struct import warnings . la = lambda t : normedsin(440,t) # plot rápido da onda nos primeiros 25 ms plot(time[0:1000], la(time)[0:1000]) ampl = la(time).astype(np.int16) # função que salva o áudio e toca ele em HTML5 wavPlayer(ampl, rate) . Your browser does not support the audio element. As diferentes frequências emitidas por um átomo de hidrogênio são analiticamente calculadas através da formula de Rydberg: . $$ frac{1}{ lambda} = R left( frac{1}{n_1} - frac{1}{n_2} right) $$ . Como sabemos, essa relação tem um formato muito similar a das freqências emitidas do hidrogênio . $$ f_{n,m} = frac{c}{ lambda} = frac{R_h}{h} left( frac{1}{n} - frac{1}{m} right) $$ . Para o caso de $n = 1$ reconhecemos a famosa série de Lyman, e para $n = 2$ a série de Balmer . f0 = const.Rydberg*const.c print(&quot;A maior frequência do hidrogênio é &quot;,f0,&quot;Hz. e corresponde a n = 1, m = ∞&quot;) fshift = 440 print(&quot;todavia odemos shiftar o espectro para 440 Hz (A)&quot;) . A maior frequência do hidrogênio é 3289841960250880.5 Hz. e corresponde a n = 1, m = ∞ todavia odemos shiftar o espectro para 440 Hz (A) . ryd = lambda n,m : fshift*(1/(n**2) -1/(m**2)) flyman = lambda x : ryd(1,x) fbalmer = lambda x : ryd(2,x) . ser = lambda t : sum( [normedsin(flyman(i),t)+normedsin(fbalmer(i+1),t) for i in range(2,8)]) # e uma função em forma de vetor para realizar operações em cada um dos seus elementos serv = scipy.vectorize(ser) . ss= serv(time) . plot(time, ss) ss = 2**15*ss/ss.max() . wavPlayer(ss.astype(np.int16),rate) . Your browser does not support the audio element.",
            "url": "https://resteche.github.io/REsteche_blog/estrutura%20da%20mat%C3%A9ria/f%C3%ADsica%20moderna/f%C3%ADsica%20atomica/musica/2021/06/10/Som_hidrog%C3%AAnio.html",
            "relUrl": "/estrutura%20da%20mat%C3%A9ria/f%C3%ADsica%20moderna/f%C3%ADsica%20atomica/musica/2021/06/10/Som_hidrog%C3%AAnio.html",
            "date": " • Jun 10, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "fastcore: An Underrated Python Library",
            "content": ". Background . I recently embarked on a journey to sharpen my python skills: I wanted to learn advanced patterns, idioms, and techniques. I started with reading books on advanced Python, however, the information didn&#39;t seem to stick without having somewhere to apply it. I also wanted the ability to ask questions from an expert while I was learning -- which is an arrangement that is hard to find! That&#39;s when it occurred to me: What if I could find an open source project that has fairly advanced python code and write documentation and tests? I made a bet that if I did this it would force me to learn everything very deeply, and the maintainers would be appreciative of my work and be willing to answer my questions. . And that&#39;s exactly what I did over the past month! I&#39;m pleased to report that it has been the most efficient learning experience I&#39;ve ever experienced. I&#39;ve discovered that writing documentation forced me to deeply understand not just what the code does but also why the code works the way it does, and to explore edge cases while writing tests. Most importantly, I was able to ask questions when I was stuck, and maintainers were willing to devote extra time knowing that their mentorship was in service of making their code more accessible! It turns out the library I choose, fastcore is some of the most fascinating Python I have ever encountered as its purpose and goals are fairly unique. . For the uninitiated, fastcore is a library on top of which many fast.ai projects are built on. Most importantly, fastcore extends the python programming language and strives to eliminate boilerplate and add useful functionality for common tasks. In this blog post, I&#39;m going to highlight some of my favorite tools that fastcore provides, rather than sharing what I learned about python. My goal is to pique your interest in this library, and hopefully motivate you to check out the documentation after you are done to learn more! . Why fastcore is interesting . Get exposed to ideas from other languages without leaving python: I’ve always heard that it is beneficial to learn other languages in order to become a better programmer. From a pragmatic point of view, I’ve found it difficult to learn other languages because I could never use them at work. Fastcore extends python to include patterns found in languages as diverse as Julia, Ruby and Haskell. Now that I understand these tools I am motivated to learn other languages. | You get a new set of pragmatic tools: fastcore includes utilities that will allow you to write more concise expressive code, and perhaps solve new problems. | Learn more about the Python programming language: Because fastcore extends the python programming language, many advanced concepts are exposed during the process. For the motivated, this is a great way to see how many of the internals of python work. | A whirlwind tour through fastcore . Here are some things you can do with fastcore that immediately caught my attention. . . Making **kwargs transparent . Whenever I see a function that has the argument **kwargs, I cringe a little. This is because it means the API is obfuscated and I have to read the source code to figure out what valid parameters might be. Consider the below example: . def baz(a, b=2, c=3, d=4): return a + b + c def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, **kwargs)&gt; . Without reading the source code, it might be hard for me to know that foo also accepts and additional parameters b and d. We can fix this with delegates: . def baz(a, b=2, c=3, d=4): return a + b + c @delegates(baz) # this decorator will pass down keyword arguments from baz def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, b=2, d=4)&gt; . You can customize the behavior of this decorator. For example, you can have your cake and eat it too by passing down your arguments and also keeping **kwargs: . @delegates(baz, keep=True) def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, b=2, d=4, **kwargs)&gt; . You can also exclude arguments. For example, we exclude argument d from delegation: . def basefoo(a, b=2, c=3, d=4): pass @delegates(basefoo, but=[&#39;d&#39;]) # exclude `d` def foo(c, a, **kwargs): pass inspect.signature(foo) . &lt;Signature (c, a, b=2)&gt; . You can also delegate between classes: . class BaseFoo: def __init__(self, e, c=2): pass @delegates()# since no argument was passsed here we delegate to the superclass class Foo(BaseFoo): def __init__(self, a, b=1, **kwargs): super().__init__(**kwargs) inspect.signature(Foo) . &lt;Signature (a, b=1, c=2)&gt; . For more information, read the docs on delegates. . . Avoid boilerplate when setting instance attributes . Have you ever wondered if it was possible to avoid the boilerplate involved with setting attributes in __init__? . class Test: def __init__(self, a, b ,c): self.a, self.b, self.c = a, b, c . Ouch! That was painful. Look at all the repeated variable names. Do I really have to repeat myself like this when defining a class? Not Anymore! Checkout store_attr: . class Test: def __init__(self, a, b, c): store_attr() t = Test(5,4,3) assert t.b == 4 . You can also exclude certain attributes: . class Test: def __init__(self, a, b, c): store_attr(but=[&#39;c&#39;]) t = Test(5,4,3) assert t.b == 4 assert not hasattr(t, &#39;c&#39;) . There are many more ways of customizing and using store_attr than I highlighted here. Check out the docs for more detail. . P.S. you might be thinking that Python dataclasses also allow you to avoid this boilerplate. While true in some cases, store_attr is more flexible.1 . 1. For example, store_attr does not rely on inheritance, which means you won&#39;t get stuck using multiple inheritance when using this with your own classes. Also, unlike dataclasses, store_attr does not require python 3.7 or higher. Furthermore, you can use store_attr anytime in the object lifecycle, and in any location in your class to customize the behavior of how and when variables are stored.↩ . . Avoiding subclassing boilerplate . One thing I hate about python is the __super__().__init__() boilerplate associated with subclassing. For example: . class ParentClass: def __init__(self): self.some_attr = &#39;hello&#39; class ChildClass(ParentClass): def __init__(self): super().__init__() cc = ChildClass() assert cc.some_attr == &#39;hello&#39; # only accessible b/c you used super . We can avoid this boilerplate by using the metaclass PrePostInitMeta. We define a new class called NewParent that is a wrapper around the ParentClass: . class NewParent(ParentClass, metaclass=PrePostInitMeta): def __pre_init__(self, *args, **kwargs): super().__init__() class ChildClass(NewParent): def __init__(self):pass sc = ChildClass() assert sc.some_attr == &#39;hello&#39; . . Type Dispatch . Type dispatch, or Multiple dispatch, allows you to change the way a function behaves based upon the input types it receives. This is a prominent feature in some programming languages like Julia. For example, this is a conceptual example of how multiple dispatch works in Julia, returning different values depending on the input types of x and y: . collide_with(x::Asteroid, y::Asteroid) = ... # deal with asteroid hitting asteroid collide_with(x::Asteroid, y::Spaceship) = ... # deal with asteroid hitting spaceship collide_with(x::Spaceship, y::Asteroid) = ... # deal with spaceship hitting asteroid collide_with(x::Spaceship, y::Spaceship) = ... # deal with spaceship hitting spaceship . Type dispatch can be especially useful in data science, where you might allow different input types (i.e. Numpy arrays and Pandas dataframes) to a function that processes data. Type dispatch allows you to have a common API for functions that do similar tasks. . Unfortunately, Python does not support this out-of-the box. Fortunately, there is the @typedispatch decorator to the rescue. This decorator relies upon type hints in order to route inputs the correct version of the function: . @typedispatch def f(x:str, y:str): return f&#39;{x}{y}&#39; @typedispatch def f(x:np.ndarray): return x.sum() @typedispatch def f(x:int, y:int): return x+y . Below is a demonstration of type dispatch at work for the function f: . f(&#39;Hello &#39;, &#39;World!&#39;) . &#39;Hello World!&#39; . f(2,3) . 5 . f(np.array([5,5,5,5])) . 20 . There are limitations of this feature, as well as other ways of using this functionality that you can read about here. In the process of learning about typed dispatch, I also found a python library called multipledispatch made by Mathhew Rocklin (the creator of Dask). . After using this feature, I am now motivated to learn languages like Julia to discover what other paradigms I might be missing. . . A better version of functools.partial . functools.partial is a great utility that creates functions from other functions that lets you set default values. Lets take this function for example that filters a list to only contain values &gt;= val: . test_input = [1,2,3,4,5,6] def f(arr, val): &quot;Filter a list to remove any values that are less than val.&quot; return [x for x in arr if x &gt;= val] f(test_input, 3) . [3, 4, 5, 6] . You can create a new function out of this function using partial that sets the default value to 5: . filter5 = partial(f, val=5) filter5(test_input) . [5, 6] . One problem with partial is that it removes the original docstring and replaces it with a generic docstring: . filter5.__doc__ . &#39;partial(func, *args, **keywords) - new function with partial application n of the given arguments and keywords. n&#39; . fastcore.utils.partialler fixes this, and makes sure the docstring is retained such that the new API is transparent: . filter5 = partialler(f, val=5) filter5.__doc__ . &#39;Filter a list to remove any values that are less than val.&#39; . . Composition of functions . A technique that is pervasive in functional programming languages is function composition, whereby you chain a bunch of functions together to achieve some kind of result. This is especially useful when applying various data transformations. Consider a toy example where I have three functions: (1) Removes elements of a list less than 5 (from the prior section) (2) adds 2 to each number (3) sums all the numbers: . def add(arr, val): return [x + val for x in arr] def arrsum(arr): return sum(arr) # See the previous section on partialler add2 = partialler(add, val=2) transform = compose(filter5, add2, arrsum) transform([1,2,3,4,5,6]) . 15 . But why is this useful? You might me thinking, I can accomplish the same thing with: . arrsum(add2(filter5([1,2,3,4,5,6]))) . You are not wrong! However, composition gives you a convenient interface in case you want to do something like the following: . def fit(x, transforms:list): &quot;fit a model after performing transformations&quot; x = compose(*transforms)(x) y = [np.mean(x)] * len(x) # its a dumb model. Don&#39;t judge me return y # filters out elements &lt; 5, adds 2, then predicts the mean fit(x=[1,2,3,4,5,6], transforms=[filter5, add2]) . [7.5, 7.5] . For more information about compose, read the docs. . . A more useful __repr__ . In python, __repr__ helps you get information about an object for logging and debugging. Below is what you get by default when you define a new class. (Note: we are using store_attr, which was discussed earlier). . class Test: def __init__(self, a, b=2, c=3): store_attr() # `store_attr` was discussed previously Test(1) . &lt;__main__.Test at 0x7ffcd766cee0&gt; . We can use basic_repr to quickly give us a more sensible default: . class Test: def __init__(self, a, b=2, c=3): store_attr() __repr__ = basic_repr(&#39;a,b,c&#39;) Test(2) . Test(a=2, b=2, c=3) . . Monkey Patching With A Decorator . It can be convenient to monkey patch with a decorator, which is especially helpful when you want to patch an external library you are importing. We can use the decorator @patch from fastcore.foundation along with type hints like so: . class MyClass(int): pass @patch def func(self:MyClass, a): return self+a mc = MyClass(3) . Now, MyClass has an additional method named func: . mc.func(10) . 13 . Still not convinced? I&#39;ll show you another example of this kind of patching in the next section. . . A better pathlib.Path . When you see these extensions to pathlib.path you won&#39;t ever use vanilla pathlib again! A number of additional methods have been added to pathlib, such as: . Path.readlines: same as with open(&#39;somefile&#39;, &#39;r&#39;) as f: f.readlines() | Path.read: same as with open(&#39;somefile&#39;, &#39;r&#39;) as f: f.read() | Path.save: saves file as pickle | Path.load: loads pickle file | Path.ls: shows the contents of the path as a list. | etc. | . Read more about this here. Here is a demonstration of ls: . from fastcore.utils import * from pathlib import Path p = Path(&#39;.&#39;) p.ls() # you don&#39;t get this with vanilla Pathlib.Path!! . (#7) [Path(&#39;2020-09-01-fastcore.ipynb&#39;),Path(&#39;README.md&#39;),Path(&#39;fastcore_imgs&#39;),Path(&#39;2020-02-20-test.ipynb&#39;),Path(&#39;.ipynb_checkpoints&#39;),Path(&#39;2020-02-21-introducing-fastpages.ipynb&#39;),Path(&#39;my_icons&#39;)] . Wait! What&#39;s going on here? We just imported pathlib.Path - why are we getting this new functionality? Thats because we imported the fastcore.utils module, which patches this module via the @patch decorator discussed earlier. Just to drive the point home on why the @patch decorator is useful, I&#39;ll go ahead and add another method to Path right now: . @patch def fun(self:Path): return &quot;This is fun!&quot; p.fun() . &#39;This is fun!&#39; . That is magical, right? I know! That&#39;s why I&#39;m writing about it! . . An Even More Concise Way To Create Lambdas . Self, with an uppercase S, is an even more concise way to create lambdas that are calling methods on an object. For example, let&#39;s create a lambda for taking the sum of a Numpy array: . arr=np.array([5,4,3,2,1]) f = lambda a: a.sum() assert f(arr) == 15 . You can use Self in the same way: . f = Self.sum() assert f(arr) == 15 . Let&#39;s create a lambda that does a groupby and max of a Pandas dataframe: . import pandas as pd df=pd.DataFrame({&#39;Some Column&#39;: [&#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, ], &#39;Another Column&#39;: [5, 7, 50, 70]}) f = Self.groupby(&#39;Some Column&#39;).mean() f(df) . Another Column . Some Column . a 6 | . b 60 | . Read more about Self in the docs). . . Notebook Functions . These are simple but handy, and allow you to know whether or not code is executing in a Jupyter Notebook, Colab, or an Ipython Shell: . from fastcore.imports import in_notebook, in_colab, in_ipython in_notebook(), in_colab(), in_ipython() . (True, False, True) . This is useful if you are displaying certain types of visualizations, progress bars or animations in your code that you may want to modify or toggle depending on the environment. . . A Drop-In Replacement For List . You might be pretty happy with Python&#39;s list. This is one of those situations that you don&#39;t know you needed a better list until someone showed one to you. Enter L, a list like object with many extra goodies. . The best way I can describe L is to pretend that list and numpy had a pretty baby: . define a list (check out the nice __repr__ that shows the length of the list!) . L(1,2,3) . (#3) [1,2,3] . Shuffle a list: . p = L.range(20).shuffle() p . (#20) [8,7,5,12,14,16,2,15,19,6...] . Index into a list: . p[2,4,6] . (#3) [5,14,2] . L has sensible defaults, for example appending an element to a list: . 1 + L(2,3,4) . (#4) [1,2,3,4] . There is much more L has to offer. Read the docs to learn more. . But Wait ... There&#39;s More! . There are more things I would like to show you about fastcore, but there is no way they would reasonably fit into a blog post. Here is a list of some of my favorite things that I didn&#39;t demo in this blog post: . Utilities . The Basics section contain many shortcuts to perform common tasks or provide an additional interface to what standard python provides. . mk_class: quickly add a bunch of attributes to a class | wrap_class: add new methods to a class with a simple decorator | groupby: similar to Scala&#39;s groupby | merge: merge dicts | fasttuple: a tuple on steroids | Infinite Lists: useful for padding and testing | chunked: for batching and organizing stuff | . Multiprocessing . The Multiprocessing section extends python&#39;s multiprocessing library by offering features like: . progress bars | ability to pause to mitigate race conditions with external services | processing things in batches on each worker, ex: if you have a vectorized operation to perform in chunks | . Functional Programming . The functional programming section is my favorite part of this library. . maps: a map that also composes functions | mapped: A more robust map | using_attr: compose a function that operates on an attribute | . Transforms . Transforms is a collection of utilities for creating data transformations and associated pipelines. These transformation utilities build upon many of the building blocks discussed in this blog post. . Further Reading . It should be noted that you should read the main page of the docs first, followed by the section on tests to fully understand the documentation. . The fastcore documentation site. | The fastcore GitHub repo. | Blog post on delegation. | . Shameless plug: fastpages . This blog post was written entirely in a Jupyter Notebook, which GitHub automatically converted into to a blog post! Sound interesting? Check out fastpages. .",
            "url": "https://resteche.github.io/REsteche_blog/fastcore/",
            "relUrl": "/fastcore/",
            "date": " • Sep 1, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Text Classifier Network",
            "content": "introduction . The main goal of this projesct is to develop a deep learning project using Pytorch and Fast.ai. After PyTorch came out in 2017 it has become the world&#39;s fastest-growing deep learning library and is already used for most research papers at top conferences. This is generally a leading indicator of usage in industry, because these are the papers that end up getting used in products and services commercially. PyTorch is considered nowadays the most flexible and expressive library for deep learning. It does not trade off speed for simplicity, but provides both. . PyTorch works best as a low-level foundation library, providing the basic operations for higher-level functionality, the fast.ai library is the most popular library for adding this higher-level functionality on top of PyTorch (there&#39;s even a peer-reviewed academic paper about this layered API) and bearing in mind these reasons, these were the tools that I found most appropriate for this project. . Here, we will see how we can train a model to classify text (here based on their sentiment). First we will see how to do this quickly in a few lines of code, then how to get state-of-the art results using the approach of the ULMFit paper, using the IMDb dataset from the paper Learning Word Vectors for Sentiment Analysis, containing a few thousand movie reviews. In addition , I am planning to add later also a mid-level API for data collection fromn the wikitext dataset, inspired by the teaching tutorial of fast.ai from github. . Installation of the packages . First, it is important to clarify that you can use fastai without any installation by using Google Colab.If you want to run things locally, you can install fastai on your own machines with conda (highly recommended), as long as you&#39;re running Linux or Windows (NB: Mac is not supported). . to proceed with installation in Anaconda, run the following command line: . conda install -c fastchan fastai anaconda . To install with pip, use: pip install fastai. If you install with pip, you should install PyTorch first by following the PyTorch installation instructions, which for my setup design consists on only running the following command line: . conda install pytorch torchvision torchaudio cpuonly -c pytorch . but you can use the mentioned website to configure your installation command line according to your setup settings. . Train a text classifier from a pretrained model . Here, we will try to train a classifier using a pretrained model. To get our data ready, we will first use the high-level API. First, we can download the data and decompress it with the following command: . from fastai.text.all import * path = untar_data(URLs.IMDB) path.ls() . (#5) [Path(&#39;/home/sgugger/.fastai/data/imdb/unsup&#39;),Path(&#39;/home/sgugger/.fastai/data/imdb/models&#39;),Path(&#39;/home/sgugger/.fastai/data/imdb/train&#39;),Path(&#39;/home/sgugger/.fastai/data/imdb/test&#39;),Path(&#39;/home/sgugger/.fastai/data/imdb/README&#39;)] . (path/&#39;train&#39;).ls() . (#4) [Path(&#39;/home/sgugger/.fastai/data/imdb/train/pos&#39;),Path(&#39;/home/sgugger/.fastai/data/imdb/train/unsupBow.feat&#39;),Path(&#39;/home/sgugger/.fastai/data/imdb/train/labeledBow.feat&#39;),Path(&#39;/home/sgugger/.fastai/data/imdb/train/neg&#39;)] . The data follows an ImageNet-style organization, in the train folder, we have two subfolders, pos and neg (for positive reviews and negative reviews). We can gather it by using the TextDataLoaders.from_folder method. The only thing we need to specify is the name of the validation folder, which is &quot;test&quot; (and not the default &quot;valid&quot;). . dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid=&#39;test&#39;) . We can then have a look at the data with the show_batch method: . dls.show_batch() . text category . 0 xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules of the match , both opponents have to go through tables in order to get the win . xxmaj benoit and xxmaj guerrero heated up early on by taking turns hammering first xxmaj spike and then xxmaj bubba xxmaj ray . a xxmaj german xxunk by xxmaj benoit to xxmaj bubba took the wind out of the xxmaj dudley brother . xxmaj spike tried to help his brother , but the referee restrained him while xxmaj benoit and xxmaj guerrero | pos | . 1 xxbos xxmaj warning : xxmaj does contain spoilers . n n xxmaj open xxmaj your xxmaj eyes n n xxmaj if you have not seen this film and plan on doing so , just stop reading here and take my word for it . xxmaj you have to see this film . i have seen it four times so far and i still have n&#39;t made up my mind as to what exactly happened in the film . xxmaj that is all i am going to say because if you have not seen this film , then stop reading right now . n n xxmaj if you are still reading then i am going to pose some questions to you and maybe if anyone has any answers you can email me and let me know what you think . n n i remember my xxmaj grade 11 xxmaj english teacher quite well . xxmaj | pos | . 2 xxbos i thought that xxup rotj was clearly the best out of the three xxmaj star xxmaj wars movies . i find it surprising that xxup rotj is considered the weakest installment in the xxmaj trilogy by many who have voted . xxmaj to me it seemed like xxup rotj was the best because it had the most profound plot , the most suspense , surprises , most xxunk the ending ) and definitely the most episodic movie . i personally like the xxmaj empire xxmaj strikes xxmaj back a lot also but i think it is slightly less good than than xxup rotj since it was slower - moving , was not as episodic , and i just did not feel as much suspense or emotion as i did with the third movie . n n xxmaj it also seems like to me that after reading these surprising reviews that | pos | . 3 xxbos xxup the xxup shop xxup around xxup the xxup corner is one of the sweetest and most feel - good romantic comedies ever made . xxmaj there &#39;s just no getting around that , and it &#39;s hard to actually put one &#39;s feeling for this film into words . xxmaj it &#39;s not one of those films that tries too hard , nor does it come up with the oddest possible scenarios to get the two protagonists together in the end . xxmaj in fact , all its charm is innate , contained within the characters and the setting and the plot … which is highly believable to boot . xxmaj it &#39;s easy to think that such a love story , as beautiful as any other ever told , * could * happen to you … a feeling you do n&#39;t often get from other romantic comedies | pos | . 4 xxbos xxmaj the premise of this movie has been tickling my imagination for quite some time now . xxmaj we &#39;ve all heard or read about it in some kind of con - text . xxmaj what would you do if you were all alone in the world ? xxmaj what would you do if the entire world suddenly disappeared in front of your eyes ? xxmaj in fact , the last part is actually what happens to xxmaj dave and xxmaj andrew , two room - mates living in a run - down house in the middle of a freeway system . xxmaj andrew is a nervous wreck to say the least and xxmaj dave is considered being one of the biggest losers of society . xxmaj that alone is the main reason to why these two guys get so well along , because they simply only have each | pos | . 5 xxbos xxrep 3 * xxup spoilers xxrep 3 * xxrep 3 * xxup spoilers xxrep 3 * xxmaj continued … n n xxmaj from here on in the whole movie collapses in on itself . xxmaj first we meet a rogue program with the indication we &#39;re gon na get ghosts and vampires and werewolves and the like . xxmaj we get a guy with a retarded accent talking endless garbage , two &#39; ghosts &#39; that serve no real purpose and have no character what - so - ever and a bunch of henchmen . xxmaj someone &#39;s told me they &#39;re vampires ( straight out of xxmaj blade 2 ) , but they &#39;re so undefined i did n&#39;t realise . n n xxmaj the funny accented guy with a ridiculous name suffers the same problem as the xxmaj oracle , only for far longer and far far worse . | neg | . 6 xxbos xxmaj i &#39;ve rented and watched this movie for the 1st time on xxup dvd without reading any reviews about it . xxmaj so , after 15 minutes of watching xxmaj i &#39;ve noticed that something is wrong with this movie ; it &#39;s xxup terrible ! i mean , in the trailers it looked scary and serious ! n n i think that xxmaj eli xxmaj roth ( mr . xxmaj director ) thought that if all the characters in this film were stupid , the movie would be funny … ( so stupid , it &#39;s funny … ? xxup wrong ! ) xxmaj he should watch and learn from better horror - comedies such xxunk xxmaj night &quot; , &quot; the xxmaj lost xxmaj boys &quot; and &quot; the xxmaj return xxmaj of the xxmaj living xxmaj dead &quot; ! xxmaj those are funny ! n n &quot; | neg | . 7 xxbos xxup myra xxup breckinridge is one of those rare films that established its place in film history immediately . xxmaj praise for the film was absolutely nonexistent , even from the people involved in making it . xxmaj this film was loathed from day one . xxmaj while every now and then one will come across some maverick who will praise the film on philosophical grounds ( aggressive feminism or the courage to tackle the issue of xxunk ) , the film has not developed a cult following like some notorious flops do . xxmaj it &#39;s not hailed as a misunderstood masterpiece like xxup scarface , or trotted out to be ridiculed as a camp classic like xxup showgirls . n n xxmaj undoubtedly the reason is that the film , though outrageously awful , is not lovable , or even likable . xxup myra xxup breckinridge is just | neg | . 8 xxbos xxmaj after reading the previous comments , xxmaj i &#39;m just glad that i was n&#39;t the only person left confused , especially by the last 20 minutes . xxmaj john xxmaj carradine is shown twice walking down into a grave and pulling the lid shut after him . i anxiously awaited some kind of explanation for this odd behavior … naturally i assumed he had something to do with the evil goings - on at the house , but since he got killed off by the first rising corpse ( hereafter referred to as xxmaj zombie # 1 ) , these scenes made absolutely no sense . xxmaj please , if someone out there knows why xxmaj carradine kept climbing down into graves -- let the rest of us in on it ! ! n n xxmaj all the action is confined to the last 20 minutes so xxmaj | neg | . We can see that the library automatically processed all the texts to split then in tokens, adding some special tokens like: . xxbos to indicate the beginning of a text | xxmaj to indicate the next word was capitalized | . Then, we can define a Learner suitable for text classification in one line . learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy) . Here, we have used the AWD LSTM architecture, drop_mult is a parameter that controls the magnitude of all dropouts in that model, and we use accuracy to track down how well we are doing. We can then fine-tune our pretrained model as can be seen below . learn.fine_tune(4, 1e-2) . epoch train_loss valid_loss accuracy time . 0 | 0.587251 | 0.386230 | 0.828960 | 01:35 | . epoch train_loss valid_loss accuracy time . 0 | 0.307347 | 0.263843 | 0.892800 | 03:03 | . 1 | 0.215867 | 0.226208 | 0.911800 | 02:55 | . 2 | 0.155399 | 0.231144 | 0.913960 | 03:12 | . 3 | 0.129277 | 0.200941 | 0.925920 | 03:01 | . As you can see from the progress of the last line, our model is doing pretty good until now! In order to quantize through objective criteria how well the model is performing, we can use the show_results method as it follows . learn.show_results() . text category category_ . 0 xxbos xxmaj there &#39;s a sign on xxmaj the xxmaj lost xxmaj highway that says : n n * major xxup spoilers xxup ahead * n n ( but you already knew that , did n&#39;t you ? ) n n xxmaj since there &#39;s a great deal of people that apparently did not get the point of this movie , xxmaj i &#39;d like to contribute my interpretation of why the plot makes perfect sense . xxmaj as others have pointed out , one single viewing of this movie is not sufficient . xxmaj if you have the xxup dvd of xxup md , you can &quot; cheat &quot; by looking at xxmaj david xxmaj lynch &#39;s &quot; top 10 xxmaj hints to xxmaj unlocking xxup md &quot; ( but only upon second or third viewing , please . ) ;) n n xxmaj first of all , xxmaj mulholland xxmaj drive is | pos | pos | . 1 xxbos ( some spoilers included : ) n n xxmaj although , many commentators have called this film surreal , the term fits poorly here . xxmaj to quote from xxmaj encyclopedia xxmaj xxunk &#39;s , surreal means : n n &quot; fantastic or incongruous imagery &quot; : xxmaj one need n&#39;t explain to the unimaginative how many ways a plucky ten - year - old boy at large and seeking his fortune in the driver &#39;s seat of a red xxmaj mustang could be fantastic : those curious might read xxmaj james xxmaj kincaid ; but if you asked said lad how he were incongruous behind the wheel of a sports car , he &#39;d surely protest , &quot; no way ! &quot; xxmaj what fantasies and incongruities the film offers mostly appear within the first fifteen minutes . xxmaj thereafter we get more iterations of the same , in an | pos | neg | . 2 xxbos xxmaj hearkening back to those &quot; good xxmaj old xxmaj days &quot; of 1971 , we can vividly recall when we were treated with a whole xxmaj season of xxmaj charles xxmaj chaplin at the xxmaj cinema . xxmaj that &#39;s what the promotional guy called it when we saw him on somebody &#39;s old talk show . ( we ca n&#39;t recall just whose it was ; either xxup merv xxup griffin or xxup woody xxup woodbury , one or the other ! ) xxmaj the guest talked about xxmaj sir xxmaj charles &#39; career and how his films had been out of circulation ever since the 1952 exclusion of the former &quot; little xxmaj tramp &#39; from xxmaj los xxmaj xxunk xxmaj xxunk on the grounds of his being an &quot; undesirable xxmaj alien &quot; . ( no xxmaj schultz , he &#39;s xxup not from another | pos | pos | . 3 xxbos &quot; buffalo xxmaj bill , xxmaj hero of the xxmaj far xxmaj west &quot; director xxmaj mario xxmaj costa &#39;s unsavory xxmaj spaghetti western &quot; the xxmaj beast &quot; with xxmaj klaus xxmaj kinski could only have been produced in xxmaj europe . xxmaj hollywood would never dared to have made a western about a sexual predator on the prowl as the protagonist of a movie . xxmaj never mind that xxmaj kinski is ideally suited to the role of &#39; crazy &#39; xxmaj johnny . xxmaj he plays an individual entirely without sympathy who is ironically dressed from head to toe in a white suit , pants , and hat . xxmaj this low - budget oater has nothing appetizing about it . xxmaj the typically breathtaking xxmaj spanish scenery around xxmaj almeria is nowhere in evidence . xxmaj instead , xxmaj costa and his director of photography | pos | pos | . 4 xxbos xxmaj if you &#39;ve seen the trailer for this movie , you pretty much know what to expect , because what you see here is what you get . xxmaj and even if you have n&#39;t seen the previews , it wo n&#39;t take you long to pick up on what you &#39;re in for-- specifically , a good time and plenty of xxunk from this clever satire of ` reality xxup tv &#39; shows and ` buddy xxmaj cop &#39; movies , ` showtime , &#39; directed by xxmaj tom xxmaj dey , starring xxmaj robert xxmaj de xxmaj niro and xxmaj eddie xxmaj murphy . n n t xxmaj mitch xxmaj preston ( de xxmaj niro ) is a detective with the xxup l.a.p.d . , and he &#39;s good at what he does ; but working a case one night , things suddenly go south when another cop | pos | pos | . 5 xxbos * xxmaj some spoilers * n n xxmaj this movie is sometimes subtitled &quot; life xxmaj everlasting . &quot; xxmaj that &#39;s often taken as reference to the final scene , but more accurately describes how dead and buried this once - estimable series is after this sloppy and illogical send - off . n n xxmaj there &#39;s a &quot; hey kids , let &#39;s put on a show air &quot; about this telemovie , which can be endearing in spots . xxmaj some fans will feel like insiders as they enjoy picking out all the various cameo appearances . xxmaj co - writer , co - producer xxmaj tom xxmaj fontana and his pals pack the goings - on with friends and favorites from other shows , as well as real xxmaj baltimore personages . n n xxmaj that &#39;s on top of the returns of virtually all the members | neg | neg | . 6 xxbos ( caution : several spoilers ) n n xxmaj someday , somewhere , there &#39;s going to be a post - apocalyptic movie made that does n&#39;t stink . xxmaj unfortunately , xxup the xxup postman is not that movie , though i have to give it credit for trying . n n xxmaj kevin xxmaj costner plays somebody credited only as &quot; the xxmaj postman . &quot; xxmaj he &#39;s not actually a postman , just a wanderer with a mule in the wasteland of a western xxmaj america devastated by some unspecified catastrophe . xxmaj he trades with isolated villages by performing xxmaj shakespeare . xxmaj suddenly a pack of bandits called the xxmaj holnists , the self - declared warlords of the xxmaj west , descend upon a village that xxmaj costner &#39;s visiting , and their evil leader xxmaj gen . xxmaj bethlehem ( will xxmaj patton | neg | neg | . 7 xxbos xxmaj in a style reminiscent of the best of xxmaj david xxmaj lean , this romantic love story sweeps across the screen with epic proportions equal to the vast desert regions against which it is set . xxmaj it &#39;s a film which purports that one does not choose love , but rather that it &#39;s love that does the choosing , regardless of who , where or when ; and furthermore , that it &#39;s a matter of the heart often contingent upon prevailing conditions and circumstances . xxmaj and thus is the situation in ` the xxmaj english xxmaj patient , &#39; directed by xxmaj anthony xxmaj minghella , the story of two people who discover passion and true love in the most inopportune of places and times , proving that when it is predestined , love will find a way . n n xxmaj it &#39;s xxup | pos | pos | . 8 xxbos xxmaj no one is going to mistake xxup the xxup squall for a good movie , but it sure is a memorable one . xxmaj once you &#39;ve taken in xxmaj myrna xxmaj loy &#39;s performance as xxmaj nubi the hot - blooded gypsy girl you &#39;re not likely to forget the experience . xxmaj when this film was made the exotically beautiful xxmaj miss xxmaj loy was still being cast as foreign vixens , often xxmaj asian and usually sinister . xxmaj she &#39;s certainly an eyeful here . xxmaj it appears that her skin was darkened and her hair was curled . xxmaj in most scenes she &#39;s barefoot and wearing little more than a skirt and a loose - fitting peasant blouse , while in one scene she wears nothing but a patterned towel . i suppose xxmaj i &#39;m focusing on xxmaj miss xxmaj loy | neg | neg | . And we can predict on new texts quite easily from now on as our model has already undergone more advanced training . learn.predict(&quot;I really liked that movie!&quot;) . (&#39;pos&#39;, tensor(1), tensor([0.0092, 0.9908])) . Here we can see the model has considered the review to be positive. The second part of the result is the index of &quot;pos&quot; in our data vocabulary and the last part is the probabilities attributed to each class (99.1% for &quot;pos&quot; and 0.9% for &quot;neg&quot;). . Using what has been done so far, you can even write your own mini movie review, or copy one from the Internet, and we can see what this model thinks about it by performing the same command that we run before! . For the curious ones: Using the data block API . We can also use the data block API to get our data in a DataLoaders. This is a bit more advanced, The intention with this is just to refine our work a little more so far in order to learn details about the structure of the data and the way it will be stored. . A datablock is built by giving the fastai library a bunch of information: . the types used, through an argument called blocks: here we have images and categories, so we pass TextBlock and CategoryBlock. To inform the library our texts are files in a folder, we use the from_folder class method. | how to get the raw items, here our function get_text_files. | how to label those items, here with the parent folder. | how to split those items, here with the grandparent folder. | . imdb = DataBlock(blocks=(TextBlock.from_folder(path), CategoryBlock), get_items=get_text_files, get_y=parent_label, splitter=GrandparentSplitter(valid_name=&#39;test&#39;)) . This only gives a blueprint on how to assemble the data. To actually create it, we need to use the dataloaders method: . dls = imdb.dataloaders(path) . Train a text classifier using the ULMFit approach . The pretrained model we used in the first stage of this project is called a language model. It was pretrained on Wikipedia on the task of guessing the next word, after reading all the words before. We got great results by directly fine-tuning this language model to a movie review classifier, but with one extra step, we can do even better: the Wikipedia English is slightly different from the IMDb English. So instead of jumping directly to the classifier, we could fine-tune our pretrained language model to the IMDb corpus and then use that as the base for our classifier. . One reason, of course, is that it is helpful to understand the foundations of the models that you are using. But there is another very practical reason, which is that you get even better results if you fine tune the (sequence-based) language model prior to fine tuning the classification model. For instance, in the IMDb sentiment analysis task, the dataset includes 50,000 additional movie reviews that do not have any positive or negative labels attached in the unsup folder. We can use all of these reviews to fine tune the pretrained language model — this will result in a language model that is particularly good at predicting the next word of a movie review. In contrast, the pretrained model was trained only on Wikipedia articles. . The whole ideia of this stage can be summarized by this picture: . . We can get our texts in a DataLoaders suitable for language modeling very easily: . dls_lm = TextDataLoaders.from_folder(path, is_lm=True, valid_pct=0.1) . We need to pass something for valid_pct otherwise this method will try to split the data by using the grandparent folder names. By passing valid_pct=0.1, we tell it to get a random 10% of those reviews for the validation set. . We can have a look at our data using show_batch. Here the task is to guess the next word, so we can see the targets have all shifted one word to the right. . dls_lm.show_batch(max_n=5) . text text_ . 0 xxbos xxmaj about thirty minutes into the film , i thought this was one of the weakest &quot; xxunk ever because it had the usual beginning ( a murder happening , then xxmaj columbo coming , inspecting everything and interrogating the main suspect ) squared ! xxmaj it was boring because i thought i knew everything already . n n xxmaj but then there was a surprising twist that turned this episode into | xxmaj about thirty minutes into the film , i thought this was one of the weakest &quot; xxunk ever because it had the usual beginning ( a murder happening , then xxmaj columbo coming , inspecting everything and interrogating the main suspect ) squared ! xxmaj it was boring because i thought i knew everything already . n n xxmaj but then there was a surprising twist that turned this episode into a | . 1 yeon . xxmaj these two girls were magical on the screen . i will certainly be looking into their other films . xxmaj xxunk xxmaj jeong - ah is xxunk cheerful and hauntingly evil as the stepmother . xxmaj finally , xxmaj xxunk - su xxmaj kim gives an excellent performance as the weary , broken father . n n i truly love this film . xxmaj if you have yet to see | . xxmaj these two girls were magical on the screen . i will certainly be looking into their other films . xxmaj xxunk xxmaj jeong - ah is xxunk cheerful and hauntingly evil as the stepmother . xxmaj finally , xxmaj xxunk - su xxmaj kim gives an excellent performance as the weary , broken father . n n i truly love this film . xxmaj if you have yet to see &#39; | . 2 tends to be tedious whenever there are n&#39;t any hideous monsters on display . xxmaj luckily the gutsy killings and eerie set designs ( by no less than xxmaj bill xxmaj paxton ! ) compensate for a lot ! a nine - headed expedition is send ( at hyper speed ) to the unexplored regions of space to find out what happened to a previously vanished spaceship and its crew . xxmaj | to be tedious whenever there are n&#39;t any hideous monsters on display . xxmaj luckily the gutsy killings and eerie set designs ( by no less than xxmaj bill xxmaj paxton ! ) compensate for a lot ! a nine - headed expedition is send ( at hyper speed ) to the unexplored regions of space to find out what happened to a previously vanished spaceship and its crew . xxmaj bad | . 3 movie just sort of meanders around and nothing happens ( i do n&#39;t mean in terms of plot - no plot is fine , but no action ? xxmaj come on . ) xxmaj in hindsight , i should have expected this - after all , how much can really happen between 4 teens and a bear ? xxmaj so although special effects , acting , etc are more or less on | just sort of meanders around and nothing happens ( i do n&#39;t mean in terms of plot - no plot is fine , but no action ? xxmaj come on . ) xxmaj in hindsight , i should have expected this - after all , how much can really happen between 4 teens and a bear ? xxmaj so although special effects , acting , etc are more or less on par | . 4 greetings again from the darkness . xxmaj writer / xxmaj director ( and xxmaj wes xxmaj anderson collaborator ) xxmaj noah xxmaj baumbach presents a semi - autobiographical therapy session where he unleashes the anguish and turmoil that has carried over from his childhood . xxmaj the result is an amazing insight into what many people go through in a desperate attempt to try and make their family work . n n xxmaj | again from the darkness . xxmaj writer / xxmaj director ( and xxmaj wes xxmaj anderson collaborator ) xxmaj noah xxmaj baumbach presents a semi - autobiographical therapy session where he unleashes the anguish and turmoil that has carried over from his childhood . xxmaj the result is an amazing insight into what many people go through in a desperate attempt to try and make their family work . n n xxmaj the | . Then we have a convenience method to directly grab a Learner from it, using the AWD_LSTM architecture like before. We use accuracy and perplexity as metrics (the later is the exponential of the loss) and we set a default weight decay of 0.1. to_fp16 puts the Learner in mixed precision, which is going to help speed up training on GPUs that have Tensor Cores. . learn = language_model_learner(dls_lm, AWD_LSTM, metrics=[accuracy, Perplexity()], path=path, wd=0.1).to_fp16() . By default, a pretrained Learner is in a frozen state, meaning that only the head of the model will train while the body stays frozen. We show you what is behind the fine_tune method here and use a fit_one_cycle method to fit the model: . learn.fit_one_cycle(1, 1e-2) . epoch train_loss valid_loss accuracy perplexity time . 0 | 4.120048 | 3.912788 | 0.299565 | 50.038246 | 11:39 | . This model takes a while to train, so it&#39;s a good opportunity to talk about saving intermediary results. . One should know that the state of your model can easily be saved like so: . learn.save(&#39;1epoch&#39;) . It will create a file in learn.path/models/ named &quot;1epoch.pth&quot;. If you want to load your model on another machine after creating your Learner the same way, or resume training later, you can load the content of this file with: . learn = learn.load(&#39;1epoch&#39;) . We can them fine-tune the model after unfreezing: . learn.unfreeze() learn.fit_one_cycle(10, 1e-3) . epoch train_loss valid_loss accuracy perplexity time . 0 | 3.893486 | 3.772820 | 0.317104 | 43.502548 | 12:37 | . 1 | 3.820479 | 3.717197 | 0.323790 | 41.148880 | 12:30 | . 2 | 3.735622 | 3.659760 | 0.330321 | 38.851997 | 12:09 | . 3 | 3.677086 | 3.624794 | 0.333960 | 37.516987 | 12:12 | . 4 | 3.636646 | 3.601300 | 0.337017 | 36.645859 | 12:05 | . 5 | 3.553636 | 3.584241 | 0.339355 | 36.026001 | 12:04 | . 6 | 3.507634 | 3.571892 | 0.341353 | 35.583862 | 12:08 | . 7 | 3.444101 | 3.565988 | 0.342194 | 35.374371 | 12:08 | . 8 | 3.398597 | 3.566283 | 0.342647 | 35.384815 | 12:11 | . 9 | 3.375563 | 3.568166 | 0.342528 | 35.451500 | 12:05 | . Once this is done, we save all of our model except the final layer that converts activations to probabilities of picking each token in our vocabulary. The model not including the final layer is called the encoder. We can save it with save_encoder: . learn.save_encoder(&#39;finetuned&#39;) . Jargon:Encoder: The model not including the task-specific final layer(s). It means much the same thing as body when applied to vision CNNs, but tends to be more used for NLP and generative models. . Before using this to fine-tune a classifier on the reviews, we can use our model to generate random reviews: since it&#39;s trained to guess what the next word of the sentence is, we can use it to write new reviews: . TEXT = &quot;I liked this movie because&quot; N_WORDS = 40 N_SENTENCES = 2 preds = [learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)] . print(&quot; n&quot;.join(preds)) . i liked this movie because of its story and characters . The story line was very strong , very good for a sci - fi film . The main character , Alucard , was very well developed and brought the whole story i liked this movie because i like the idea of the premise of the movie , the ( very ) convenient virus ( which , when you have to kill a few people , the &#34; evil &#34; machine has to be used to protect . Finally training a text classifier . We can gather our data for text classification almost exactly like before: . dls_clas = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid=&#39;test&#39;, text_vocab=dls_lm.vocab) . The main difference is that we have to use the exact same vocabulary as when we were fine-tuning our language model, or the weights learned won&#39;t make any sense. We pass that vocabulary with text_vocab. . Then we can define our text classifier like before: . learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, metrics=accuracy) . The difference is that before training it, we load the previous encoder: . learn = learn.load_encoder(&#39;finetuned&#39;) . The last step is to train with discriminative learning rates and gradual unfreezing. In computer vision, we often unfreeze the model all at once, but for NLP classifiers, we find that unfreezing a few layers at a time makes a real difference. . learn.fit_one_cycle(1, 2e-2) . epoch train_loss valid_loss accuracy time . 0 | 0.347427 | 0.184480 | 0.929320 | 00:33 | . In just one epoch we get the same result as our training in the first section, not too bad! We can pass -2 to freeze_to to freeze all except the last two parameter groups: . learn.freeze_to(-2) learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2)) . epoch train_loss valid_loss accuracy time . 0 | 0.247763 | 0.171683 | 0.934640 | 00:37 | . Then we can unfreeze a bit more, and continue training: . learn.freeze_to(-3) learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3)) . epoch train_loss valid_loss accuracy time . 0 | 0.193377 | 0.156696 | 0.941200 | 00:45 | . And finally, the whole model! . learn.unfreeze() learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3)) . epoch train_loss valid_loss accuracy time . 0 | 0.172888 | 0.153770 | 0.943120 | 01:01 | . 1 | 0.161492 | 0.155567 | 0.942640 | 00:57 | . Conclusion of the project . So far, I hope the power of artificial intelligences that use predefined python libraries like pytorch has become clear. With simple scripts like the one developed here for didactic purposes, we can create models capable of predicting market behavior, human writing patterns, and even performing detailed analyzes on databases that would cost us many hours of relatively automated work, which can be saved with a simple implementation like the one shown here. . Ihope you like it, and feel free to try on your own machine and tell me later your thoughts on this project, as well as any suggested modifications, everything can be discussed in the comments of this push on github or through private conversations in my email: . ruben.esteche@ufpe.br . or . rubenesteche@hotmail.com .",
            "url": "https://resteche.github.io/REsteche_blog/deep%20learning/pytorch/text%20classification/neural%20network/2020/03/01/DL_text_project.html",
            "relUrl": "/deep%20learning/pytorch/text%20classification/neural%20network/2020/03/01/DL_text_project.html",
            "date": " • Mar 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hello again 👋, . In case you want to know more about my recent work, you can check my github here . I promiss I’ll do my best to keep it updated! . Remember you can contact me through my institutional or professional email anytime for doubts or sugestions. . Make sure you check out everything that interests you on my site again! I hope you like my content. .",
          "url": "https://resteche.github.io/REsteche_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://resteche.github.io/REsteche_blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}